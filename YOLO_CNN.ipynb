{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81463e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from keras.models import load_model # type: ignore\n",
    "from keras.applications.imagenet_utils import preprocess_input # type: ignore\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4077487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Using cache found in C:\\Users\\Anurag Katkar/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-7-13 Python-3.10.18 torch-2.5.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Load your trained CNN model\n",
    "cnn_model = load_model('cnn_model_v01.h5')\n",
    "\n",
    "# Load YOLOv5s model (CPU-only)\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', device='cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1830e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your traffic sign class names (adjust according to your dataset)\n",
    "class_names = [\n",
    "    \"Speed limit 20\", \"Speed limit 30\", \"Speed limit 50\", \"Speed limit 60\",\n",
    "    \"Speed limit 70\", \"Speed limit 80\", \"End of speed limit 80\", \"Speed limit 100\",\n",
    "    \"Speed limit 120\", \"No passing\", \"No passing for trucks\", \"Right-of-way at intersection\",\n",
    "    \"Priority road\", \"Yield\", \"Stop\", \"No vehicles\", \"No trucks\",\n",
    "    \"No entry\", \"General caution\", \"Dangerous curve left\", \"Dangerous curve right\",\n",
    "    \"Double curve\", \"Bumpy road\", \"Slippery road\", \"Road narrows on the right\",\n",
    "    \"Road work\", \"Traffic signals\", \"Pedestrians\", \"Children crossing\",\n",
    "    \"Bicycles crossing\", \"Beware of ice/snow\", \"Wild animals crossing\",\n",
    "    \"End of all restrictions\", \"Turn right ahead\", \"Turn left ahead\",\n",
    "    \"Ahead only\", \"Go straight or right\", \"Go straight or left\", \"Keep right\",\n",
    "    \"Keep left\", \"Roundabout\", \"End of no passing\", \"End of no passing for trucks\"\n",
    "]\n",
    "\n",
    "# Preprocess cropped sign for CNN\n",
    "def preprocess_sign(image):\n",
    "    image = cv2.resize(image, (32, 32))            # Resize to model input size\n",
    "    image = image.astype(\"float32\") / 255.0         # Normalize\n",
    "    image = np.expand_dims(image, axis=0)           # Add batch dimension\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8e27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 Results: image 1/1: 480x640 1 bottle, 1 tv\n",
      "Speed: 7.0ms pre-process, 125.5ms inference, 14.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11804e+02, 1.30358e+02, 5.35367e+02, 3.42207e+02, 5.40250e-01, 6.20000e+01],\n",
      "        [1.98318e+02, 3.62983e+02, 2.10422e+02, 3.91719e+02, 3.42317e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 8.0ms pre-process, 202.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.21952e+02, 1.31939e+02, 5.28606e+02, 3.42757e+02, 4.59097e-01, 6.20000e+01],\n",
      "        [2.05696e+02, 1.49115e+02, 6.39811e+02, 4.74656e+02, 4.29901e-01, 0.00000e+00],\n",
      "        [1.98044e+02, 3.62818e+02, 2.10472e+02, 3.91811e+02, 3.46726e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 3.0ms pre-process, 154.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98095e+02, 3.62528e+02, 2.10601e+02, 3.91756e+02, 4.04902e-01, 3.90000e+01],\n",
      "        [3.59661e+02, 1.47220e+02, 6.22961e+02, 4.77630e+02, 3.82360e-01, 0.00000e+00],\n",
      "        [1.86757e+02, 1.26192e+02, 5.70662e+02, 3.53579e+02, 3.03325e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 150.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98053e+02, 3.62887e+02, 2.10477e+02, 3.91974e+02, 4.29654e-01, 3.90000e+01],\n",
      "        [3.41737e+02, 1.46656e+02, 6.02117e+02, 4.76831e+02, 3.60882e-01, 0.00000e+00],\n",
      "        [1.89152e+02, 1.34005e+02, 5.70152e+02, 3.51002e+02, 3.12474e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 3.0ms pre-process, 143.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.43025e+02, 1.48656e+02, 5.87893e+02, 4.79342e+02, 4.92384e-01, 0.00000e+00],\n",
      "        [1.88214e+02, 1.27938e+02, 5.69888e+02, 3.53353e+02, 4.67321e-01, 6.20000e+01],\n",
      "        [1.98301e+02, 3.62684e+02, 2.10317e+02, 3.91852e+02, 4.11824e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 152.7ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.30038e+02, 1.52507e+02, 5.76276e+02, 4.75660e+02, 3.87763e-01, 0.00000e+00],\n",
      "        [1.98402e+02, 3.62832e+02, 2.10004e+02, 3.91605e+02, 3.79631e-01, 3.90000e+01],\n",
      "        [1.82586e+02, 1.22205e+02, 5.69623e+02, 3.56700e+02, 3.68070e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 157.2ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.17823e+02, 1.52947e+02, 5.64290e+02, 4.74201e+02, 5.80298e-01, 0.00000e+00],\n",
      "        [1.89800e+02, 1.31851e+02, 5.60893e+02, 3.54338e+02, 4.85439e-01, 6.20000e+01],\n",
      "        [1.98201e+02, 3.63245e+02, 2.10120e+02, 3.91788e+02, 2.56819e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 3.0ms pre-process, 152.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.67499e+02, 1.22100e+02, 5.65052e+02, 3.62429e+02, 4.25269e-01, 6.20000e+01],\n",
      "        [3.07150e+02, 1.54918e+02, 5.56084e+02, 4.74520e+02, 3.08208e-01, 0.00000e+00],\n",
      "        [1.97817e+02, 3.63407e+02, 2.10430e+02, 3.91754e+02, 2.95115e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 bottle\n",
      "Speed: 2.0ms pre-process, 153.5ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[198.05025, 363.02539, 210.28807, 391.95618,   0.39655,  39.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle\n",
      "Speed: 2.0ms pre-process, 149.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.99572e+02, 1.55652e+02, 5.32847e+02, 4.75854e+02, 4.86939e-01, 0.00000e+00],\n",
      "        [1.98151e+02, 3.63067e+02, 2.10482e+02, 3.91747e+02, 3.54247e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle\n",
      "Speed: 2.0ms pre-process, 150.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98161e+02, 3.63265e+02, 2.10456e+02, 3.91849e+02, 3.40714e-01, 3.90000e+01],\n",
      "        [2.89072e+02, 1.59519e+02, 5.71321e+02, 4.74291e+02, 3.33355e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle\n",
      "Speed: 4.0ms pre-process, 148.1ms inference, 4.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.97646e+02, 3.63225e+02, 2.10087e+02, 3.91729e+02, 3.89257e-01, 3.90000e+01],\n",
      "        [2.89101e+02, 1.54145e+02, 6.22159e+02, 4.76761e+02, 2.91811e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 147.2ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.99009e+02, 1.58037e+02, 5.32422e+02, 4.74926e+02, 3.64978e-01, 0.00000e+00],\n",
      "        [1.98256e+02, 3.63069e+02, 2.10384e+02, 3.91889e+02, 3.62214e-01, 3.90000e+01],\n",
      "        [1.82359e+02, 1.16403e+02, 5.50405e+02, 3.75781e+02, 3.34126e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 4.0ms pre-process, 161.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.04270e+02, 1.57750e+02, 5.41222e+02, 4.76346e+02, 3.96383e-01, 0.00000e+00],\n",
      "        [1.98138e+02, 3.63123e+02, 2.10814e+02, 3.92093e+02, 3.57258e-01, 3.90000e+01],\n",
      "        [1.83018e+02, 1.24687e+02, 5.46858e+02, 3.69195e+02, 2.84821e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 4.0ms pre-process, 154.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.78874e+02, 1.27221e+02, 5.54714e+02, 3.62886e+02, 4.46280e-01, 6.20000e+01],\n",
      "        [3.08632e+02, 1.55973e+02, 5.48022e+02, 4.75931e+02, 4.07411e-01, 0.00000e+00],\n",
      "        [1.98170e+02, 3.63241e+02, 2.10510e+02, 3.91828e+02, 3.70381e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 4.0ms pre-process, 153.5ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.68431e+02, 1.19570e+02, 5.69370e+02, 3.62238e+02, 4.18380e-01, 6.20000e+01],\n",
      "        [1.07024e+02, 1.36555e+02, 5.70937e+02, 4.75188e+02, 3.17997e-01, 0.00000e+00],\n",
      "        [1.98294e+02, 3.63185e+02, 2.10513e+02, 3.91804e+02, 3.00574e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 4.0ms pre-process, 155.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.30413e+02, 1.57842e+02, 6.08195e+02, 4.77446e+02, 5.66592e-01, 0.00000e+00],\n",
      "        [1.97980e+02, 3.62855e+02, 2.10178e+02, 3.91709e+02, 4.55704e-01, 3.90000e+01],\n",
      "        [1.88693e+02, 1.34484e+02, 5.68827e+02, 3.47269e+02, 4.00027e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 172.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.82145e+02, 1.28180e+02, 5.71427e+02, 3.51489e+02, 3.61479e-01, 6.20000e+01],\n",
      "        [1.98473e+02, 3.62986e+02, 2.10069e+02, 3.91759e+02, 3.53622e-01, 3.90000e+01],\n",
      "        [1.32050e+02, 1.41211e+02, 6.05454e+02, 4.74058e+02, 3.01237e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 6.0ms pre-process, 190.0ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.77788e+02, 1.25475e+02, 5.80535e+02, 3.62928e+02, 4.63387e-01, 6.20000e+01],\n",
      "        [1.98172e+02, 3.62569e+02, 2.10562e+02, 3.91950e+02, 3.07871e-01, 3.90000e+01],\n",
      "        [1.35167e+02, 1.35292e+02, 5.87160e+02, 4.75899e+02, 2.51435e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 168.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98222e+02, 3.62753e+02, 2.10284e+02, 3.91849e+02, 4.60826e-01, 3.90000e+01],\n",
      "        [1.72246e+02, 1.19909e+02, 5.71271e+02, 3.64077e+02, 3.35399e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 3.0ms pre-process, 149.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.76471e+02, 1.25355e+02, 5.59902e+02, 3.58447e+02, 3.95331e-01, 6.20000e+01],\n",
      "        [1.98297e+02, 3.63245e+02, 2.10594e+02, 3.91712e+02, 2.90508e-01, 3.90000e+01],\n",
      "        [1.15386e+02, 1.46782e+02, 5.57253e+02, 4.80000e+02, 2.75271e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 2.0ms pre-process, 138.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.83096e+02, 1.23340e+02, 5.64773e+02, 3.58737e+02, 3.26691e-01, 6.20000e+01],\n",
      "        [3.04663e+02, 1.48778e+02, 5.56757e+02, 4.76545e+02, 3.22450e-01, 0.00000e+00],\n",
      "        [1.98259e+02, 3.63039e+02, 2.09942e+02, 3.91711e+02, 3.18717e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle, 1 tv\n",
      "Speed: 4.0ms pre-process, 162.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.36441e+02, 1.46752e+02, 6.03260e+02, 4.76018e+02, 6.75162e-01, 0.00000e+00],\n",
      "        [1.98625e+02, 3.62818e+02, 2.10312e+02, 3.91937e+02, 4.08516e-01, 3.90000e+01],\n",
      "        [1.71610e+02, 1.13418e+02, 5.81544e+02, 3.68146e+02, 3.82765e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 144.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.21145e+02, 8.17959e+01, 6.29185e+02, 4.79418e+02, 4.22794e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 6.1ms pre-process, 156.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.08969e+02, 1.16484e+01, 6.39353e+02, 4.67586e+02, 4.03617e-01, 0.00000e+00],\n",
      "        [1.78197e+02, 1.82620e+01, 5.79513e+02, 2.50563e+02, 3.33594e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 144.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.62771e+01, 0.00000e+00, 6.28705e+02, 4.74856e+02, 4.96636e-01, 0.00000e+00],\n",
      "        [2.22031e+02, 9.39240e-01, 3.32265e+02, 1.76632e+02, 3.30121e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 5.0ms pre-process, 160.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.95050,   0.00000, 590.75104, 480.00000,   0.69557,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 149.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 42.53015,   4.48737, 613.48761, 480.00000,   0.73089,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 150.6ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 43.11771,   8.88734, 613.32764, 462.36053,   0.67924,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.1ms pre-process, 161.1ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.17560e+01, 1.21268e+01, 6.31402e+02, 4.60437e+02, 6.02797e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 145.9ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 76.55508,  19.66267, 602.00708, 460.02863,   0.68414,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 bottle\n",
      "Speed: 3.0ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.41309e+01, 1.83931e+01, 6.09693e+02, 4.55137e+02, 6.74934e-01, 0.00000e+00],\n",
      "        [1.91925e+02, 2.53300e+02, 2.04108e+02, 2.80990e+02, 2.86995e-01, 3.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 tv\n",
      "Speed: 1.0ms pre-process, 141.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.70326e+02, 4.89250e-01, 3.62343e+02, 2.38240e+02, 3.63665e-01, 6.20000e+01],\n",
      "        [2.96566e+02, 3.54460e+00, 5.51938e+02, 4.03135e+02, 3.33429e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 151.2ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.01144e+01, 7.21860e+00, 6.39749e+02, 4.80000e+02, 5.21517e-01, 0.00000e+00],\n",
      "        [2.19665e+02, 2.43988e-02, 3.02159e+02, 1.44429e+02, 4.11343e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 150.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.87096e+01, 5.01595e+00, 6.31620e+02, 4.79675e+02, 7.13710e-01, 0.00000e+00],\n",
      "        [2.25476e+02, 6.54030e-01, 3.04384e+02, 1.31323e+02, 3.53159e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 145.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.16818e+01, 1.90964e-01, 6.37179e+02, 4.80000e+02, 6.25904e-01, 0.00000e+00],\n",
      "        [2.32826e+02, 1.97754e-02, 3.17839e+02, 1.53762e+02, 3.30010e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 153.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.80304e+01, 4.41681e-01, 6.20056e+02, 4.80000e+02, 6.24664e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 148.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.13447e+01, 0.00000e+00, 6.17307e+02, 4.80000e+02, 6.18157e-01, 0.00000e+00],\n",
      "        [2.40440e+02, 1.17463e+00, 3.53356e+02, 1.81810e+02, 2.62917e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 149.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 28.58405,   4.86162, 572.65625, 480.00000,   0.66194,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 155.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 52.34401,   0.00000, 551.58301, 480.00000,   0.62228,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 159.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.35831e+01, 0.00000e+00, 5.74471e+02, 4.80000e+02, 5.20266e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 147.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.53357e+01, 0.00000e+00, 5.56870e+02, 4.80000e+02, 4.99284e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 148.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 54.35071,   1.74132, 620.87787, 480.00000,   0.64960,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.3ms pre-process, 152.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 44.82828,   0.00000, 616.63696, 480.00000,   0.64180,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.5ms pre-process, 146.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 52.45862,   0.00000, 619.49872, 480.00000,   0.72400,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 137.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 67.64909,   0.00000, 547.97137, 480.00000,   0.61039,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 153.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.37693e+01, 0.00000e+00, 6.22849e+02, 4.80000e+02, 6.05538e-01, 0.00000e+00],\n",
      "        [1.15225e+02, 2.60446e+02, 1.95261e+02, 3.88113e+02, 2.91660e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 166.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 61.14984,   0.00000, 624.68848, 480.00000,   0.68242,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 170.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 71.08060,   0.00000, 608.44312, 480.00000,   0.74806,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 148.1ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.79597e+01, 0.00000e+00, 6.21460e+02, 4.80000e+02, 6.52621e-01, 0.00000e+00],\n",
      "        [1.14095e+02, 2.60668e+02, 1.95935e+02, 3.88874e+02, 3.23182e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 164.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 74.02869,   0.00000, 632.36475, 480.00000,   0.75315,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 146.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 72.08807,   0.00000, 601.17627, 480.00000,   0.77442,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 142.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 65.47485,   0.00000, 604.86328, 480.00000,   0.79419,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 145.5ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.14609e+01, 0.00000e+00, 5.98480e+02, 4.80000e+02, 8.06435e-01, 0.00000e+00],\n",
      "        [6.59846e+01, 2.59936e+02, 1.96832e+02, 4.59239e+02, 2.53954e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 148.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.73132e+01, 0.00000e+00, 5.95899e+02, 4.80000e+02, 8.15149e-01, 0.00000e+00],\n",
      "        [2.41560e+02, 5.80788e-01, 3.56019e+02, 1.81575e+02, 2.85641e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 65.51675,   0.00000, 596.44861, 480.00000,   0.79058,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 113.2ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.63906e+01, 1.28866e+01, 6.19694e+02, 4.72308e+02, 3.94406e-01, 0.00000e+00],\n",
      "        [3.33370e+02, 0.00000e+00, 6.22854e+02, 3.62673e+02, 3.51850e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.3ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.39047e+02, 1.18011e+00, 4.05802e+02, 1.81770e+02, 4.67754e-01, 6.20000e+01],\n",
      "        [6.67863e+01, 1.16522e+01, 6.13485e+02, 4.71799e+02, 4.63386e-01, 0.00000e+00],\n",
      "        [3.42438e+02, 0.00000e+00, 6.32869e+02, 3.96117e+02, 3.39396e-01, 0.00000e+00],\n",
      "        [2.49310e+01, 2.60280e+02, 1.97893e+02, 4.80000e+02, 3.31712e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.30893e+01, 1.26044e+01, 5.95797e+02, 4.66894e+02, 6.45283e-01, 0.00000e+00],\n",
      "        [2.38779e+02, 7.21313e-01, 3.97513e+02, 1.81526e+02, 3.80577e-01, 6.20000e+01],\n",
      "        [3.33432e+02, 9.00574e-01, 6.08482e+02, 3.58716e+02, 3.24072e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 131.2ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37396e+02, 7.85248e-01, 4.08913e+02, 1.82216e+02, 4.55338e-01, 6.20000e+01],\n",
      "        [5.83663e+01, 2.02968e+01, 6.10105e+02, 4.66882e+02, 3.95902e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 tvs\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.64811e+01, 3.44630e+00, 6.01319e+02, 4.72456e+02, 6.30714e-01, 0.00000e+00],\n",
      "        [2.08013e+02, 2.21610e+00, 6.15061e+02, 1.99263e+02, 3.82700e-01, 6.20000e+01],\n",
      "        [2.36319e+02, 3.37709e+00, 4.23687e+02, 1.87059e+02, 3.80675e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 bowl, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.66158e+02, 2.31684e+02, 3.62533e+02, 2.88302e+02, 5.47785e-01, 4.50000e+01],\n",
      "        [2.38452e+02, 1.45539e+00, 4.19671e+02, 1.80945e+02, 4.16522e-01, 6.20000e+01],\n",
      "        [3.45218e+02, 4.83398e-02, 5.88125e+02, 2.66579e+02, 3.82784e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 132.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.61214e+01, 1.15802e+01, 5.90192e+02, 4.71977e+02, 6.67049e-01, 0.00000e+00],\n",
      "        [2.19062e+02, 2.20432e+00, 5.89107e+02, 1.96766e+02, 2.89434e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.82491e+01, 7.66167e+00, 6.03440e+02, 4.75987e+02, 4.85705e-01, 0.00000e+00],\n",
      "        [3.28089e+02, 0.00000e+00, 6.27512e+02, 3.88576e+02, 2.67034e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 168.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.97165e+01, 7.99709e+00, 5.86063e+02, 4.80000e+02, 7.92242e-01, 0.00000e+00],\n",
      "        [4.55595e+01, 2.60671e+02, 1.99537e+02, 4.80000e+02, 2.52817e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 140.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.59657e+01, 3.95633e+00, 5.96765e+02, 4.74360e+02, 7.94904e-01, 0.00000e+00],\n",
      "        [2.39592e+02, 8.38524e-01, 3.80042e+02, 1.81567e+02, 2.90893e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 156.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.32426e+02, 1.92734e-01, 5.95809e+02, 2.48768e+02, 5.43258e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 147.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.27699e+02, 2.54860e-01, 5.96745e+02, 2.37791e+02, 6.15352e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 tv\n",
      "Speed: 2.0ms pre-process, 159.7ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.17247e+02, 0.00000e+00, 5.92965e+02, 2.30047e+02, 6.68945e-01, 0.00000e+00],\n",
      "        [4.82362e+01, 1.05956e+00, 6.12650e+02, 4.78768e+02, 3.35605e-01, 0.00000e+00],\n",
      "        [2.40473e+02, 8.72597e-01, 3.64756e+02, 1.83465e+02, 3.10169e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog, 1 tv\n",
      "Speed: 3.6ms pre-process, 137.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.15274e+02, 0.00000e+00, 5.89763e+02, 2.17844e+02, 6.47894e-01, 1.60000e+01],\n",
      "        [2.39944e+02, 4.03999e-01, 3.64560e+02, 1.81989e+02, 3.66653e-01, 6.20000e+01],\n",
      "        [4.16743e+01, 0.00000e+00, 6.09724e+02, 4.76577e+02, 2.74114e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 clock\n",
      "Speed: 3.0ms pre-process, 131.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.28367e+02, 1.84783e+00, 6.01873e+02, 2.05492e+02, 5.68196e-01, 0.00000e+00],\n",
      "        [4.52834e+02, 3.94970e+02, 5.93391e+02, 4.79918e+02, 2.82609e-01, 7.40000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 154.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.80663e+02, 4.60136e-01, 6.37337e+02, 1.74577e+02, 2.72836e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 135.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.57472e+02, 1.74484e-02, 6.39649e+02, 2.38872e+02, 3.21923e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 laptop\n",
      "Speed: 2.0ms pre-process, 139.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.86945e+02, 6.27930e-01, 6.39428e+02, 2.53704e+02, 3.58219e-01, 1.50000e+01],\n",
      "        [3.01896e+01, 8.52175e+01, 6.27997e+02, 4.79663e+02, 2.75158e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 152.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.17152e+02, 3.52837e+00, 6.39544e+02, 4.22355e+02, 2.51039e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 141.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 139.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.21627e+02, 2.64948e+00, 6.40000e+02, 1.67312e+02, 3.74141e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 142.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.10724e+02, 0.00000e+00, 6.40000e+02, 1.59483e+02, 3.96466e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 142.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.04646e+02, 0.00000e+00, 6.40000e+02, 1.74145e+02, 4.20051e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 138.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat\n",
      "Speed: 2.0ms pre-process, 143.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.94113e+02, 0.00000e+00, 6.39533e+02, 2.26054e+02, 3.92836e-01, 1.50000e+01],\n",
      "        [5.43267e+02, 2.10996e+02, 6.39676e+02, 4.57650e+02, 2.61442e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 141.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.75665e+02, 0.00000e+00, 6.40000e+02, 2.23347e+02, 4.78703e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat\n",
      "Speed: 2.0ms pre-process, 152.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.67849e+02, 2.75302e+00, 6.40000e+02, 2.30441e+02, 3.87396e-01, 1.50000e+01],\n",
      "        [5.37855e+02, 2.24706e+02, 6.39685e+02, 4.63925e+02, 3.18443e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 143.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.69175e+02, 1.60754e+00, 6.40000e+02, 2.34082e+02, 4.76970e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 3.0ms pre-process, 143.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.76040e+02, 6.54984e-02, 6.38779e+02, 6.80046e+01, 3.39517e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 3.0ms pre-process, 143.7ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.82805e+02, 1.01410e-01, 6.37867e+02, 6.88139e+01, 4.10695e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat\n",
      "Speed: 2.0ms pre-process, 142.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.84730e+02, 0.00000e+00, 6.38740e+02, 6.91945e+01, 5.68209e-01, 1.50000e+01],\n",
      "        [1.55819e+00, 2.30488e+02, 1.46357e+02, 4.79380e+02, 2.94192e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 2 cats\n",
      "Speed: 3.0ms pre-process, 138.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.80867e+02, 9.94980e-01, 6.40000e+02, 2.29370e+02, 4.88597e-01, 1.50000e+01],\n",
      "        [5.56414e+02, 2.40765e+02, 6.39946e+02, 4.77032e+02, 3.83223e-01, 0.00000e+00],\n",
      "        [7.66337e+01, 6.32271e+01, 6.40000e+02, 4.78677e+02, 3.03206e-01, 1.10000e+01],\n",
      "        [4.82854e+02, 2.51953e-01, 6.38217e+02, 7.89705e+01, 2.52239e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 stop sign, 1 cat\n",
      "Speed: 2.0ms pre-process, 144.3ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.06625e+02, 8.34816e-01, 6.40000e+02, 2.40554e+02, 3.17375e-01, 1.50000e+01],\n",
      "        [4.81413e+01, 6.68274e+01, 6.23532e+02, 4.79134e+02, 2.74859e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 6.0ms pre-process, 159.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.10851e+02, 0.00000e+00, 6.40000e+02, 1.57075e+02, 2.55679e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 stop sign\n",
      "Speed: 2.0ms pre-process, 139.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.87568e+01, 6.56084e+01, 6.21986e+02, 4.78339e+02, 2.90570e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat\n",
      "Speed: 2.0ms pre-process, 142.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.61237e+02, 2.35072e+02, 6.39857e+02, 4.75260e+02, 3.66814e-01, 0.00000e+00],\n",
      "        [5.11142e+02, 0.00000e+00, 6.40000e+02, 2.36805e+02, 3.09402e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat\n",
      "Speed: 2.0ms pre-process, 145.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.60310e+02, 2.36532e+02, 6.39749e+02, 4.75134e+02, 3.35286e-01, 0.00000e+00],\n",
      "        [5.08420e+02, 0.00000e+00, 6.40000e+02, 2.49090e+02, 2.57291e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 145.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.57359e+02, 2.37438e+02, 6.39756e+02, 4.76012e+02, 4.22046e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 cat\n",
      "Speed: 2.0ms pre-process, 144.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.57715e+02, 2.36464e+02, 6.40000e+02, 4.76436e+02, 3.43636e-01, 0.00000e+00],\n",
      "        [5.06221e+02, 1.18813e-01, 6.40000e+02, 2.42908e+02, 3.08714e-01, 1.50000e+01],\n",
      "        [4.34848e+01, 6.76251e+01, 6.30040e+02, 4.76989e+02, 2.74606e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 cat\n",
      "Speed: 2.0ms pre-process, 144.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.58508e+02, 2.34486e+02, 6.39789e+02, 4.75315e+02, 3.75691e-01, 0.00000e+00],\n",
      "        [5.07216e+02, 5.63431e-02, 6.40000e+02, 2.41088e+02, 3.49588e-01, 1.50000e+01],\n",
      "        [4.46323e+01, 6.51895e+01, 6.28962e+02, 4.72009e+02, 2.85034e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 3.0ms pre-process, 138.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.09951e+02, 9.46777e-01, 6.40000e+02, 2.32963e+02, 3.02520e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 142.6ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.02444e+02, 6.96304e-01, 6.38954e+02, 9.26598e+01, 3.90710e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 3.0ms pre-process, 155.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.87349e+02, 3.65757e-01, 6.38714e+02, 8.07934e+01, 5.38968e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 clocks\n",
      "Speed: 2.0ms pre-process, 135.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30743e+02, 3.21070e+02, 3.84789e+02, 4.77591e+02, 4.90467e-01, 7.40000e+01],\n",
      "        [4.81626e+02, 1.74516e+02, 6.40000e+02, 3.40208e+02, 3.50892e-01, 7.40000e+01],\n",
      "        [4.09421e+02, 7.85698e-01, 6.38547e+02, 1.54972e+02, 3.22803e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 3.0ms pre-process, 138.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.67804e+02, 4.29771e-01, 6.33867e+02, 1.80094e+02, 5.36753e-01, 0.00000e+00],\n",
      "        [3.66089e+02, 2.17010e-01, 6.35869e+02, 1.81077e+02, 3.75701e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog, 1 remote\n",
      "Speed: 4.0ms pre-process, 140.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.30246e+02, 2.30515e-01, 5.96378e+02, 1.94219e+02, 5.21894e-01, 1.60000e+01],\n",
      "        [5.12570e+01, 0.00000e+00, 6.23495e+02, 4.78629e+02, 2.97443e-01, 0.00000e+00],\n",
      "        [1.55974e+02, 3.13761e+02, 2.99161e+02, 4.42952e+02, 2.58748e-01, 6.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 2.0ms pre-process, 150.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.21550e+02, 3.19687e-01, 5.92711e+02, 1.97298e+02, 5.68831e-01, 1.60000e+01],\n",
      "        [7.18762e+01, 0.00000e+00, 6.16548e+02, 4.76890e+02, 5.42400e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 remote\n",
      "Speed: 3.0ms pre-process, 149.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.28098e+02, 0.00000e+00, 5.98361e+02, 1.93644e+02, 4.88478e-01, 1.60000e+01],\n",
      "        [1.26388e+02, 2.64986e+02, 2.94297e+02, 4.79588e+02, 2.61782e-01, 6.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 remote\n",
      "Speed: 2.0ms pre-process, 152.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.35145e+02, 8.20045e-01, 6.00667e+02, 1.86613e+02, 5.88690e-01, 1.60000e+01],\n",
      "        [1.33047e+02, 2.84160e+02, 2.97241e+02, 4.75461e+02, 2.50528e-01, 6.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat, 1 dog, 1 tv\n",
      "Speed: 3.0ms pre-process, 147.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.42076e+02, 9.25606e-01, 6.18314e+02, 1.88931e+02, 3.33637e-01, 1.60000e+01],\n",
      "        [3.43282e+02, 6.42792e-01, 6.16074e+02, 1.86978e+02, 3.08530e-01, 1.50000e+01],\n",
      "        [2.19388e+02, 0.00000e+00, 6.20715e+02, 1.85235e+02, 3.05445e-01, 6.20000e+01],\n",
      "        [6.55752e+01, 2.60378e+02, 3.38108e+02, 4.78537e+02, 2.57413e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 tv\n",
      "Speed: 3.0ms pre-process, 143.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.59335e+02, 5.43716e-01, 6.25387e+02, 1.76272e+02, 5.86121e-01, 0.00000e+00],\n",
      "        [2.37883e+02, 9.48196e-01, 4.02986e+02, 1.80318e+02, 4.53481e-01, 6.20000e+01],\n",
      "        [5.11068e+01, 2.59147e+02, 3.54642e+02, 4.77656e+02, 2.92987e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 3.0ms pre-process, 136.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58833e+02, 4.39522e-01, 6.25627e+02, 1.75876e+02, 5.62766e-01, 1.60000e+01],\n",
      "        [6.12360e+02, 3.29189e+02, 6.39739e+02, 4.79301e+02, 3.25213e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 2.0ms pre-process, 142.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.48795e+02, 2.56927e-01, 6.22840e+02, 1.76821e+02, 7.16997e-01, 1.60000e+01],\n",
      "        [5.97600e+02, 3.17627e+02, 6.39782e+02, 4.79877e+02, 3.63500e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 5.0ms pre-process, 143.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.52048e+02, 5.22736e-01, 6.24855e+02, 1.79832e+02, 6.73152e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 153.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.54216e+02, 7.60506e-01, 6.30901e+02, 1.82654e+02, 4.94445e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 142.6ms inference, 2.8ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58034e+02, 8.20663e-01, 6.32019e+02, 1.88111e+02, 4.50163e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 3.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[359.69345,   0.67749, 632.88428, 186.49887,   0.73462,  16.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 124.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58829e+02, 1.10382e+00, 6.33212e+02, 1.88003e+02, 4.40228e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 3.0ms pre-process, 115.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57437e+02, 7.19978e-01, 6.32508e+02, 1.89636e+02, 3.66478e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 120.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57320e+02, 4.56497e-01, 6.31457e+02, 1.92703e+02, 3.68846e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57863e+02, 4.18793e-01, 6.32309e+02, 1.96889e+02, 4.51040e-01, 0.00000e+00],\n",
      "        [6.15150e+02, 3.57964e+02, 6.39850e+02, 4.79607e+02, 2.79950e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57927e+02, 6.63834e-01, 6.33285e+02, 1.98307e+02, 5.75308e-01, 1.60000e+01],\n",
      "        [6.15649e+02, 3.57708e+02, 6.39854e+02, 4.80000e+02, 2.86102e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 114.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.56305e+02, 2.17545e-01, 6.33052e+02, 2.02033e+02, 6.00668e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58157e+02, 6.13632e-01, 6.32752e+02, 1.98198e+02, 7.58294e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 dog\n",
      "Speed: 2.0ms pre-process, 119.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.55263e+02, 6.68777e-01, 6.30895e+02, 1.97311e+02, 7.35437e-01, 1.60000e+01],\n",
      "        [1.15595e+02, 1.75282e+02, 6.34731e+02, 4.80000e+02, 2.55315e-01, 1.10000e+01],\n",
      "        [6.18377e+02, 3.61745e+02, 6.39906e+02, 4.78389e+02, 2.50119e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 stop sign, 1 dog\n",
      "Speed: 4.0ms pre-process, 134.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.56977e+02, 0.00000e+00, 6.34285e+02, 2.05246e+02, 6.33315e-01, 1.60000e+01],\n",
      "        [1.02266e+02, 1.72222e+02, 6.29953e+02, 4.80000e+02, 2.54954e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 tv\n",
      "Speed: 3.0ms pre-process, 132.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.46764e+02, 0.00000e+00, 6.26336e+02, 2.25582e+02, 4.57545e-01, 1.60000e+01],\n",
      "        [2.37544e+02, 0.00000e+00, 4.15828e+02, 1.84364e+02, 3.91298e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[9.42570e+01, 0.00000e+00, 6.35164e+02, 4.78829e+02, 4.48612e-01, 0.00000e+00],\n",
      "        [2.36883e+02, 1.38211e+00, 4.00370e+02, 1.83510e+02, 4.36414e-01, 6.20000e+01],\n",
      "        [3.43801e+02, 2.65732e-01, 6.20389e+02, 2.65804e+02, 3.33886e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 132.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[9.90612e+01, 9.00406e-01, 6.40000e+02, 4.73321e+02, 6.28264e-01, 0.00000e+00],\n",
      "        [2.38467e+02, 7.59361e-01, 4.10161e+02, 1.83032e+02, 4.25097e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.06366e+02, 3.25464e+00, 6.40000e+02, 4.75779e+02, 6.50328e-01, 0.00000e+00],\n",
      "        [2.39651e+02, 9.57687e-01, 4.18719e+02, 1.82389e+02, 3.85206e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 tvs\n",
      "Speed: 2.0ms pre-process, 123.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.14012e+02, 4.79126e-03, 6.32444e+02, 4.80000e+02, 8.30445e-01, 0.00000e+00],\n",
      "        [2.16037e+02, 2.51032e+00, 6.26990e+02, 2.00369e+02, 2.79598e-01, 6.20000e+01],\n",
      "        [2.36163e+02, 3.23984e+00, 4.28076e+02, 1.84054e+02, 2.51805e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 tvs\n",
      "Speed: 2.0ms pre-process, 117.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36809e+02, 3.62769e+00, 4.34071e+02, 1.84918e+02, 4.22728e-01, 6.20000e+01],\n",
      "        [9.75565e+01, 0.00000e+00, 6.23921e+02, 4.80000e+02, 4.07293e-01, 0.00000e+00],\n",
      "        [2.33442e+02, 1.41617e+00, 6.34469e+02, 2.01877e+02, 2.90952e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 dog, 1 tv\n",
      "Speed: 1.5ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35533e+02, 1.20303e+00, 4.70191e+02, 1.83027e+02, 5.04760e-01, 6.20000e+01],\n",
      "        [4.21817e+02, 0.00000e+00, 6.38940e+02, 3.66923e+02, 3.46651e-01, 1.50000e+01],\n",
      "        [4.20457e+02, 0.00000e+00, 6.40000e+02, 3.84354e+02, 3.06896e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 117.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29225e+01, 2.58560e+02, 2.61887e+02, 4.80000e+02, 6.48415e-01, 5.60000e+01],\n",
      "        [2.31025e+02, 3.25555e+00, 4.69779e+02, 1.89739e+02, 4.36631e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 107.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.60021e+01, 2.58641e+02, 2.63729e+02, 4.80000e+02, 5.27679e-01, 5.60000e+01],\n",
      "        [2.43191e+02, 7.98810e+00, 6.39670e+02, 4.74526e+02, 4.99102e-01, 0.00000e+00],\n",
      "        [2.27444e+02, 2.76060e+00, 4.73022e+02, 1.94931e+02, 4.13400e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.65274e+02, 3.43408e+00, 6.40000e+02, 4.76704e+02, 6.64199e-01, 0.00000e+00],\n",
      "        [1.99292e+01, 2.59066e+02, 2.65023e+02, 4.79740e+02, 6.03245e-01, 5.60000e+01],\n",
      "        [2.35013e+02, 1.19318e+00, 4.64022e+02, 1.85463e+02, 3.99030e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.45968e+02, 4.63443e+00, 6.38008e+02, 4.75682e+02, 6.75973e-01, 0.00000e+00],\n",
      "        [2.62551e+01, 2.59084e+02, 2.63326e+02, 4.79869e+02, 5.90312e-01, 5.60000e+01],\n",
      "        [2.34485e+02, 1.48169e+00, 4.63325e+02, 1.86391e+02, 4.27227e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 99.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.81591e+01, 2.58242e+02, 2.63832e+02, 4.80000e+02, 6.20181e-01, 5.60000e+01],\n",
      "        [2.32954e+02, 1.93488e+00, 4.62222e+02, 1.85555e+02, 4.85034e-01, 6.20000e+01],\n",
      "        [2.33452e+02, 6.05362e+00, 6.39823e+02, 4.68249e+02, 3.88917e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.10906e+01, 2.58308e+02, 2.60772e+02, 4.80000e+02, 7.10693e-01, 5.60000e+01],\n",
      "        [2.32949e+02, 1.65694e+00, 4.62756e+02, 1.85026e+02, 4.28589e-01, 6.20000e+01],\n",
      "        [2.79318e+02, 2.03990e+01, 6.40000e+02, 4.76525e+02, 3.61154e-01, 1.50000e+01],\n",
      "        [2.45606e+02, 1.94454e+01, 6.39559e+02, 4.72861e+02, 2.74787e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 couch, 1 tv\n",
      "Speed: 3.0ms pre-process, 134.5ms inference, 5.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.26094e+01, 2.58332e+02, 2.60130e+02, 4.80000e+02, 5.14099e-01, 5.60000e+01],\n",
      "        [2.31001e+02, 3.09016e+00, 4.68233e+02, 1.90055e+02, 5.05510e-01, 6.20000e+01],\n",
      "        [2.84656e+02, 2.34513e+02, 6.34606e+02, 4.78723e+02, 4.73412e-01, 5.70000e+01],\n",
      "        [2.54038e+02, 1.50532e+01, 6.40000e+02, 4.71058e+02, 3.83418e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 couch, 1 tv\n",
      "Speed: 3.0ms pre-process, 124.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.15272e+01, 2.58657e+02, 2.59866e+02, 4.79932e+02, 7.14179e-01, 5.60000e+01],\n",
      "        [2.50299e+02, 1.15509e+01, 6.36398e+02, 4.75770e+02, 4.45467e-01, 0.00000e+00],\n",
      "        [2.85085e+02, 2.40583e+02, 6.33154e+02, 4.78612e+02, 4.32523e-01, 5.70000e+01],\n",
      "        [2.33960e+02, 1.61042e+00, 4.85469e+02, 2.25393e+02, 4.20168e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.46532e+01, 2.58250e+02, 2.60477e+02, 4.80000e+02, 6.98747e-01, 5.60000e+01],\n",
      "        [2.29558e+02, 2.91924e+00, 4.69817e+02, 2.02387e+02, 4.81378e-01, 6.20000e+01],\n",
      "        [4.77177e+02, 3.35968e-01, 6.39850e+02, 2.63869e+02, 3.00333e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 couch, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.99527e+01, 2.59192e+02, 2.59803e+02, 4.80000e+02, 5.22958e-01, 5.60000e+01],\n",
      "        [2.42424e+02, 6.73141e+00, 6.34877e+02, 4.77558e+02, 4.61239e-01, 0.00000e+00],\n",
      "        [2.27145e+02, 2.82749e+00, 4.66421e+02, 1.90234e+02, 3.57581e-01, 6.20000e+01],\n",
      "        [2.65556e+02, 2.50205e+02, 6.35133e+02, 4.77002e+02, 2.59707e-01, 5.70000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.23492e+01, 2.58843e+02, 2.61517e+02, 4.80000e+02, 5.65016e-01, 5.60000e+01],\n",
      "        [2.30311e+02, 2.65404e+00, 4.63956e+02, 1.89234e+02, 4.84288e-01, 6.20000e+01],\n",
      "        [2.59182e+02, 0.00000e+00, 6.37734e+02, 4.78990e+02, 4.48308e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36055e+02, 2.27729e+00, 4.65489e+02, 1.82043e+02, 4.59366e-01, 6.20000e+01],\n",
      "        [4.10447e+02, 1.86859e-01, 6.37418e+02, 4.00124e+02, 3.35784e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 tv\n",
      "Speed: 4.0ms pre-process, 130.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.01792e+02, 1.72424e-01, 6.19024e+02, 2.26025e+02, 4.56487e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 1.5ms pre-process, 118.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.88251e+01, 0.00000e+00, 6.40000e+02, 4.76780e+02, 4.18163e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.35525e+01, 2.39047e+00, 5.80376e+02, 4.76424e+02, 6.82084e-01, 0.00000e+00],\n",
      "        [2.40265e+02, 5.34622e-01, 3.71998e+02, 1.82222e+02, 3.71788e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 tv\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37324e+02, 9.60533e-01, 4.02684e+02, 1.85008e+02, 2.94003e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.17353e+02, 0.00000e+00, 6.20282e+02, 4.70258e+02, 3.62810e-01, 0.00000e+00],\n",
      "        [2.36466e+02, 2.74658e-03, 4.10019e+02, 1.82664e+02, 3.08861e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.94909e+01, 9.51776e+00, 6.15548e+02, 4.73274e+02, 5.60151e-01, 0.00000e+00],\n",
      "        [2.39005e+02, 1.01822e+00, 3.99080e+02, 1.81339e+02, 3.91257e-01, 6.20000e+01],\n",
      "        [5.48307e+01, 2.59079e+02, 2.01060e+02, 4.75064e+02, 2.70134e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 5.0ms pre-process, 129.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 87.54694,   1.58716, 633.40265, 480.00000,   0.66134,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.98821e+01, 0.00000e+00, 6.35870e+02, 4.80000e+02, 5.44601e-01, 0.00000e+00],\n",
      "        [2.38770e+02, 1.08787e+00, 4.16929e+02, 1.81335e+02, 3.78203e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 1.0ms pre-process, 118.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.01202e+02, 0.00000e+00, 6.28741e+02, 4.80000e+02, 7.19783e-01, 0.00000e+00],\n",
      "        [2.37866e+02, 1.21562e+00, 4.08880e+02, 1.78774e+02, 4.19724e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.95708e+01, 0.00000e+00, 6.30554e+02, 4.80000e+02, 6.08042e-01, 0.00000e+00],\n",
      "        [2.33196e+02, 1.59310e+00, 4.68114e+02, 1.82808e+02, 4.88265e-01, 6.20000e+01],\n",
      "        [3.72953e+02, 7.06464e+00, 6.40000e+02, 4.80000e+02, 2.91248e-01, 0.00000e+00],\n",
      "        [2.65077e+01, 2.61110e+02, 1.96502e+02, 4.78140e+02, 2.75157e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 114.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30885e+02, 2.14342e+00, 4.62731e+02, 1.87321e+02, 4.55455e-01, 6.20000e+01],\n",
      "        [8.27824e+01, 4.81825e+00, 6.38721e+02, 4.77120e+02, 3.92969e-01, 0.00000e+00],\n",
      "        [6.25811e+01, 2.63158e+02, 1.97720e+02, 4.80000e+02, 2.52657e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32953e+02, 1.67738e+00, 4.63747e+02, 1.89702e+02, 5.53031e-01, 6.20000e+01],\n",
      "        [1.50935e+02, 4.02776e+01, 6.40000e+02, 4.80000e+02, 2.50040e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30408e+02, 1.48991e+00, 4.64162e+02, 1.87368e+02, 4.69956e-01, 6.20000e+01],\n",
      "        [2.12922e+01, 2.60321e+02, 2.02514e+02, 4.78664e+02, 3.83245e-01, 5.60000e+01],\n",
      "        [8.06601e+01, 1.01242e+00, 6.36222e+02, 4.80000e+02, 2.86530e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 1.0ms pre-process, 131.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35722e+02, 1.82455e+00, 4.56343e+02, 1.83191e+02, 6.60094e-01, 6.20000e+01],\n",
      "        [8.26845e+01, 1.33859e+00, 6.36255e+02, 4.79075e+02, 3.28176e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 121.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38195e+02, 2.97974e+00, 4.67210e+02, 1.81733e+02, 4.43938e-01, 6.20000e+01],\n",
      "        [1.63770e+02, 0.00000e+00, 6.38562e+02, 4.80000e+02, 3.10350e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv, 1 remote\n",
      "Speed: 4.0ms pre-process, 108.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.47806e+02, 3.59287e+02, 6.39662e+02, 4.30197e+02, 6.79636e-01, 6.50000e+01],\n",
      "        [1.52715e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 6.53115e-01, 0.00000e+00],\n",
      "        [2.31841e+02, 2.95840e+00, 6.36952e+02, 2.00504e+02, 3.71772e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 114.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.03820e+02, 0.00000e+00, 6.32444e+02, 4.80000e+02, 7.47477e-01, 0.00000e+00],\n",
      "        [2.46194e+01, 2.59734e+02, 2.02356e+02, 4.78359e+02, 4.19590e-01, 5.60000e+01],\n",
      "        [2.31197e+02, 2.84382e+00, 6.33347e+02, 1.99609e+02, 3.84781e-01, 6.20000e+01],\n",
      "        [2.36940e+02, 2.78029e+00, 4.29080e+02, 1.82895e+02, 2.67119e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.1ms pre-process, 112.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.10466e+02, 0.00000e+00, 6.37416e+02, 4.80000e+02, 6.83370e-01, 0.00000e+00],\n",
      "        [2.13940e+01, 2.60078e+02, 2.00969e+02, 4.78966e+02, 3.84536e-01, 5.60000e+01],\n",
      "        [2.13319e+02, 2.65804e+00, 6.26026e+02, 1.98080e+02, 2.54280e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cup, 1 chair\n",
      "Speed: 2.0ms pre-process, 105.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.42061e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 7.50499e-01, 0.00000e+00],\n",
      "        [2.53072e+01, 2.60874e+02, 1.98078e+02, 4.78933e+02, 3.70905e-01, 5.60000e+01],\n",
      "        [5.57607e+02, 3.67453e+02, 6.40000e+02, 4.67483e+02, 3.12873e-01, 4.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 1.6ms pre-process, 110.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.17992e+02, 0.00000e+00, 6.36942e+02, 4.80000e+02, 7.09513e-01, 0.00000e+00],\n",
      "        [2.46344e+01, 2.59861e+02, 1.97046e+02, 4.79683e+02, 3.36070e-01, 5.60000e+01],\n",
      "        [3.55105e+02, 0.00000e+00, 6.31141e+02, 3.40327e+02, 2.87127e-01, 0.00000e+00],\n",
      "        [2.37342e+02, 9.79607e-01, 4.12272e+02, 1.80869e+02, 2.76821e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.49391e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 7.38216e-01, 0.00000e+00],\n",
      "        [2.13982e+02, 1.89628e+00, 6.16628e+02, 1.98088e+02, 3.19876e-01, 6.20000e+01],\n",
      "        [5.10780e+01, 2.59863e+02, 2.06550e+02, 4.79246e+02, 3.14205e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.20377e+02, 1.46556e+00, 6.34833e+02, 4.80000e+02, 7.00935e-01, 0.00000e+00],\n",
      "        [1.93551e+01, 2.60327e+02, 1.99690e+02, 4.78492e+02, 3.10642e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 107.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.14956e+02, 0.00000e+00, 6.33330e+02, 4.80000e+02, 7.08651e-01, 0.00000e+00],\n",
      "        [3.82802e+01, 2.59803e+02, 2.06677e+02, 4.79584e+02, 3.20028e-01, 5.60000e+01],\n",
      "        [2.13152e+02, 2.66624e+00, 6.16239e+02, 1.95496e+02, 2.64889e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 133.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.40919e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 6.74634e-01, 0.00000e+00],\n",
      "        [2.14265e+02, 1.29471e+00, 6.24997e+02, 2.06547e+02, 3.12912e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 remote\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.22032e+02, 0.00000e+00, 6.33317e+02, 4.80000e+02, 7.90787e-01, 0.00000e+00],\n",
      "        [5.36566e+02, 3.45839e+02, 6.39950e+02, 4.78794e+02, 3.76538e-01, 6.50000e+01],\n",
      "        [2.09497e+01, 2.60801e+02, 2.01644e+02, 4.79055e+02, 3.15183e-01, 5.60000e+01],\n",
      "        [2.38996e+02, 9.31320e-01, 4.12824e+02, 1.82120e+02, 2.92068e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.00300e+02, 0.00000e+00, 6.35858e+02, 4.80000e+02, 6.17653e-01, 0.00000e+00],\n",
      "        [2.36448e+01, 2.59430e+02, 1.99354e+02, 4.79074e+02, 5.04499e-01, 5.60000e+01],\n",
      "        [2.38460e+02, 6.03233e-01, 4.17655e+02, 1.82058e+02, 3.25676e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 130.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[9.42277e+01, 0.00000e+00, 6.30427e+02, 4.80000e+02, 5.75520e-01, 0.00000e+00],\n",
      "        [2.60677e+01, 2.60518e+02, 1.98836e+02, 4.78961e+02, 5.33294e-01, 5.60000e+01],\n",
      "        [2.39063e+02, 1.34908e+00, 4.14919e+02, 1.81741e+02, 4.17801e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.13971e+02, 0.00000e+00, 6.22672e+02, 4.80000e+02, 6.20100e-01, 0.00000e+00],\n",
      "        [2.38218e+02, 1.07339e+00, 4.21811e+02, 1.82700e+02, 2.51826e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.06582e+02, 2.33765e-01, 6.24790e+02, 4.80000e+02, 6.80934e-01, 0.00000e+00],\n",
      "        [2.38121e+02, 1.35627e+00, 4.23596e+02, 1.81379e+02, 2.98317e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.9ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.82205e+01, 0.00000e+00, 6.29123e+02, 4.80000e+02, 6.37078e-01, 0.00000e+00],\n",
      "        [2.57444e+01, 2.60574e+02, 1.98792e+02, 4.78124e+02, 3.26855e-01, 5.60000e+01],\n",
      "        [2.27997e+02, 3.70000e+00, 6.33342e+02, 2.03380e+02, 3.06712e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.1ms pre-process, 120.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36233e+02, 4.61744e+00, 6.40000e+02, 2.06989e+02, 4.50474e-01, 6.20000e+01],\n",
      "        [1.66906e+02, 6.56998e+00, 6.40000e+02, 4.80000e+02, 3.17184e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36424e+02, 2.02299e+00, 4.60175e+02, 1.84992e+02, 5.64709e-01, 6.20000e+01],\n",
      "        [8.40617e+01, 5.10626e+00, 6.40000e+02, 4.79954e+02, 4.81380e-01, 0.00000e+00],\n",
      "        [2.33695e+01, 2.60438e+02, 2.02651e+02, 4.79078e+02, 2.78781e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 137.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35864e+02, 2.14423e+00, 4.81296e+02, 1.81885e+02, 5.31895e-01, 6.20000e+01],\n",
      "        [2.08007e+02, 7.87064e-01, 6.38696e+02, 4.80000e+02, 4.83187e-01, 0.00000e+00],\n",
      "        [2.20426e+01, 2.59489e+02, 2.41518e+02, 4.78554e+02, 3.96928e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38137e+02, 1.43114e+00, 4.77719e+02, 1.84909e+02, 4.63581e-01, 6.20000e+01],\n",
      "        [1.57305e+02, 6.40952e+00, 6.40000e+02, 4.80000e+02, 4.50358e-01, 0.00000e+00],\n",
      "        [2.95813e+01, 2.59372e+02, 2.41200e+02, 4.78390e+02, 3.76883e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 133.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38840e+02, 2.06001e+00, 4.65178e+02, 1.81359e+02, 5.24629e-01, 6.20000e+01],\n",
      "        [1.53309e+02, 8.77643e+00, 6.40000e+02, 4.79937e+02, 3.88040e-01, 0.00000e+00],\n",
      "        [3.88651e+01, 2.59200e+02, 2.45422e+02, 4.78842e+02, 3.00130e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.1ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34093e+02, 1.60549e+00, 4.58825e+02, 1.86979e+02, 5.88458e-01, 6.20000e+01],\n",
      "        [2.98594e+01, 2.59312e+02, 2.46791e+02, 4.76161e+02, 3.64261e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 143.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.28519e+02, 2.34616e+00, 4.65203e+02, 1.89102e+02, 3.67533e-01, 6.20000e+01],\n",
      "        [3.23716e+01, 2.59297e+02, 2.41315e+02, 4.79292e+02, 3.64771e-01, 5.60000e+01],\n",
      "        [1.95806e+02, 0.00000e+00, 6.36328e+02, 4.80000e+02, 3.31826e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 tv\n",
      "Speed: 3.0ms pre-process, 136.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36316e+02, 1.33511e+00, 4.60238e+02, 1.86286e+02, 5.88462e-01, 6.20000e+01],\n",
      "        [4.13879e+02, 4.46704e+00, 6.39798e+02, 4.11700e+02, 4.36393e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 133.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.27139e+02, 4.57885e+00, 5.99147e+02, 1.94884e+02, 3.24843e-01, 6.20000e+01],\n",
      "        [8.70914e+01, 1.89445e+01, 6.31960e+02, 4.77707e+02, 2.72813e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 142.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 68.88599,   6.67822, 640.00000, 480.00000,   0.68184,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 138.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 79.92883,   4.75185, 640.00000, 478.21515,   0.74842,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 142.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.66233e+01, 9.16096e+00, 6.37804e+02, 4.78583e+02, 5.76304e-01, 0.00000e+00],\n",
      "        [2.53186e+01, 2.59539e+02, 1.96525e+02, 4.78966e+02, 2.68828e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 155.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 90.39752,   0.00000, 636.32227, 479.72537,   0.81899,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 135.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.74409e+01, 0.00000e+00, 6.35633e+02, 4.80000e+02, 6.58168e-01, 0.00000e+00],\n",
      "        [2.39625e+01, 2.60297e+02, 1.98363e+02, 4.79134e+02, 2.97357e-01, 5.60000e+01],\n",
      "        [2.37699e+02, 1.29204e+00, 4.24750e+02, 1.81033e+02, 2.63644e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 135.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34131e+02, 2.60188e+00, 4.75759e+02, 1.79401e+02, 3.94073e-01, 6.20000e+01],\n",
      "        [4.00292e+02, 5.91406e+00, 6.38915e+02, 3.60632e+02, 3.18491e-01, 1.50000e+01],\n",
      "        [2.20461e+01, 2.60268e+02, 2.06055e+02, 4.77279e+02, 2.82151e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 136.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.39136e+02, 1.28178e+00, 4.62780e+02, 1.83336e+02, 4.65240e-01, 6.20000e+01],\n",
      "        [1.64135e+02, 1.21586e+01, 6.40000e+02, 4.73924e+02, 4.39053e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34033e+02, 1.73968e+00, 4.81624e+02, 2.03206e+02, 4.39686e-01, 6.20000e+01],\n",
      "        [1.91935e+02, 4.46617e+00, 6.40000e+02, 4.76308e+02, 2.58222e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35255e+02, 1.12137e+00, 4.91633e+02, 1.90217e+02, 4.98658e-01, 6.20000e+01],\n",
      "        [8.75935e+01, 1.09365e+01, 6.38996e+02, 4.69340e+02, 4.64892e-01, 0.00000e+00],\n",
      "        [2.91788e+01, 2.59771e+02, 2.41479e+02, 4.78115e+02, 2.54877e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38821e+02, 1.83681e+00, 4.63926e+02, 1.80917e+02, 6.39581e-01, 6.20000e+01],\n",
      "        [1.66937e+02, 8.19402e+00, 6.40000e+02, 4.73425e+02, 3.43087e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37875e+02, 2.74968e+00, 4.73229e+02, 1.80851e+02, 5.47167e-01, 6.20000e+01],\n",
      "        [1.65760e+02, 1.71936e-01, 6.40000e+02, 4.80000e+02, 5.44027e-01, 0.00000e+00],\n",
      "        [4.43613e+01, 2.59404e+02, 2.29225e+02, 4.79463e+02, 2.96778e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 102.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35782e+02, 3.03975e+00, 4.75472e+02, 1.81657e+02, 5.69342e-01, 6.20000e+01],\n",
      "        [1.45953e+02, 4.06715e+00, 6.40000e+02, 4.80000e+02, 4.52077e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36860e+02, 1.82161e+00, 4.79112e+02, 1.82042e+02, 5.50531e-01, 6.20000e+01],\n",
      "        [1.42732e+02, 1.75876e+00, 6.40000e+02, 4.80000e+02, 4.51815e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38880e+02, 2.47595e+00, 4.92581e+02, 1.87664e+02, 5.84839e-01, 6.20000e+01],\n",
      "        [1.53512e+02, 2.09799e+01, 6.40000e+02, 4.80000e+02, 4.37454e-01, 0.00000e+00],\n",
      "        [2.71629e+01, 2.59452e+02, 2.34110e+02, 4.77834e+02, 3.44259e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.5ms pre-process, 148.0ms inference, 1.7ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37728e+02, 1.26189e+00, 4.77647e+02, 1.84130e+02, 5.44809e-01, 6.20000e+01],\n",
      "        [1.54517e+02, 2.90286e+01, 6.40000e+02, 4.80000e+02, 3.59878e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 119.5ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.51153e+02, 7.52029e-01, 6.40000e+02, 4.79226e+02, 4.99317e-01, 0.00000e+00],\n",
      "        [2.32943e+02, 2.44821e+00, 4.61754e+02, 1.93364e+02, 4.65684e-01, 6.20000e+01],\n",
      "        [3.39759e+01, 2.59813e+02, 2.34419e+02, 4.78363e+02, 3.63369e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36580e+02, 1.86729e+00, 4.76649e+02, 1.81582e+02, 5.21443e-01, 6.20000e+01],\n",
      "        [1.61555e+02, 2.61119e+01, 6.40000e+02, 4.80000e+02, 3.38804e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 126.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38070e+02, 2.29996e+00, 4.79653e+02, 1.82461e+02, 5.41122e-01, 6.20000e+01],\n",
      "        [1.57137e+02, 6.33820e-01, 6.40000e+02, 4.80000e+02, 4.18894e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 108.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.64582e+01, 2.59496e+02, 2.33277e+02, 4.79372e+02, 5.19348e-01, 5.60000e+01],\n",
      "        [2.38780e+02, 2.18954e+00, 4.74581e+02, 1.83595e+02, 4.90612e-01, 6.20000e+01],\n",
      "        [1.61315e+02, 3.77353e+00, 6.40000e+02, 4.80000e+02, 3.97058e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 129.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35242e+02, 2.97954e+00, 4.72039e+02, 1.80180e+02, 5.53175e-01, 6.20000e+01],\n",
      "        [1.52998e+02, 7.75011e+00, 6.40000e+02, 4.78475e+02, 4.82506e-01, 0.00000e+00],\n",
      "        [3.07556e+01, 2.59576e+02, 2.33249e+02, 4.79173e+02, 3.36273e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 135.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.47046e+02, 2.19022e+01, 6.40000e+02, 4.80000e+02, 5.41859e-01, 0.00000e+00],\n",
      "        [2.36497e+02, 1.91870e+00, 4.66428e+02, 1.83160e+02, 4.99748e-01, 6.20000e+01],\n",
      "        [3.07360e+01, 2.58620e+02, 2.29287e+02, 4.79262e+02, 2.71927e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.67560e+02, 9.64397e+00, 6.40000e+02, 4.71100e+02, 4.59493e-01, 0.00000e+00],\n",
      "        [2.33566e+02, 2.93658e+00, 4.63117e+02, 1.98382e+02, 3.98952e-01, 6.20000e+01],\n",
      "        [3.29546e+01, 2.59198e+02, 2.30752e+02, 4.77937e+02, 3.65084e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.00002e+02, 6.47379e+00, 6.23342e+02, 4.77856e+02, 4.88965e-01, 0.00000e+00],\n",
      "        [2.31650e+02, 2.31602e+00, 4.66018e+02, 1.93040e+02, 4.50889e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[9.51113e+01, 0.00000e+00, 6.25405e+02, 4.80000e+02, 5.04055e-01, 0.00000e+00],\n",
      "        [2.31700e+02, 2.22061e+00, 4.64170e+02, 1.92420e+02, 4.81507e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29776e+02, 2.44492e+00, 4.64492e+02, 1.92923e+02, 5.12283e-01, 6.20000e+01],\n",
      "        [1.02218e+02, 0.00000e+00, 6.28391e+02, 4.80000e+02, 4.26294e-01, 0.00000e+00],\n",
      "        [2.26596e+01, 2.59252e+02, 2.00058e+02, 4.78953e+02, 3.68239e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35582e+02, 2.13076e+00, 4.58673e+02, 1.83958e+02, 6.16958e-01, 6.20000e+01],\n",
      "        [8.93237e+01, 6.18169e+00, 6.32939e+02, 4.77661e+02, 4.46970e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 130.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33123e+02, 1.94762e+00, 4.54752e+02, 1.84655e+02, 4.28689e-01, 6.20000e+01],\n",
      "        [1.44629e+02, 6.06094e-01, 6.40000e+02, 4.80000e+02, 3.74824e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58211e+02, 2.25191e+00, 6.39103e+02, 3.47769e+02, 5.33147e-01, 1.50000e+01],\n",
      "        [2.40767e+02, 1.68544e+00, 4.20270e+02, 1.82324e+02, 4.84481e-01, 6.20000e+01],\n",
      "        [1.33638e+02, 2.48064e+02, 6.30572e+02, 4.76430e+02, 2.59973e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 139.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.12941e+02, 0.00000e+00, 6.00656e+02, 4.80000e+02, 7.03805e-01, 0.00000e+00],\n",
      "        [2.21464e+02, 2.76394e+00, 5.97986e+02, 1.97312e+02, 4.04473e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.10873e+01, 2.73543e+00, 6.05435e+02, 4.75008e+02, 7.83892e-01, 0.00000e+00],\n",
      "        [2.38765e+02, 2.92589e+00, 4.14570e+02, 1.92537e+02, 3.76024e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.30688e+02, 1.33914e+02, 6.20123e+02, 4.77179e+02, 3.46457e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.55468e+02, 8.24982e-01, 6.37397e+02, 4.78456e+02, 7.09883e-01, 0.00000e+00],\n",
      "        [2.36694e+02, 0.00000e+00, 4.47632e+02, 8.06145e+01, 2.51157e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 95.63138,   0.00000, 640.00000, 479.82703,   0.68290,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.5ms pre-process, 134.0ms inference, 2.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[9.13158e+01, 1.04945e+01, 6.37626e+02, 4.79296e+02, 5.64325e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.98220e+01, 5.67407e+00, 6.28543e+02, 4.80000e+02, 5.83808e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 128.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.11521e+01, 0.00000e+00, 6.24032e+02, 4.80000e+02, 5.35593e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 remote\n",
      "Speed: 3.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.04689e+02, 1.20922e+02, 6.39784e+02, 1.44262e+02, 5.30143e-01, 6.50000e+01],\n",
      "        [5.38738e+01, 8.29073e+00, 6.27481e+02, 4.80000e+02, 4.75953e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.55481e+01, 2.03122e+01, 6.33052e+02, 4.75930e+02, 5.85552e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 116.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 97.63452,  19.28674, 640.00000, 473.04916,   0.69040,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 remote\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.05781e+02, 2.62711e+02, 6.39493e+02, 4.50963e+02, 4.38057e-01, 6.50000e+01],\n",
      "        [4.09165e+01, 3.87846e+01, 6.37287e+02, 4.65796e+02, 3.39722e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.00730e+02, 1.71302e+01, 6.39543e+02, 4.75006e+02, 4.84966e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cell phone\n",
      "Speed: 3.0ms pre-process, 125.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.46065e+02, 9.52147e+01, 2.43212e+02, 1.72221e+02, 3.87127e-01, 6.70000e+01],\n",
      "        [4.07730e+02, 2.16994e+01, 6.36526e+02, 4.72014e+02, 2.72426e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 120.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.03148e+02, 1.96906e+01, 6.40000e+02, 4.72758e+02, 3.45159e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 117.6ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.55921e+02, 7.89673e+00, 6.40000e+02, 4.80000e+02, 4.06469e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cell phone\n",
      "Speed: 2.0ms pre-process, 119.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.03150e+02, 0.00000e+00, 6.34968e+02, 4.80000e+02, 4.38463e-01, 0.00000e+00],\n",
      "        [1.11853e+02, 1.43540e+02, 1.99955e+02, 1.83631e+02, 2.91937e-01, 6.70000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.58448e+02, 1.11794e+01, 6.34405e+02, 4.79568e+02, 3.87075e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 112.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.69190e+02, 1.40756e+01, 6.32959e+02, 4.79680e+02, 5.74498e-01, 0.00000e+00],\n",
      "        [2.36292e+01, 1.37720e+02, 5.98671e+02, 4.78920e+02, 3.46860e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.57961e+02, 3.43080e+00, 6.40000e+02, 4.80000e+02, 4.09737e-01, 0.00000e+00],\n",
      "        [1.15407e+01, 1.72229e+02, 6.03524e+02, 4.80000e+02, 3.23353e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 1.5ms pre-process, 103.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92111e+02, 0.00000e+00, 6.39682e+02, 4.80000e+02, 4.51398e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 125.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57374e+02, 2.20540e+01, 6.37204e+02, 4.76730e+02, 4.88374e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.60488e+02, 1.51174e+01, 6.39077e+02, 4.80000e+02, 4.71175e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 1.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.59507e+02, 1.55869e+01, 6.37625e+02, 4.79866e+02, 4.38842e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.54962e+02, 1.75146e+01, 6.40000e+02, 4.77784e+02, 4.76562e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.51480e+02, 1.46589e+01, 6.37154e+02, 4.78112e+02, 4.25924e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.38907e+02, 0.00000e+00, 6.30377e+02, 4.80000e+02, 4.60999e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.58698e+02, 6.18651e+00, 6.40000e+02, 4.80000e+02, 6.78322e-01, 0.00000e+00],\n",
      "        [2.27514e+02, 4.07402e-01, 4.70246e+02, 1.85534e+02, 3.64432e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv, 1 mouse\n",
      "Speed: 4.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38285e+02, 1.64523e+00, 4.73124e+02, 2.11720e+02, 5.27604e-01, 6.20000e+01],\n",
      "        [3.72047e+02, 0.00000e+00, 6.37040e+02, 4.80000e+02, 3.18478e-01, 0.00000e+00],\n",
      "        [3.27842e+02, 3.73535e+02, 5.50038e+02, 4.79722e+02, 2.50675e-01, 6.40000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37408e+02, 2.18768e+00, 4.74052e+02, 1.82710e+02, 6.48124e-01, 6.20000e+01],\n",
      "        [3.45936e+02, 6.26038e-01, 6.40000e+02, 4.80000e+02, 4.37570e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 134.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34074e+02, 1.20334e+00, 4.59602e+02, 1.87961e+02, 5.52835e-01, 6.20000e+01],\n",
      "        [3.95881e+02, 3.13412e+00, 6.40000e+02, 4.79312e+02, 4.30358e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.63211e+02, 4.99617e+00, 6.38337e+02, 4.76056e+02, 2.93997e-01, 0.00000e+00],\n",
      "        [2.22603e+02, 2.08220e+00, 4.68084e+02, 1.98412e+02, 2.64142e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31144e+02, 2.34173e+00, 4.66141e+02, 1.92556e+02, 4.32691e-01, 6.20000e+01],\n",
      "        [5.37936e+02, 4.72147e+00, 6.39415e+02, 3.47257e+02, 2.99427e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[235.78683,   1.52862, 458.94473, 186.44928,   0.65865,  62.00000],\n",
      "        [ 22.89265, 259.33975, 262.65045, 480.00000,   0.59484,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 toilet, 1 tv\n",
      "Speed: 4.0ms pre-process, 130.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.42939e+01, 2.60747e+02, 2.62108e+02, 4.80000e+02, 5.76208e-01, 5.60000e+01],\n",
      "        [2.33868e+02, 1.36097e+00, 4.60538e+02, 1.87174e+02, 5.18814e-01, 6.20000e+01],\n",
      "        [3.05093e+02, 1.06408e+01, 6.40000e+02, 4.76264e+02, 2.95815e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.1ms pre-process, 120.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37432e+02, 2.40649e+00, 4.50132e+02, 1.87333e+02, 6.14307e-01, 6.20000e+01],\n",
      "        [1.61117e+02, 2.67731e+02, 6.11448e+02, 4.80000e+02, 2.96548e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33973e+02, 1.23958e+00, 4.63656e+02, 1.86150e+02, 4.16864e-01, 6.20000e+01],\n",
      "        [4.33582e+02, 7.97478e+00, 6.36916e+02, 3.08082e+02, 2.87248e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 117.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.06312e+01, 2.59276e+02, 2.58995e+02, 4.80000e+02, 5.59846e-01, 5.60000e+01],\n",
      "        [2.29742e+02, 2.32626e+00, 4.65936e+02, 1.94210e+02, 3.84177e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[232.84930,   2.89787, 461.35846, 187.65472,   0.53056,  62.00000],\n",
      "        [ 27.18474, 259.49356, 258.71027, 480.00000,   0.49845,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 117.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.70087,   1.28843, 460.27985, 186.78513,   0.65608,  62.00000],\n",
      "        [ 17.25845, 258.90509, 259.21939, 480.00000,   0.59907,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 106.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.37332,   1.47484, 459.29282, 186.28757,   0.61572,  62.00000],\n",
      "        [ 24.92682, 259.86267, 259.94812, 480.00000,   0.50954,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30011e+02, 1.89256e+00, 4.60515e+02, 1.87789e+02, 5.18979e-01, 6.20000e+01],\n",
      "        [1.59352e+01, 2.58892e+02, 2.59932e+02, 4.80000e+02, 4.06414e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 133.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 105.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 125.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 120.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.4ms inference, 0.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.2ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 117.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.11649e+02, 1.90759e+02, 6.38409e+02, 3.56327e+02, 3.97240e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.09610e+02, 1.84329e+02, 6.39870e+02, 3.55880e+02, 4.77543e-01, 0.00000e+00],\n",
      "        [1.89777e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 2.99452e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.07123e+02, 1.83286e+02, 6.40000e+02, 3.50749e+02, 6.10657e-01, 0.00000e+00],\n",
      "        [1.85073e+02, 0.00000e+00, 6.40000e+02, 4.80000e+02, 2.99539e-01, 0.00000e+00],\n",
      "        [2.99923e+01, 2.58085e+02, 2.60059e+02, 4.80000e+02, 2.86149e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.05142e+02, 1.79680e+02, 6.40000e+02, 3.48709e+02, 5.85044e-01, 0.00000e+00],\n",
      "        [3.06372e+01, 2.58810e+02, 2.62069e+02, 4.80000e+02, 2.96686e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 124.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.81225e+01, 2.59833e+02, 2.59930e+02, 4.80000e+02, 4.90504e-01, 5.60000e+01],\n",
      "        [4.07853e+02, 1.75112e+02, 6.39127e+02, 3.44061e+02, 4.25336e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 121.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23779e+02, 1.52076e+02, 6.40000e+02, 3.13433e+02, 5.88082e-01, 0.00000e+00],\n",
      "        [2.11880e+01, 2.59491e+02, 2.60955e+02, 4.80000e+02, 4.77621e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 1.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.57158e+01, 2.59808e+02, 2.62008e+02, 4.80000e+02, 3.49177e-01, 5.60000e+01],\n",
      "        [2.07840e+02, 3.05667e+01, 6.34998e+02, 4.64063e+02, 3.03829e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 19.91294, 259.74500, 259.69232, 480.00000,   0.59535,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 laptop\n",
      "Speed: 2.0ms pre-process, 119.9ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.55661e+02, 3.45720e+02, 6.39991e+02, 4.67224e+02, 3.97677e-01, 6.30000e+01],\n",
      "        [2.78208e+01, 2.59507e+02, 2.60851e+02, 4.80000e+02, 3.81019e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 134.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.10893e+01, 2.60137e+02, 2.60534e+02, 4.80000e+02, 5.44968e-01, 5.60000e+01],\n",
      "        [3.38183e+02, 1.16829e+02, 6.38671e+02, 3.67221e+02, 4.34045e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 1.5ms pre-process, 117.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.41232e+02, 1.17997e+02, 6.38580e+02, 3.65144e+02, 5.66191e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 laptop\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.98284e+01, 2.60302e+02, 2.59219e+02, 4.80000e+02, 5.76376e-01, 5.60000e+01],\n",
      "        [3.39008e+02, 1.15436e+02, 6.37718e+02, 3.65014e+02, 3.17712e-01, 0.00000e+00],\n",
      "        [5.55157e+02, 3.43764e+02, 6.40000e+02, 4.72043e+02, 3.17533e-01, 6.30000e+01],\n",
      "        [2.25084e+02, 0.00000e+00, 6.36843e+02, 4.40180e+02, 2.85171e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair\n",
      "Speed: 3.0ms pre-process, 136.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.62070e+01, 2.60036e+02, 2.60594e+02, 4.80000e+02, 5.41148e-01, 5.60000e+01],\n",
      "        [3.30624e+02, 1.04487e+02, 6.37493e+02, 3.77433e+02, 3.83305e-01, 0.00000e+00],\n",
      "        [2.18911e+02, 0.00000e+00, 6.40000e+02, 4.35343e+02, 2.90010e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 0.9ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31117e+01, 2.60188e+02, 2.58562e+02, 4.80000e+02, 5.19544e-01, 5.60000e+01],\n",
      "        [3.50191e+02, 8.18525e+01, 6.37209e+02, 3.91459e+02, 3.72403e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 127.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.56531e+02, 8.88838e+01, 6.37695e+02, 3.67589e+02, 5.76468e-01, 0.00000e+00],\n",
      "        [2.16926e+01, 2.60209e+02, 2.60048e+02, 4.80000e+02, 3.19666e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 125.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.59610e+02, 9.58335e+01, 6.38174e+02, 3.72497e+02, 5.31109e-01, 0.00000e+00],\n",
      "        [1.91886e+01, 2.59467e+02, 2.59632e+02, 4.80000e+02, 3.58912e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.63952e+02, 1.04179e+02, 6.38705e+02, 3.61377e+02, 6.87481e-01, 0.00000e+00],\n",
      "        [2.67827e+01, 2.60129e+02, 2.59886e+02, 4.80000e+02, 5.45911e-01, 5.60000e+01],\n",
      "        [2.56035e+02, 7.84645e+01, 5.01067e+02, 2.68694e+02, 3.02261e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.40256e+01, 2.59634e+02, 2.59212e+02, 4.80000e+02, 6.67606e-01, 5.60000e+01],\n",
      "        [3.56912e+02, 9.07991e+01, 6.39921e+02, 3.70395e+02, 6.21871e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 5.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.53031e+02, 7.96748e+01, 6.40000e+02, 3.58885e+02, 5.81668e-01, 0.00000e+00],\n",
      "        [2.28540e+01, 2.60431e+02, 2.60309e+02, 4.80000e+02, 4.91128e-01, 5.60000e+01],\n",
      "        [2.47736e+02, 7.39858e+01, 5.06383e+02, 3.02819e+02, 3.28425e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 3.0ms pre-process, 128.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.95988e+01, 2.59754e+02, 2.59704e+02, 4.80000e+02, 6.07683e-01, 5.60000e+01],\n",
      "        [3.52912e+02, 7.00929e+01, 6.40000e+02, 3.54760e+02, 5.30873e-01, 0.00000e+00],\n",
      "        [2.48306e+02, 7.64442e+01, 5.07016e+02, 2.95576e+02, 4.05280e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 2.0ms pre-process, 117.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.52664e+02, 6.37775e+01, 6.40000e+02, 3.55738e+02, 3.71929e-01, 0.00000e+00],\n",
      "        [2.50688e+02, 7.44232e+01, 5.04559e+02, 3.27004e+02, 3.62644e-01, 1.10000e+01],\n",
      "        [2.86270e+01, 2.59741e+02, 2.60354e+02, 4.80000e+02, 2.98710e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11807e+01, 2.59909e+02, 2.58107e+02, 4.80000e+02, 7.00584e-01, 5.60000e+01],\n",
      "        [3.51753e+02, 6.54929e+01, 6.40000e+02, 3.55230e+02, 5.42470e-01, 0.00000e+00],\n",
      "        [2.49588e+02, 7.94460e+01, 5.05304e+02, 2.91989e+02, 2.76137e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 3.0ms pre-process, 150.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.63795e+01, 2.58454e+02, 2.59786e+02, 4.79696e+02, 5.73427e-01, 5.60000e+01],\n",
      "        [3.42570e+02, 5.82535e+01, 6.39670e+02, 3.55664e+02, 3.87503e-01, 0.00000e+00],\n",
      "        [2.50395e+02, 7.90648e+01, 5.07382e+02, 2.95182e+02, 2.77072e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.1ms pre-process, 122.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36142e+01, 2.59768e+02, 2.59966e+02, 4.80000e+02, 5.13129e-01, 5.60000e+01],\n",
      "        [3.44625e+02, 5.58743e+01, 6.40000e+02, 3.68856e+02, 3.43634e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 120.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.58724e+01, 2.59620e+02, 2.61381e+02, 4.80000e+02, 5.85239e-01, 5.60000e+01],\n",
      "        [3.44451e+02, 5.28365e+01, 6.40000e+02, 3.71294e+02, 4.30838e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair, 1 laptop\n",
      "Speed: 3.0ms pre-process, 136.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.88588e+01, 2.59395e+02, 2.60410e+02, 4.80000e+02, 5.74376e-01, 5.60000e+01],\n",
      "        [2.47041e+02, 8.73343e+01, 5.05271e+02, 2.92478e+02, 3.66585e-01, 1.10000e+01],\n",
      "        [3.44488e+02, 5.24792e+01, 6.38158e+02, 3.65027e+02, 3.56610e-01, 0.00000e+00],\n",
      "        [5.55251e+02, 3.43663e+02, 6.39949e+02, 4.71148e+02, 2.72056e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair\n",
      "Speed: 2.0ms pre-process, 102.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.47548e+02, 4.26668e+01, 6.39789e+02, 3.70116e+02, 3.86453e-01, 0.00000e+00],\n",
      "        [1.88123e+01, 2.59255e+02, 2.61389e+02, 4.80000e+02, 3.83933e-01, 5.60000e+01],\n",
      "        [2.48810e+02, 8.48668e+01, 5.07622e+02, 2.97295e+02, 2.51989e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 6.0ms pre-process, 131.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.92340e+01, 2.59484e+02, 2.60632e+02, 4.80000e+02, 4.48190e-01, 5.60000e+01],\n",
      "        [3.50014e+02, 5.62966e+01, 6.38504e+02, 3.68467e+02, 3.28471e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94728e+01, 2.60082e+02, 2.60929e+02, 4.80000e+02, 4.77655e-01, 5.60000e+01],\n",
      "        [3.48259e+02, 7.20494e+01, 6.38213e+02, 4.19828e+02, 3.22490e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 114.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.18859e+01, 2.59684e+02, 2.61019e+02, 4.80000e+02, 5.13075e-01, 5.60000e+01],\n",
      "        [3.42953e+02, 5.22722e+01, 6.38464e+02, 3.74717e+02, 3.34849e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 123.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.49503e+02, 6.48789e+01, 6.38247e+02, 3.65837e+02, 4.53310e-01, 0.00000e+00],\n",
      "        [2.16195e+01, 2.59642e+02, 2.59436e+02, 4.80000e+02, 2.71428e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.46217e+02, 4.41337e+01, 6.38418e+02, 3.74677e+02, 5.23734e-01, 0.00000e+00],\n",
      "        [2.51683e+01, 2.59434e+02, 2.60646e+02, 4.80000e+02, 4.10164e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 113.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.48725e+02, 5.35335e+01, 6.38497e+02, 3.71764e+02, 5.18870e-01, 0.00000e+00],\n",
      "        [2.94593e+01, 2.60611e+02, 2.61653e+02, 4.80000e+02, 3.37864e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 119.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.57370e+02, 4.22118e+01, 6.38963e+02, 3.75495e+02, 5.88895e-01, 0.00000e+00],\n",
      "        [2.10654e+01, 2.59239e+02, 2.58987e+02, 4.80000e+02, 4.77865e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 133.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.56164e+02, 5.18668e+01, 6.38955e+02, 3.73787e+02, 5.58024e-01, 0.00000e+00],\n",
      "        [2.15179e+01, 2.59278e+02, 2.60599e+02, 4.80000e+02, 4.69455e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.54961e+02, 4.42012e+01, 6.38915e+02, 3.82442e+02, 6.50514e-01, 0.00000e+00],\n",
      "        [3.04784e+01, 2.59921e+02, 2.58790e+02, 4.80000e+02, 3.42178e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 5.0ms pre-process, 124.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.53286e+02, 5.86571e+01, 6.39554e+02, 3.70918e+02, 5.49915e-01, 0.00000e+00],\n",
      "        [1.80632e+01, 2.59552e+02, 2.60334e+02, 4.80000e+02, 3.57595e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 1.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.52817e+02, 6.15424e+01, 6.40000e+02, 3.71810e+02, 6.28302e-01, 0.00000e+00],\n",
      "        [2.36130e+01, 2.60406e+02, 2.60831e+02, 4.80000e+02, 3.47159e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.63548e+01, 2.59293e+02, 2.61041e+02, 4.80000e+02, 6.25664e-01, 5.60000e+01],\n",
      "        [3.97987e+02, 1.12744e+02, 6.39020e+02, 3.63714e+02, 3.51211e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv, 1 laptop\n",
      "Speed: 1.5ms pre-process, 112.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35044e+02, 1.40720e+00, 4.55221e+02, 1.84063e+02, 6.93888e-01, 6.20000e+01],\n",
      "        [2.27769e+01, 2.59202e+02, 2.59784e+02, 4.80000e+02, 5.41703e-01, 5.60000e+01],\n",
      "        [5.55417e+02, 3.48366e+02, 6.39665e+02, 4.43948e+02, 3.08044e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 127.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[233.78230,   0.97346, 460.28918, 185.35194,   0.67967,  62.00000],\n",
      "        [ 26.30247, 258.67358, 258.93387, 480.00000,   0.65256,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv, 1 remote\n",
      "Speed: 2.0ms pre-process, 104.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.45775e+01, 2.59232e+02, 2.59142e+02, 4.80000e+02, 6.11760e-01, 5.60000e+01],\n",
      "        [2.34089e+02, 1.10612e+00, 4.57970e+02, 1.86005e+02, 5.96780e-01, 6.20000e+01],\n",
      "        [3.90574e+02, 1.50317e+02, 6.18507e+02, 2.63265e+02, 3.27055e-01, 6.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 remote\n",
      "Speed: 5.0ms pre-process, 131.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33637e+02, 2.66585e+00, 4.60379e+02, 1.88425e+02, 6.68402e-01, 6.20000e+01],\n",
      "        [2.37237e+01, 2.59462e+02, 2.59837e+02, 4.80000e+02, 5.91130e-01, 5.60000e+01],\n",
      "        [4.13430e+02, 1.88614e+02, 4.74372e+02, 2.14318e+02, 3.70514e-01, 6.50000e+01],\n",
      "        [4.16470e+02, 3.61396e+01, 6.37957e+02, 4.76536e+02, 2.58148e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.42634e+01, 2.58289e+02, 2.58931e+02, 4.80000e+02, 5.72763e-01, 5.60000e+01],\n",
      "        [5.15803e+02, 5.83797e+01, 6.38999e+02, 4.80000e+02, 5.12552e-01, 0.00000e+00],\n",
      "        [2.33351e+02, 6.28365e-01, 5.13691e+02, 2.27886e+02, 4.67048e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 stop sign, 1 chair, 1 tv, 1 book\n",
      "Speed: 2.0ms pre-process, 120.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94535e+01, 2.59264e+02, 2.59469e+02, 4.79918e+02, 5.63215e-01, 5.60000e+01],\n",
      "        [2.34417e+02, 2.80126e+00, 4.62386e+02, 1.87716e+02, 4.33823e-01, 6.20000e+01],\n",
      "        [5.15118e+02, 1.47526e+02, 6.38861e+02, 3.57520e+02, 3.37816e-01, 1.10000e+01],\n",
      "        [5.15046e+02, 1.48357e+02, 6.39889e+02, 3.50768e+02, 2.89823e-01, 7.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.5ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31909e+02, 2.86132e+00, 4.65419e+02, 1.94017e+02, 4.51574e-01, 6.20000e+01],\n",
      "        [3.15673e+01, 2.59557e+02, 2.60046e+02, 4.80000e+02, 3.03587e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.99612e+01, 2.58372e+02, 2.58980e+02, 4.79365e+02, 3.95885e-01, 5.60000e+01],\n",
      "        [2.35722e+02, 3.94646e+00, 4.65092e+02, 1.99088e+02, 3.80351e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 116.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.54721e+01, 2.58842e+02, 2.60420e+02, 4.80000e+02, 5.53988e-01, 5.60000e+01],\n",
      "        [2.31398e+02, 3.23032e+00, 4.63242e+02, 1.90806e+02, 4.49185e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11574e+01, 2.58780e+02, 2.59320e+02, 4.80000e+02, 5.82449e-01, 5.60000e+01],\n",
      "        [2.34856e+02, 1.64303e+00, 4.60058e+02, 1.92247e+02, 3.89682e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.23384e+01, 2.59620e+02, 2.57948e+02, 4.80000e+02, 6.09261e-01, 5.60000e+01],\n",
      "        [2.31467e+02, 2.30566e+00, 4.65474e+02, 1.98739e+02, 3.98759e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.01056, 260.61465, 260.82220, 480.00000,   0.61688,  56.00000],\n",
      "        [233.93954,   1.53300, 461.46310, 183.40894,   0.49297,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 126.3ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.03389, 259.46393, 257.68750, 480.00000,   0.59042,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 1.0ms pre-process, 109.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.22697, 259.57794, 259.77753, 480.00000,   0.56902,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 113.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.21854e+01, 2.59270e+02, 2.59553e+02, 4.80000e+02, 3.16079e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 1.0ms pre-process, 125.9ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.67281, 260.19244, 261.66992, 480.00000,   0.54253,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 106.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.57238e+01, 2.60053e+02, 2.59462e+02, 4.80000e+02, 3.69441e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 119.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11274e+01, 2.59904e+02, 2.58999e+02, 4.80000e+02, 3.25768e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 130.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.42888e+01, 2.60651e+02, 2.62259e+02, 4.80000e+02, 2.54503e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.52115e+01, 2.59363e+02, 2.61431e+02, 4.80000e+02, 4.70070e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 114.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 119.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.27321e+01, 2.59167e+02, 2.56972e+02, 4.80000e+02, 4.53417e-01, 5.60000e+01],\n",
      "        [4.98352e+02, 2.22106e+02, 6.39783e+02, 4.75747e+02, 2.80457e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.39421e+01, 2.60178e+02, 2.55979e+02, 4.80000e+02, 3.52556e-01, 5.60000e+01],\n",
      "        [2.36372e+02, 1.49898e+00, 4.60253e+02, 1.84477e+02, 2.81549e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.88950e+01, 2.59960e+02, 2.56378e+02, 4.80000e+02, 5.03901e-01, 5.60000e+01],\n",
      "        [5.26362e+02, 2.62212e+02, 6.39674e+02, 4.78661e+02, 3.69531e-01, 0.00000e+00],\n",
      "        [2.33469e+02, 1.50211e+00, 4.60344e+02, 1.85581e+02, 3.62005e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 123.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.18665e+01, 2.60251e+02, 2.58039e+02, 4.80000e+02, 4.17810e-01, 5.60000e+01],\n",
      "        [2.37540e+02, 9.02847e-01, 4.63645e+02, 1.82477e+02, 4.06071e-01, 6.20000e+01],\n",
      "        [5.39354e+02, 2.59072e+02, 6.39594e+02, 4.78253e+02, 3.31198e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 133.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36327e+02, 8.81874e-01, 4.58138e+02, 1.79801e+02, 6.49966e-01, 6.20000e+01],\n",
      "        [1.82561e+01, 2.59641e+02, 2.59249e+02, 4.80000e+02, 5.64529e-01, 5.60000e+01],\n",
      "        [5.58057e+02, 2.59568e+02, 6.39627e+02, 4.75035e+02, 3.40071e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.9ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11749e+01, 2.59512e+02, 2.60272e+02, 4.79700e+02, 5.65370e-01, 5.60000e+01],\n",
      "        [2.36796e+02, 1.17442e+00, 4.61441e+02, 1.80724e+02, 4.07814e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 146.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.68932e+01, 2.59438e+02, 2.58744e+02, 4.80000e+02, 4.56603e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.1ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.25250e+02, 2.07622e+02, 6.39322e+02, 4.67471e+02, 2.72579e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 132.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.1ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 104.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.35123e+02, 2.13325e+02, 6.39329e+02, 4.65390e+02, 3.45715e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.48207e+02, 2.03308e+02, 6.39680e+02, 4.73433e+02, 2.82584e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 117.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 117.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 118.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.98352e+02, 1.76555e+02, 6.39318e+02, 4.75226e+02, 4.97964e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 144.4ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.17676e+02, 1.83904e+02, 6.40000e+02, 4.69327e+02, 2.64837e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 elephant\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.31746e+02, 1.64049e+02, 6.40000e+02, 4.57283e+02, 3.15580e-01, 2.00000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.1ms pre-process, 129.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.29536e+02, 1.61769e+02, 6.39705e+02, 4.57437e+02, 2.61778e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.29940e+02, 1.66376e+02, 6.39769e+02, 4.62566e+02, 2.72587e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.29929e+02, 1.78225e+02, 6.40000e+02, 4.70871e+02, 4.96419e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 5.0ms pre-process, 134.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.29497e+02, 1.83339e+02, 6.39746e+02, 4.75858e+02, 3.46827e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.19565e+02, 2.05464e+02, 6.39599e+02, 4.73185e+02, 3.51574e-01, 0.00000e+00],\n",
      "        [2.04486e+01, 2.60362e+02, 2.59868e+02, 4.80000e+02, 3.33645e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 5.0ms pre-process, 126.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.17993e+01, 2.60428e+02, 2.56197e+02, 4.80000e+02, 4.76805e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 108.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.05672e+01, 2.60517e+02, 2.60375e+02, 4.80000e+02, 3.18702e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 6.0ms pre-process, 144.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.94427e+01, 2.60247e+02, 2.60043e+02, 4.80000e+02, 4.71544e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.67961e+02, 1.96778e+02, 6.38879e+02, 3.81994e+02, 4.62825e-01, 0.00000e+00],\n",
      "        [2.49506e+01, 2.60923e+02, 2.55161e+02, 4.78089e+02, 4.35750e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.93706e+01, 2.60340e+02, 2.60092e+02, 4.80000e+02, 5.08385e-01, 5.60000e+01],\n",
      "        [4.69804e+02, 1.94314e+02, 6.39037e+02, 3.76850e+02, 4.31971e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 145.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.48031e+01, 2.60196e+02, 2.59114e+02, 4.80000e+02, 5.26019e-01, 5.60000e+01],\n",
      "        [4.73508e+02, 1.97821e+02, 6.39109e+02, 3.83881e+02, 3.75369e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 115.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.55139e+01, 2.60679e+02, 2.62541e+02, 4.80000e+02, 4.65420e-01, 5.60000e+01],\n",
      "        [4.73526e+02, 1.98296e+02, 6.39108e+02, 3.84656e+02, 4.56866e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 123.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.66706e+02, 1.97931e+02, 6.38799e+02, 3.87403e+02, 6.06688e-01, 0.00000e+00],\n",
      "        [2.72741e+01, 2.61552e+02, 2.56793e+02, 4.80000e+02, 3.82346e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 117.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.59800e+02, 2.01791e+02, 6.38561e+02, 3.88508e+02, 4.86375e-01, 0.00000e+00],\n",
      "        [3.07132e+01, 2.60224e+02, 2.60545e+02, 4.80000e+02, 4.52016e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.53718e+02, 1.97438e+02, 6.38589e+02, 3.83151e+02, 6.15908e-01, 0.00000e+00],\n",
      "        [2.04486e+01, 2.60670e+02, 2.60086e+02, 4.79677e+02, 3.64299e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.48429e+02, 2.08854e+02, 6.39222e+02, 3.93849e+02, 5.15709e-01, 0.00000e+00],\n",
      "        [3.02273e+01, 2.60779e+02, 2.56828e+02, 4.80000e+02, 4.15195e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 120.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.68184e+01, 2.59954e+02, 2.61029e+02, 4.80000e+02, 4.89485e-01, 5.60000e+01],\n",
      "        [4.54537e+02, 2.16717e+02, 6.39069e+02, 4.01382e+02, 4.10595e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 118.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.54805e+02, 2.15670e+02, 6.39285e+02, 4.02314e+02, 4.95153e-01, 0.00000e+00],\n",
      "        [3.47814e+01, 2.60451e+02, 2.57939e+02, 4.80000e+02, 4.10359e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 127.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.12349e+01, 2.60098e+02, 2.60047e+02, 4.80000e+02, 4.64160e-01, 5.60000e+01],\n",
      "        [4.60726e+02, 2.14130e+02, 6.39076e+02, 4.26480e+02, 4.41000e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 119.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.70396, 259.56042, 260.41891, 480.00000,   0.59334,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 157.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.53559, 259.75024, 260.89822, 480.00000,   0.54226,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.17300e+01, 2.58858e+02, 2.60843e+02, 4.80000e+02, 4.32180e-01, 5.60000e+01],\n",
      "        [2.35894e+02, 2.09457e+00, 4.59256e+02, 1.78010e+02, 3.35121e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 elephant, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.65158e+01, 2.59629e+02, 2.60945e+02, 4.80000e+02, 6.50213e-01, 5.60000e+01],\n",
      "        [5.63156e+02, 2.50952e+02, 6.40000e+02, 4.29137e+02, 5.55849e-01, 2.00000e+01],\n",
      "        [2.36117e+02, 1.56414e+00, 4.57031e+02, 1.81548e+02, 5.27744e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 138.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.47255, 260.45978, 260.63983, 480.00000,   0.63582,  56.00000],\n",
      "        [237.02814,   1.54284, 458.49969, 182.09109,   0.62699,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.6ms pre-process, 125.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.17684, 259.40216, 259.15799, 480.00000,   0.68914,  56.00000],\n",
      "        [238.45419,   2.03635, 455.22519, 183.50504,   0.63412,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 129.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[235.54778,   1.59432, 456.30145, 183.65637,   0.65820,  62.00000],\n",
      "        [ 25.29828, 258.86237, 259.77496, 480.00000,   0.53865,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.45388e+02, 5.14736e+01, 6.39424e+02, 4.75599e+02, 6.44815e-01, 0.00000e+00],\n",
      "        [2.36106e+02, 1.84775e+00, 4.56714e+02, 1.85401e+02, 6.40319e-01, 6.20000e+01],\n",
      "        [2.10328e+01, 2.58237e+02, 2.60175e+02, 4.80000e+02, 5.76637e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36982e+02, 1.65049e+00, 4.61199e+02, 1.80980e+02, 5.30848e-01, 6.20000e+01],\n",
      "        [2.21079e+01, 2.59670e+02, 2.58834e+02, 4.80000e+02, 4.25884e-01, 5.60000e+01],\n",
      "        [5.40108e+02, 1.65916e+01, 6.39646e+02, 4.75737e+02, 4.01649e-01, 0.00000e+00],\n",
      "        [2.55830e+02, 2.32693e+02, 5.55528e+02, 4.80000e+02, 2.59757e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 128.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.35030, 258.13190, 259.72604, 479.78735,   0.56913,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 132.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31472e+01, 2.59421e+02, 2.58488e+02, 4.79536e+02, 5.95946e-01, 5.60000e+01],\n",
      "        [5.43405e+02, 6.34143e+01, 6.39540e+02, 1.72645e+02, 2.88959e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.19639e+02, 2.41869e+01, 6.39471e+02, 4.69468e+02, 4.93855e-01, 0.00000e+00],\n",
      "        [2.72207e+01, 2.58045e+02, 2.57901e+02, 4.80000e+02, 4.67469e-01, 5.60000e+01],\n",
      "        [5.16425e+02, 5.06563e+00, 6.39281e+02, 1.88746e+02, 3.43507e-01, 0.00000e+00],\n",
      "        [2.56338e+02, 2.33468e+02, 5.57246e+02, 4.80000e+02, 2.65487e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 146.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32643e+02, 1.63867e+00, 4.61861e+02, 1.87258e+02, 5.06596e-01, 6.20000e+01],\n",
      "        [1.89110e+01, 2.58671e+02, 2.57746e+02, 4.79093e+02, 4.47143e-01, 5.60000e+01],\n",
      "        [4.82521e+02, 0.00000e+00, 6.38766e+02, 2.33431e+02, 4.30676e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 106.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.10764e+01, 2.59265e+02, 2.59028e+02, 4.80000e+02, 5.93109e-01, 5.60000e+01],\n",
      "        [3.17359e+02, 1.04703e+02, 4.93906e+02, 2.39886e+02, 5.88620e-01, 6.30000e+01],\n",
      "        [2.37962e+02, 2.00327e+00, 4.57214e+02, 1.85853e+02, 5.35037e-01, 6.20000e+01],\n",
      "        [4.54029e+02, 0.00000e+00, 6.38035e+02, 2.49547e+02, 4.89766e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 128.3ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.92918e+01, 2.59476e+02, 2.56532e+02, 4.80000e+02, 5.87425e-01, 5.60000e+01],\n",
      "        [4.29628e+02, 0.00000e+00, 6.39886e+02, 2.33646e+02, 5.20165e-01, 0.00000e+00],\n",
      "        [2.32433e+02, 3.28726e+00, 4.63318e+02, 1.91863e+02, 4.11562e-01, 6.20000e+01],\n",
      "        [4.26094e+02, 1.53944e+02, 6.37917e+02, 2.57682e+02, 2.92506e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.18057e+01, 2.59534e+02, 2.60124e+02, 4.80000e+02, 6.03607e-01, 5.60000e+01],\n",
      "        [4.23606e+02, 3.11111e-01, 6.39639e+02, 2.57478e+02, 5.18120e-01, 0.00000e+00],\n",
      "        [2.99123e+02, 1.03217e+02, 4.95942e+02, 2.36892e+02, 4.05147e-01, 6.30000e+01],\n",
      "        [2.35842e+02, 2.09011e+00, 4.57173e+02, 1.85310e+02, 2.68792e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.9ms pre-process, 121.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.25491e+02, 3.71307e-01, 6.39489e+02, 2.58245e+02, 4.94763e-01, 0.00000e+00],\n",
      "        [2.59136e+01, 2.59495e+02, 2.59173e+02, 4.80000e+02, 4.57178e-01, 5.60000e+01],\n",
      "        [2.33371e+02, 1.65645e+00, 4.59779e+02, 1.85225e+02, 2.95761e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 134.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.43623e+01, 2.59443e+02, 2.62603e+02, 4.80000e+02, 5.49829e-01, 5.60000e+01],\n",
      "        [4.26160e+02, 5.64720e-01, 6.39338e+02, 2.54268e+02, 5.26402e-01, 0.00000e+00],\n",
      "        [2.34165e+02, 2.10822e+00, 4.57846e+02, 1.84859e+02, 3.89499e-01, 6.20000e+01],\n",
      "        [3.01793e+02, 1.05430e+02, 4.75501e+02, 2.35165e+02, 2.62412e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 126.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.26766e+02, 4.16687e-01, 6.39029e+02, 2.58058e+02, 5.32956e-01, 0.00000e+00],\n",
      "        [3.01862e+01, 2.59597e+02, 2.58484e+02, 4.79245e+02, 4.68315e-01, 5.60000e+01],\n",
      "        [2.30660e+02, 2.45028e+00, 4.62104e+02, 1.92198e+02, 3.37784e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 1.0ms pre-process, 106.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.25582e+01, 2.58407e+02, 2.60890e+02, 4.80000e+02, 7.47721e-01, 5.60000e+01],\n",
      "        [4.28801e+02, 2.53952e-01, 6.39287e+02, 2.58075e+02, 5.00176e-01, 0.00000e+00],\n",
      "        [2.34242e+02, 3.74274e+00, 4.64021e+02, 1.90127e+02, 3.02717e-01, 6.20000e+01],\n",
      "        [3.01282e+02, 1.03328e+02, 4.71023e+02, 2.35979e+02, 2.95660e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.83324e+01, 2.58797e+02, 2.61138e+02, 4.80000e+02, 4.05946e-01, 5.60000e+01],\n",
      "        [4.24178e+02, 1.54910e+02, 6.36944e+02, 2.72232e+02, 3.98450e-01, 0.00000e+00],\n",
      "        [2.30546e+02, 3.27856e+00, 4.65165e+02, 1.94024e+02, 3.86797e-01, 6.20000e+01],\n",
      "        [4.28667e+02, 0.00000e+00, 6.38811e+02, 2.54031e+02, 3.13724e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 143.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.26013e+02, 8.92258e-01, 6.39357e+02, 2.57830e+02, 5.79606e-01, 0.00000e+00],\n",
      "        [1.94320e+01, 2.58945e+02, 2.59168e+02, 4.79377e+02, 5.69511e-01, 5.60000e+01],\n",
      "        [2.36158e+02, 1.75400e+00, 4.57982e+02, 1.85333e+02, 4.19607e-01, 6.20000e+01],\n",
      "        [3.00317e+02, 1.04894e+02, 4.82419e+02, 2.36261e+02, 2.71756e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.19249e+01, 2.58950e+02, 2.59595e+02, 4.80000e+02, 6.47441e-01, 5.60000e+01],\n",
      "        [4.27519e+02, 1.78194e+00, 6.39125e+02, 2.60851e+02, 5.10456e-01, 0.00000e+00],\n",
      "        [2.36170e+02, 1.91413e+00, 4.56264e+02, 1.85033e+02, 3.28648e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.53908e+01, 2.59246e+02, 2.57805e+02, 4.79660e+02, 6.44886e-01, 5.60000e+01],\n",
      "        [4.27881e+02, 1.91647e+00, 6.38857e+02, 2.59035e+02, 5.31217e-01, 0.00000e+00],\n",
      "        [2.31078e+02, 2.21334e+00, 4.61641e+02, 1.84879e+02, 3.86326e-01, 6.20000e+01],\n",
      "        [2.99692e+02, 1.06908e+02, 4.75360e+02, 2.36293e+02, 2.86072e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.97428e+01, 2.59550e+02, 2.59267e+02, 4.79627e+02, 5.47569e-01, 5.60000e+01],\n",
      "        [4.26816e+02, 3.11096e-01, 6.38928e+02, 2.56797e+02, 4.57689e-01, 0.00000e+00],\n",
      "        [2.34929e+02, 2.52809e+00, 4.60689e+02, 1.85330e+02, 2.96073e-01, 6.20000e+01],\n",
      "        [4.25210e+02, 1.45271e+02, 6.36664e+02, 2.83045e+02, 2.61337e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 1.0ms pre-process, 115.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.28544e+02, 3.29712e-01, 6.39503e+02, 2.59176e+02, 6.26951e-01, 0.00000e+00],\n",
      "        [1.66984e+01, 2.59105e+02, 2.60357e+02, 4.79659e+02, 5.85974e-01, 5.60000e+01],\n",
      "        [2.34319e+02, 1.59888e+00, 4.62396e+02, 1.87574e+02, 5.02605e-01, 6.20000e+01],\n",
      "        [3.00503e+02, 1.07975e+02, 4.62836e+02, 2.36798e+02, 2.92260e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.14565e+01, 2.58933e+02, 2.59093e+02, 4.80000e+02, 6.26873e-01, 5.60000e+01],\n",
      "        [4.28754e+02, 5.10300e-01, 6.39423e+02, 2.59232e+02, 5.56348e-01, 0.00000e+00],\n",
      "        [2.99684e+02, 1.09255e+02, 4.73588e+02, 2.37101e+02, 4.51182e-01, 6.30000e+01],\n",
      "        [2.37834e+02, 1.98714e+00, 4.66196e+02, 1.83899e+02, 2.98762e-01, 6.20000e+01],\n",
      "        [4.26735e+02, 1.41884e+02, 6.37010e+02, 2.87686e+02, 2.83769e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 104.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.28719e+02, 1.72745e-01, 6.39821e+02, 2.59133e+02, 5.71307e-01, 0.00000e+00],\n",
      "        [2.35133e+02, 2.41056e+00, 4.58684e+02, 1.83839e+02, 5.38953e-01, 6.20000e+01],\n",
      "        [2.99942e+02, 1.09717e+02, 4.77790e+02, 2.38304e+02, 4.89362e-01, 6.30000e+01],\n",
      "        [2.12396e+01, 2.58714e+02, 2.58375e+02, 4.80000e+02, 4.32793e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 4.2ms pre-process, 113.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.57995e+01, 2.57939e+02, 2.60072e+02, 4.80000e+02, 6.43897e-01, 5.60000e+01],\n",
      "        [4.27812e+02, 0.00000e+00, 6.39627e+02, 2.57012e+02, 5.95042e-01, 0.00000e+00],\n",
      "        [3.00603e+02, 1.10361e+02, 4.71720e+02, 2.40275e+02, 3.45144e-01, 6.30000e+01],\n",
      "        [4.27577e+02, 1.60617e+02, 6.36760e+02, 2.66856e+02, 3.16136e-01, 0.00000e+00],\n",
      "        [2.33303e+02, 3.13625e+00, 4.63866e+02, 1.97265e+02, 3.10338e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.29190e+02, 0.00000e+00, 6.39589e+02, 2.56625e+02, 5.54261e-01, 0.00000e+00],\n",
      "        [2.32861e+02, 1.93172e+00, 4.60370e+02, 1.91972e+02, 5.34207e-01, 6.20000e+01],\n",
      "        [3.04299e+01, 2.59761e+02, 2.56334e+02, 4.80000e+02, 3.50577e-01, 5.60000e+01],\n",
      "        [4.29106e+02, 1.62337e+02, 6.36756e+02, 2.64834e+02, 2.95040e-01, 0.00000e+00],\n",
      "        [3.00433e+02, 1.09041e+02, 4.78088e+02, 2.40560e+02, 2.82711e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 1.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.29136e+02, 0.00000e+00, 6.39665e+02, 2.55546e+02, 6.16963e-01, 0.00000e+00],\n",
      "        [3.01510e+02, 1.08840e+02, 4.89465e+02, 2.39733e+02, 6.04648e-01, 6.30000e+01],\n",
      "        [2.13074e+01, 2.59504e+02, 2.56595e+02, 4.80000e+02, 4.58403e-01, 5.60000e+01],\n",
      "        [2.35454e+02, 1.83742e+00, 4.59749e+02, 1.87114e+02, 4.27566e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05355e+01, 2.58405e+02, 2.59178e+02, 4.79689e+02, 6.03293e-01, 5.60000e+01],\n",
      "        [2.35927e+02, 3.31049e+00, 4.59882e+02, 2.04656e+02, 4.00372e-01, 6.20000e+01],\n",
      "        [4.21051e+02, 1.52927e+02, 6.38383e+02, 2.59904e+02, 3.55264e-01, 0.00000e+00],\n",
      "        [4.20251e+02, 0.00000e+00, 6.40000e+02, 2.53796e+02, 3.17322e-01, 0.00000e+00],\n",
      "        [2.90762e+02, 9.47308e+01, 4.78719e+02, 2.35906e+02, 2.94966e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 2 tvs, 1 laptop\n",
      "Speed: 2.5ms pre-process, 146.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.27487e+01, 2.59226e+02, 2.58351e+02, 4.80000e+02, 6.88922e-01, 5.60000e+01],\n",
      "        [4.22242e+02, 1.88889e-01, 6.39821e+02, 2.59090e+02, 5.58658e-01, 0.00000e+00],\n",
      "        [2.91066e+02, 1.08592e+02, 4.81630e+02, 2.39200e+02, 4.14247e-01, 6.30000e+01],\n",
      "        [2.35338e+02, 3.49545e+00, 4.61269e+02, 1.94672e+02, 3.83915e-01, 6.20000e+01],\n",
      "        [2.94005e+02, 1.10022e+02, 4.51625e+02, 2.34150e+02, 3.17509e-01, 6.20000e+01],\n",
      "        [4.21807e+02, 1.52159e+02, 6.36619e+02, 2.75767e+02, 2.61741e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs, 1 laptop\n",
      "Speed: 3.0ms pre-process, 142.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.21158e+02, 3.61847e-01, 6.40000e+02, 2.58590e+02, 6.44373e-01, 0.00000e+00],\n",
      "        [2.22264e+01, 2.59223e+02, 2.58753e+02, 4.79510e+02, 5.87756e-01, 5.60000e+01],\n",
      "        [2.34986e+02, 2.21911e+00, 4.59137e+02, 1.98021e+02, 4.26884e-01, 6.20000e+01],\n",
      "        [2.93086e+02, 1.08627e+02, 4.71824e+02, 2.38438e+02, 3.58347e-01, 6.30000e+01],\n",
      "        [2.93494e+02, 1.10640e+02, 4.50019e+02, 2.32104e+02, 3.17605e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.21055e+02, 1.10771e+00, 6.40000e+02, 2.58899e+02, 6.63389e-01, 0.00000e+00],\n",
      "        [2.35363e+02, 3.53493e+00, 4.59368e+02, 2.12580e+02, 4.21017e-01, 6.20000e+01],\n",
      "        [1.81268e+01, 2.58940e+02, 2.59945e+02, 4.80000e+02, 3.77442e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 133.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.22293e+02, 1.42075e-01, 6.40000e+02, 2.59385e+02, 5.17736e-01, 0.00000e+00],\n",
      "        [2.34663e+02, 3.24541e+00, 4.58324e+02, 2.17503e+02, 4.04258e-01, 6.20000e+01],\n",
      "        [3.03218e+01, 2.59323e+02, 2.47077e+02, 4.80000e+02, 3.84873e-01, 5.60000e+01],\n",
      "        [2.93079e+02, 1.07933e+02, 4.75714e+02, 2.42476e+02, 3.22660e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 2 tvs, 1 laptop\n",
      "Speed: 4.0ms pre-process, 108.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07194e+01, 2.58467e+02, 2.58228e+02, 4.80000e+02, 7.12256e-01, 5.60000e+01],\n",
      "        [4.20758e+02, 0.00000e+00, 6.39911e+02, 2.55530e+02, 5.63747e-01, 0.00000e+00],\n",
      "        [2.36318e+02, 1.95492e+00, 4.58171e+02, 2.02046e+02, 5.03835e-01, 6.20000e+01],\n",
      "        [2.92687e+02, 1.09320e+02, 4.71378e+02, 2.39418e+02, 3.91817e-01, 6.30000e+01],\n",
      "        [2.93082e+02, 1.11591e+02, 4.50865e+02, 2.33468e+02, 3.59341e-01, 6.20000e+01],\n",
      "        [4.22695e+02, 1.59891e+02, 6.36931e+02, 2.69701e+02, 2.71165e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.41187e+01, 2.59328e+02, 2.58908e+02, 4.80000e+02, 6.94188e-01, 5.60000e+01],\n",
      "        [4.21129e+02, 6.82220e-02, 6.40000e+02, 2.59508e+02, 5.93224e-01, 0.00000e+00],\n",
      "        [2.36816e+02, 1.93881e+00, 4.68032e+02, 2.20577e+02, 3.91999e-01, 6.20000e+01],\n",
      "        [2.89127e+02, 1.06531e+02, 4.79126e+02, 2.42369e+02, 3.11976e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.97966e+01, 2.59067e+02, 2.59341e+02, 4.80000e+02, 5.84082e-01, 5.60000e+01],\n",
      "        [4.24912e+02, 3.22650e+00, 6.39525e+02, 2.61677e+02, 5.64942e-01, 0.00000e+00],\n",
      "        [2.35442e+02, 2.67358e+00, 4.60640e+02, 1.95119e+02, 3.79083e-01, 6.20000e+01],\n",
      "        [4.22050e+02, 1.62931e+02, 6.37901e+02, 2.64479e+02, 3.33045e-01, 0.00000e+00],\n",
      "        [2.89963e+02, 1.08308e+02, 4.78177e+02, 2.40421e+02, 3.28578e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23627e+02, 1.61152e+02, 6.37234e+02, 2.52241e+02, 5.44406e-01, 0.00000e+00],\n",
      "        [1.97772e+01, 2.58665e+02, 2.60539e+02, 4.80000e+02, 4.66566e-01, 5.60000e+01],\n",
      "        [2.88549e+02, 1.10264e+02, 4.55028e+02, 2.38251e+02, 3.48744e-01, 6.20000e+01],\n",
      "        [2.30388e+02, 2.76117e+00, 4.61797e+02, 2.14660e+02, 2.89144e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 126.4ms inference, 1.9ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92101e+01, 2.59872e+02, 2.59632e+02, 4.78860e+02, 5.83696e-01, 5.60000e+01],\n",
      "        [4.21684e+02, 1.62759e+02, 6.40000e+02, 2.50291e+02, 5.33421e-01, 0.00000e+00],\n",
      "        [2.34521e+02, 2.13499e+00, 4.57208e+02, 1.97548e+02, 4.17579e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 118.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23603e+02, 1.61198e+02, 6.40000e+02, 2.49419e+02, 6.43046e-01, 0.00000e+00],\n",
      "        [3.93905e+01, 2.59393e+02, 2.58809e+02, 4.80000e+02, 5.59909e-01, 5.60000e+01],\n",
      "        [2.91405e+02, 1.13041e+02, 4.48753e+02, 2.36619e+02, 4.38829e-01, 6.20000e+01],\n",
      "        [2.32016e+02, 1.85878e+00, 4.80119e+02, 2.13115e+02, 2.64409e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 1.0ms pre-process, 100.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.99333e+01, 2.58410e+02, 2.60779e+02, 4.80000e+02, 6.36457e-01, 5.60000e+01],\n",
      "        [4.23308e+02, 1.62581e+02, 6.40000e+02, 2.49750e+02, 6.21801e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.25197e+02, 1.60667e+02, 6.40000e+02, 2.49824e+02, 7.21257e-01, 0.00000e+00],\n",
      "        [2.14445e+01, 2.59397e+02, 2.59267e+02, 4.79967e+02, 5.81097e-01, 5.60000e+01],\n",
      "        [2.32394e+02, 2.71002e+00, 4.60809e+02, 2.08153e+02, 3.78247e-01, 6.20000e+01],\n",
      "        [2.90625e+02, 1.10654e+02, 4.50279e+02, 2.39066e+02, 3.10422e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 109.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.72467e+01, 2.58122e+02, 2.58652e+02, 4.80000e+02, 6.88923e-01, 5.60000e+01],\n",
      "        [4.25757e+02, 1.62300e+02, 6.40000e+02, 2.52002e+02, 6.54488e-01, 0.00000e+00],\n",
      "        [2.91078e+02, 1.10558e+02, 4.58959e+02, 2.39099e+02, 3.64153e-01, 6.20000e+01],\n",
      "        [2.33736e+02, 2.90402e+00, 4.60648e+02, 1.98397e+02, 3.29321e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23704e+02, 1.61450e+02, 6.40000e+02, 2.49650e+02, 6.00518e-01, 0.00000e+00],\n",
      "        [2.36802e+01, 2.58977e+02, 2.59649e+02, 4.80000e+02, 5.42183e-01, 5.60000e+01],\n",
      "        [2.92352e+02, 1.10501e+02, 4.46124e+02, 2.36525e+02, 4.39374e-01, 6.20000e+01],\n",
      "        [2.32568e+02, 2.97849e+00, 4.61133e+02, 2.01369e+02, 2.86932e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.28648e+01, 2.58602e+02, 2.59540e+02, 4.80000e+02, 6.07548e-01, 5.60000e+01],\n",
      "        [4.24943e+02, 1.60703e+02, 6.40000e+02, 2.52540e+02, 5.42156e-01, 0.00000e+00],\n",
      "        [2.34233e+02, 2.35970e+00, 4.61100e+02, 2.02777e+02, 3.90972e-01, 6.20000e+01],\n",
      "        [2.90897e+02, 1.09682e+02, 4.50231e+02, 2.38114e+02, 3.00770e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23911e+02, 1.60434e+02, 6.40000e+02, 2.49897e+02, 5.59963e-01, 0.00000e+00],\n",
      "        [2.00089e+01, 2.58745e+02, 2.60131e+02, 4.80000e+02, 5.15652e-01, 5.60000e+01],\n",
      "        [2.34966e+02, 2.91384e+00, 4.58940e+02, 1.97854e+02, 3.77901e-01, 6.20000e+01],\n",
      "        [2.53193e+02, 2.29261e+02, 5.63492e+02, 4.78437e+02, 2.67743e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.23956e+02, 1.60899e+02, 6.40000e+02, 2.50185e+02, 6.59415e-01, 0.00000e+00],\n",
      "        [2.02853e+01, 2.58999e+02, 2.59658e+02, 4.80000e+02, 6.31130e-01, 5.60000e+01],\n",
      "        [2.91591e+02, 1.10805e+02, 4.59801e+02, 2.38447e+02, 4.26659e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05019e+01, 2.59668e+02, 2.60591e+02, 4.80000e+02, 6.51676e-01, 5.60000e+01],\n",
      "        [4.23217e+02, 1.59830e+02, 6.39833e+02, 2.54494e+02, 4.41219e-01, 0.00000e+00],\n",
      "        [2.91749e+02, 1.09331e+02, 4.53074e+02, 2.38776e+02, 3.14071e-01, 6.20000e+01],\n",
      "        [4.22709e+02, 1.59025e+02, 6.36797e+02, 4.80000e+02, 2.51570e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 laptop\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.44878e+01, 2.58794e+02, 2.58688e+02, 4.80000e+02, 6.79018e-01, 5.60000e+01],\n",
      "        [4.18613e+02, 1.60297e+02, 6.36724e+02, 2.67814e+02, 4.31034e-01, 0.00000e+00],\n",
      "        [2.94401e+02, 1.06165e+02, 4.52248e+02, 2.37200e+02, 4.23119e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 117.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.12293e+01, 2.59674e+02, 2.60128e+02, 4.78523e+02, 5.25289e-01, 5.60000e+01],\n",
      "        [4.30469e+02, 8.60397e-01, 6.38695e+02, 2.50242e+02, 3.78426e-01, 0.00000e+00],\n",
      "        [2.27982e+02, 2.81820e+00, 4.70321e+02, 1.95953e+02, 3.44672e-01, 6.20000e+01],\n",
      "        [3.04493e+02, 8.98164e+01, 4.76015e+02, 2.37680e+02, 2.71494e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.87408,   1.66928, 463.02185, 183.22621,   0.72501,  62.00000],\n",
      "        [ 18.77558, 258.42960, 258.52844, 480.00000,   0.59724,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 136.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[232.89510,   1.70365, 460.50647, 183.97803,   0.56393,  62.00000],\n",
      "        [ 32.81842, 258.69278, 257.49164, 480.00000,   0.52889,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30596e+01, 2.58912e+02, 2.60133e+02, 4.79517e+02, 5.67923e-01, 5.60000e+01],\n",
      "        [2.29838e+02, 2.54463e+00, 4.66003e+02, 1.90044e+02, 4.54102e-01, 6.20000e+01],\n",
      "        [5.21132e+02, 1.33416e+01, 6.39468e+02, 4.64897e+02, 3.13554e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 128.2ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.74775e+01, 2.59148e+02, 2.61618e+02, 4.80000e+02, 6.21276e-01, 5.60000e+01],\n",
      "        [2.32474e+02, 3.20530e+00, 4.64251e+02, 1.90408e+02, 4.95820e-01, 6.20000e+01],\n",
      "        [5.91315e+02, 6.34526e+01, 6.39833e+02, 2.81223e+02, 2.91938e-01, 0.00000e+00],\n",
      "        [2.53513e+02, 2.34581e+02, 5.60217e+02, 4.77348e+02, 2.72428e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.83225e+01, 2.58473e+02, 2.57559e+02, 4.80000e+02, 5.51309e-01, 5.60000e+01],\n",
      "        [2.29607e+02, 2.95940e+00, 4.63486e+02, 1.91128e+02, 4.99224e-01, 6.20000e+01],\n",
      "        [2.55343e+02, 2.33315e+02, 5.61063e+02, 4.78109e+02, 3.67411e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 142.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.63524e+01, 2.59337e+02, 2.60040e+02, 4.80000e+02, 4.27283e-01, 5.60000e+01],\n",
      "        [2.33068e+02, 2.52115e+00, 4.63158e+02, 1.90547e+02, 4.17943e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 119.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.51712e+01, 2.58470e+02, 2.57459e+02, 4.79464e+02, 5.09548e-01, 5.60000e+01],\n",
      "        [2.30977e+02, 2.63919e+00, 4.66600e+02, 1.93757e+02, 4.67628e-01, 6.20000e+01],\n",
      "        [2.58103e+02, 2.33992e+02, 5.62413e+02, 4.78811e+02, 3.25510e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 103.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86783e+01, 2.58596e+02, 2.60363e+02, 4.79220e+02, 4.96805e-01, 5.60000e+01],\n",
      "        [2.33638e+02, 1.92143e+00, 4.58533e+02, 1.88683e+02, 3.61119e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.66981e+01, 2.58962e+02, 2.59842e+02, 4.80000e+02, 6.31535e-01, 5.60000e+01],\n",
      "        [2.30000e+02, 1.94495e+00, 4.61396e+02, 1.91462e+02, 4.41366e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 135.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38968e+01, 2.59735e+02, 2.58928e+02, 4.78791e+02, 5.55584e-01, 5.60000e+01],\n",
      "        [2.30404e+02, 2.96976e+00, 4.65883e+02, 1.97320e+02, 4.11073e-01, 6.20000e+01],\n",
      "        [2.56021e+02, 2.33425e+02, 5.58867e+02, 4.78640e+02, 2.89419e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32405e+02, 2.04827e+00, 4.60601e+02, 1.89105e+02, 4.30561e-01, 6.20000e+01],\n",
      "        [2.30177e+01, 2.59751e+02, 2.60602e+02, 4.80000e+02, 4.15994e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 115.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32111e+02, 3.16841e+00, 4.60114e+02, 1.89610e+02, 4.72770e-01, 6.20000e+01],\n",
      "        [2.37012e+01, 2.59135e+02, 2.61968e+02, 4.79600e+02, 4.22933e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.91475e+01, 2.59739e+02, 2.62373e+02, 4.78405e+02, 4.03341e-01, 5.60000e+01],\n",
      "        [2.27907e+02, 2.57866e+00, 4.66161e+02, 1.90789e+02, 3.15274e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 133.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.45108, 258.86334, 259.83731, 480.00000,   0.51930,  56.00000],\n",
      "        [229.29807,   3.72987, 462.53879, 187.44237,   0.50997,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 105.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35607e+02, 1.59345e+00, 4.57515e+02, 1.86833e+02, 5.28498e-01, 6.20000e+01],\n",
      "        [2.03042e+01, 2.60236e+02, 2.62059e+02, 4.79546e+02, 5.02807e-01, 5.60000e+01],\n",
      "        [2.56628e+02, 2.32959e+02, 5.60630e+02, 4.78537e+02, 3.19982e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 101.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.54580e+01, 2.58447e+02, 2.58530e+02, 4.79356e+02, 6.21087e-01, 5.60000e+01],\n",
      "        [2.29903e+02, 2.63985e+00, 4.63351e+02, 1.90006e+02, 4.14768e-01, 6.20000e+01],\n",
      "        [2.58302e+02, 2.34268e+02, 5.59180e+02, 4.77299e+02, 2.88220e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 140.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33009e+02, 3.17850e+00, 4.60592e+02, 1.87018e+02, 5.13100e-01, 6.20000e+01],\n",
      "        [2.54052e+01, 2.58119e+02, 2.61303e+02, 4.80000e+02, 4.74586e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 128.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.45733e+01, 2.59154e+02, 2.57757e+02, 4.80000e+02, 6.10939e-01, 5.60000e+01],\n",
      "        [2.29402e+02, 2.46826e+00, 4.67663e+02, 1.94674e+02, 4.09540e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.49324e+01, 2.58006e+02, 2.59743e+02, 4.80000e+02, 5.62880e-01, 5.60000e+01],\n",
      "        [2.31288e+02, 2.85891e+00, 4.61503e+02, 1.87435e+02, 4.96969e-01, 6.20000e+01],\n",
      "        [2.55439e+02, 2.34073e+02, 5.57740e+02, 4.76935e+02, 2.71202e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98936e+01, 2.58851e+02, 2.59329e+02, 4.80000e+02, 6.01492e-01, 5.60000e+01],\n",
      "        [2.33225e+02, 2.29369e+00, 4.65511e+02, 1.94614e+02, 4.12809e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 123.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92450e+01, 2.58672e+02, 2.58448e+02, 4.79485e+02, 5.21788e-01, 5.60000e+01],\n",
      "        [2.33863e+02, 1.27472e+00, 4.59724e+02, 1.89488e+02, 4.84545e-01, 6.20000e+01],\n",
      "        [2.56636e+02, 2.34241e+02, 5.59927e+02, 4.77250e+02, 3.22152e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.01277e+01, 2.59064e+02, 2.57357e+02, 4.79551e+02, 6.52289e-01, 5.60000e+01],\n",
      "        [2.32382e+02, 2.33105e+00, 4.63473e+02, 1.90959e+02, 4.67791e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30181e+02, 2.88203e+00, 4.64391e+02, 1.92473e+02, 3.36573e-01, 6.20000e+01],\n",
      "        [2.53503e+01, 2.59286e+02, 2.61107e+02, 4.80000e+02, 3.33760e-01, 5.60000e+01],\n",
      "        [2.57782e+02, 2.32383e+02, 5.60087e+02, 4.79599e+02, 2.65822e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.38804e+02, 5.16959e+00, 6.39052e+02, 4.21783e+02, 5.94699e-01, 0.00000e+00],\n",
      "        [2.50441e+01, 2.58683e+02, 2.60505e+02, 4.80000e+02, 4.69172e-01, 5.60000e+01],\n",
      "        [2.35020e+02, 9.39491e-01, 4.59484e+02, 1.88512e+02, 3.02658e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.64713e+01, 2.58533e+02, 2.59370e+02, 4.80000e+02, 6.10574e-01, 5.60000e+01],\n",
      "        [2.29465e+02, 2.86790e+00, 4.63451e+02, 1.92925e+02, 4.50729e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.59266e+01, 2.59390e+02, 2.59448e+02, 4.79717e+02, 7.05210e-01, 5.60000e+01],\n",
      "        [2.33344e+02, 2.12057e+00, 4.66699e+02, 1.92657e+02, 3.99941e-01, 6.20000e+01],\n",
      "        [2.57081e+02, 2.33208e+02, 5.59122e+02, 4.77611e+02, 2.91711e-01, 6.20000e+01],\n",
      "        [5.33019e+02, 0.00000e+00, 6.38855e+02, 4.65878e+02, 2.51696e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07183e+01, 2.58831e+02, 2.58874e+02, 4.80000e+02, 6.85344e-01, 5.60000e+01],\n",
      "        [5.09170e+02, 1.24797e+00, 6.39143e+02, 4.62181e+02, 6.75487e-01, 0.00000e+00],\n",
      "        [2.30510e+02, 2.90205e+00, 4.63846e+02, 1.88629e+02, 4.35789e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.05346e+02, 1.40388e+00, 6.38804e+02, 4.63330e+02, 7.30017e-01, 0.00000e+00],\n",
      "        [2.53608e+01, 2.58241e+02, 2.58152e+02, 4.80000e+02, 6.54334e-01, 5.60000e+01],\n",
      "        [2.30844e+02, 2.73226e+00, 4.61804e+02, 1.88573e+02, 3.47099e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.1ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.18733e+02, 2.52846e+00, 6.39156e+02, 4.28188e+02, 7.22631e-01, 0.00000e+00],\n",
      "        [2.32878e+02, 2.67239e+00, 4.61254e+02, 1.87481e+02, 4.71585e-01, 6.20000e+01],\n",
      "        [2.26038e+01, 2.59370e+02, 2.60609e+02, 4.79405e+02, 4.58305e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.25224e+02, 0.00000e+00, 6.39438e+02, 4.37259e+02, 6.89935e-01, 0.00000e+00],\n",
      "        [1.93672e+01, 2.58606e+02, 2.59746e+02, 4.80000e+02, 6.14969e-01, 5.60000e+01],\n",
      "        [2.31600e+02, 3.13309e+00, 4.63294e+02, 1.92958e+02, 4.28008e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.93224e+01, 2.58717e+02, 2.59781e+02, 4.80000e+02, 5.18952e-01, 5.60000e+01],\n",
      "        [2.33359e+02, 1.53432e+00, 4.85742e+02, 2.07798e+02, 4.33545e-01, 6.20000e+01],\n",
      "        [5.21361e+02, 0.00000e+00, 6.39826e+02, 4.31760e+02, 4.17708e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07305e+01, 2.59625e+02, 2.59223e+02, 4.80000e+02, 6.47290e-01, 5.60000e+01],\n",
      "        [5.19263e+02, 0.00000e+00, 6.39275e+02, 4.34346e+02, 6.24537e-01, 0.00000e+00],\n",
      "        [2.29394e+02, 3.43369e+00, 4.68010e+02, 1.92275e+02, 2.51930e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 99.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.08387e+01, 2.59099e+02, 2.58496e+02, 4.80000e+02, 4.88377e-01, 5.60000e+01],\n",
      "        [2.28423e+02, 2.67776e+00, 4.65674e+02, 1.87168e+02, 4.22654e-01, 6.20000e+01],\n",
      "        [5.06525e+02, 1.42891e+01, 6.39441e+02, 4.77309e+02, 3.27293e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.65744e+01, 2.59608e+02, 2.59050e+02, 4.79158e+02, 5.99358e-01, 5.60000e+01],\n",
      "        [2.32814e+02, 2.74731e+00, 4.66853e+02, 1.91987e+02, 3.75635e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.02602e+01, 2.58224e+02, 2.60454e+02, 4.80000e+02, 5.19607e-01, 5.60000e+01],\n",
      "        [2.30813e+02, 2.73334e+00, 4.68132e+02, 1.90783e+02, 3.02096e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.97055e+01, 2.58326e+02, 2.60076e+02, 4.80000e+02, 4.37134e-01, 5.60000e+01],\n",
      "        [2.32184e+02, 2.81731e+00, 4.66147e+02, 1.87672e+02, 4.15216e-01, 6.20000e+01],\n",
      "        [5.35257e+02, 1.80839e+00, 6.38464e+02, 1.53571e+02, 4.09101e-01, 0.00000e+00],\n",
      "        [5.41536e+02, 1.43806e+02, 6.39062e+02, 4.76836e+02, 3.02148e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 141.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.66492e+01, 2.59389e+02, 2.58591e+02, 4.80000e+02, 7.10066e-01, 5.60000e+01],\n",
      "        [2.28059e+02, 3.28619e+00, 4.64165e+02, 1.88462e+02, 4.70343e-01, 6.20000e+01],\n",
      "        [5.57255e+02, 3.51271e+00, 6.39330e+02, 2.91412e+02, 3.13089e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34880e+02, 1.90137e+00, 4.57110e+02, 1.86668e+02, 4.61266e-01, 6.20000e+01],\n",
      "        [3.57446e+01, 2.59785e+02, 2.59067e+02, 4.80000e+02, 3.10148e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 5.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.59962e+01, 2.59269e+02, 2.60371e+02, 4.80000e+02, 5.90045e-01, 5.60000e+01],\n",
      "        [2.32371e+02, 1.39056e+00, 4.59932e+02, 1.89998e+02, 2.85898e-01, 6.20000e+01],\n",
      "        [2.58267e+02, 2.33761e+02, 5.62199e+02, 4.78832e+02, 2.76712e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.85909e+01, 2.58671e+02, 2.61150e+02, 4.80000e+02, 6.75835e-01, 5.60000e+01],\n",
      "        [2.34569e+02, 1.42207e+00, 4.56825e+02, 1.86444e+02, 4.83275e-01, 6.20000e+01],\n",
      "        [4.19244e+02, 2.32027e+01, 6.37989e+02, 4.56149e+02, 3.55458e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 chairs, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.35363e+01, 2.58849e+02, 2.58686e+02, 4.80000e+02, 4.84344e-01, 5.60000e+01],\n",
      "        [2.32863e+02, 1.27361e+00, 4.88312e+02, 2.24523e+02, 4.42392e-01, 6.20000e+01],\n",
      "        [4.66205e+02, 2.88243e+00, 6.39103e+02, 2.57867e+02, 3.62823e-01, 0.00000e+00],\n",
      "        [5.51413e+02, 2.39350e+02, 6.39426e+02, 4.72895e+02, 2.92290e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05827e+01, 2.58323e+02, 2.60416e+02, 4.80000e+02, 6.70185e-01, 5.60000e+01],\n",
      "        [2.30135e+02, 1.86142e+00, 4.69923e+02, 1.94237e+02, 4.06185e-01, 6.20000e+01],\n",
      "        [2.57556e+02, 2.32774e+02, 5.58791e+02, 4.78802e+02, 3.79752e-01, 6.20000e+01],\n",
      "        [4.82712e+02, 2.26056e+01, 6.38304e+02, 4.49158e+02, 2.83427e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.45306e+01, 2.58749e+02, 2.58770e+02, 4.80000e+02, 5.72368e-01, 5.60000e+01],\n",
      "        [2.35652e+02, 1.15768e+00, 4.66443e+02, 1.85645e+02, 4.34780e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.94855, 258.76059, 258.85941, 480.00000,   0.53406,  56.00000],\n",
      "        [232.22336,   2.76167, 467.51974, 190.53188,   0.52672,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cup, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 127.8ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.91299e+02, 1.41807e+02, 6.40000e+02, 1.79992e+02, 6.75528e-01, 4.10000e+01],\n",
      "        [2.32031e+02, 1.57130e+00, 5.08762e+02, 2.08909e+02, 4.68139e-01, 6.20000e+01],\n",
      "        [2.15527e+01, 2.59069e+02, 2.59139e+02, 4.80000e+02, 4.62833e-01, 5.60000e+01],\n",
      "        [4.76190e+02, 2.39233e+00, 6.38906e+02, 2.31222e+02, 4.18909e-01, 0.00000e+00],\n",
      "        [2.58537e+02, 2.33857e+02, 5.58015e+02, 4.77494e+02, 2.86239e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.17933e+01, 2.58929e+02, 2.59270e+02, 4.80000e+02, 4.04111e-01, 5.60000e+01],\n",
      "        [2.31251e+02, 3.21820e+00, 4.64012e+02, 1.88385e+02, 3.70997e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.10007e+01, 2.59059e+02, 2.57988e+02, 4.80000e+02, 5.63902e-01, 5.60000e+01],\n",
      "        [2.29399e+02, 3.31827e+00, 4.62365e+02, 1.87657e+02, 4.22312e-01, 6.20000e+01],\n",
      "        [2.55529e+02, 2.33572e+02, 5.60480e+02, 4.79380e+02, 2.62342e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 108.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37669e+01, 2.59736e+02, 2.58250e+02, 4.78170e+02, 6.33505e-01, 5.60000e+01],\n",
      "        [2.31477e+02, 2.30133e+00, 4.62269e+02, 1.88674e+02, 4.40003e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 126.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.88587e+01, 2.58506e+02, 2.59465e+02, 4.80000e+02, 6.29514e-01, 5.60000e+01],\n",
      "        [2.31580e+02, 2.16238e+00, 4.63209e+02, 1.88783e+02, 5.04632e-01, 6.20000e+01],\n",
      "        [2.56109e+02, 2.33183e+02, 5.58960e+02, 4.79175e+02, 3.24301e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.99646e+01, 2.59603e+02, 2.58794e+02, 4.80000e+02, 5.21076e-01, 5.60000e+01],\n",
      "        [2.29596e+02, 3.25489e+00, 4.62185e+02, 1.94430e+02, 3.70228e-01, 6.20000e+01],\n",
      "        [2.56611e+02, 2.33665e+02, 5.61711e+02, 4.78369e+02, 2.58075e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 99.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 19.53933, 259.26007, 257.80917, 480.00000,   0.52898,  56.00000],\n",
      "        [233.71744,   3.10790, 460.29697, 186.83658,   0.51885,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 134.6ms inference, 0.9ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.60173, 258.48865, 260.33502, 480.00000,   0.62505,  56.00000],\n",
      "        [234.35504,   3.34940, 462.08600, 187.62952,   0.49910,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 106.1ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.21207, 258.42096, 261.16998, 479.83649,   0.61968,  56.00000],\n",
      "        [229.77339,   1.38505, 458.52838, 190.13324,   0.56297,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32377e+02, 1.83541e+00, 4.59511e+02, 1.89534e+02, 5.38696e-01, 6.20000e+01],\n",
      "        [1.95624e+01, 2.58679e+02, 2.59374e+02, 4.80000e+02, 4.30491e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33702e+02, 3.10678e+00, 4.61603e+02, 1.88912e+02, 5.08403e-01, 6.20000e+01],\n",
      "        [2.21104e+01, 2.59849e+02, 2.59748e+02, 4.80000e+02, 4.66436e-01, 5.60000e+01],\n",
      "        [2.55694e+02, 2.33167e+02, 5.61079e+02, 4.78525e+02, 2.86191e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 136.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.13866, 259.28079, 259.64984, 478.74271,   0.57297,  56.00000],\n",
      "        [235.26166,   1.22754, 458.22638, 188.58127,   0.56277,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 127.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.08554e+01, 2.59528e+02, 2.60431e+02, 4.79340e+02, 5.91428e-01, 5.60000e+01],\n",
      "        [2.31602e+02, 3.19657e+00, 4.59926e+02, 1.87221e+02, 5.84104e-01, 6.20000e+01],\n",
      "        [2.57766e+02, 2.33950e+02, 5.59840e+02, 4.78834e+02, 2.66193e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 144.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.01750, 258.01151, 258.94592, 480.00000,   0.62316,  56.00000],\n",
      "        [235.46997,   1.15717, 458.18561, 187.86624,   0.51942,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.25621e+01, 2.59490e+02, 2.60068e+02, 4.78380e+02, 5.61115e-01, 5.60000e+01],\n",
      "        [2.33758e+02, 2.12423e+00, 4.60147e+02, 1.87258e+02, 3.82306e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 110.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94757e+01, 2.59387e+02, 2.57597e+02, 4.78745e+02, 5.67348e-01, 5.60000e+01],\n",
      "        [2.30773e+02, 2.62685e+00, 4.62014e+02, 1.89418e+02, 4.74005e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30038e+01, 2.58559e+02, 2.60176e+02, 4.80000e+02, 6.43233e-01, 5.60000e+01],\n",
      "        [2.32191e+02, 1.21278e+00, 4.58808e+02, 1.87362e+02, 4.33544e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 136.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.40890e+01, 2.58660e+02, 2.58434e+02, 4.79063e+02, 5.81703e-01, 5.60000e+01],\n",
      "        [2.30801e+02, 3.67546e+00, 4.62392e+02, 1.87712e+02, 5.22690e-01, 6.20000e+01],\n",
      "        [2.57314e+02, 2.34266e+02, 5.57414e+02, 4.78642e+02, 2.53755e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 106.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.83687e+01, 2.58943e+02, 2.60216e+02, 4.79624e+02, 6.70334e-01, 5.60000e+01],\n",
      "        [2.31976e+02, 2.08060e+00, 4.58926e+02, 1.86980e+02, 5.49365e-01, 6.20000e+01],\n",
      "        [2.56897e+02, 2.34488e+02, 5.62378e+02, 4.79560e+02, 2.78288e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 snowboard, 2 tvs\n",
      "Speed: 1.5ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30565e+02, 3.87908e+00, 4.62618e+02, 1.89056e+02, 2.90415e-01, 6.20000e+01],\n",
      "        [2.57210e+02, 2.34809e+02, 5.62170e+02, 4.78275e+02, 2.89117e-01, 6.20000e+01],\n",
      "        [2.47319e+01, 2.59385e+02, 2.53538e+02, 4.80000e+02, 2.77771e-01, 3.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33087e+02, 1.71816e+00, 4.58500e+02, 1.89389e+02, 4.85692e-01, 6.20000e+01],\n",
      "        [1.99171e+01, 2.58698e+02, 2.59344e+02, 4.79976e+02, 4.31181e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 5.0ms pre-process, 130.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30577e+02, 2.22554e+00, 4.58227e+02, 1.88220e+02, 4.95930e-01, 6.20000e+01],\n",
      "        [2.18062e+01, 2.58795e+02, 2.59746e+02, 4.79021e+02, 4.26663e-01, 5.60000e+01],\n",
      "        [2.59382e+02, 2.32820e+02, 5.59209e+02, 4.78323e+02, 2.99909e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.9ms pre-process, 123.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.87626, 258.75317, 260.96490, 480.00000,   0.55545,  56.00000],\n",
      "        [230.81189,   3.65823, 463.66534, 188.45782,   0.53080,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.9ms pre-process, 129.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[230.19849,   2.74547, 463.59924, 191.17000,   0.52999,  62.00000],\n",
      "        [ 16.08350, 259.16992, 259.29199, 479.93219,   0.50621,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 143.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29635e+02, 2.69105e+00, 4.59930e+02, 1.88435e+02, 4.60415e-01, 6.20000e+01],\n",
      "        [2.24408e+01, 2.58618e+02, 2.60591e+02, 4.80000e+02, 4.39406e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 104.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29473e+01, 2.57899e+02, 2.59904e+02, 4.79878e+02, 6.72586e-01, 5.60000e+01],\n",
      "        [2.34051e+02, 1.07954e+00, 4.57249e+02, 1.88834e+02, 4.94008e-01, 6.20000e+01],\n",
      "        [5.40333e+02, 2.33424e+02, 6.39600e+02, 4.78021e+02, 4.19961e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32004e+02, 2.15057e+00, 4.59702e+02, 1.88359e+02, 5.40574e-01, 6.20000e+01],\n",
      "        [2.37339e+01, 2.59354e+02, 2.61413e+02, 4.80000e+02, 5.34893e-01, 5.60000e+01],\n",
      "        [5.66979e+02, 2.31518e+02, 6.39270e+02, 3.64494e+02, 3.21149e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 chairs, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.19401e+01, 2.59104e+02, 2.58516e+02, 4.80000e+02, 5.47018e-01, 5.60000e+01],\n",
      "        [5.64197e+02, 2.39690e+02, 6.39697e+02, 3.14992e+02, 5.12691e-01, 0.00000e+00],\n",
      "        [2.35217e+02, 1.88844e+00, 4.55602e+02, 1.85987e+02, 3.18795e-01, 6.20000e+01],\n",
      "        [2.57712e+02, 2.31489e+02, 5.71807e+02, 4.79493e+02, 2.66838e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 102.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.00376e+01, 2.59361e+02, 2.58962e+02, 4.80000e+02, 6.88448e-01, 5.60000e+01],\n",
      "        [2.31104e+02, 1.46529e+00, 4.57984e+02, 1.90089e+02, 4.67938e-01, 6.20000e+01],\n",
      "        [2.58663e+02, 2.35246e+02, 5.61384e+02, 4.80000e+02, 2.56975e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 132.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94972e+01, 2.58360e+02, 2.59794e+02, 4.80000e+02, 7.18346e-01, 5.60000e+01],\n",
      "        [2.29968e+02, 7.93083e-01, 4.60033e+02, 1.89797e+02, 4.40508e-01, 6.20000e+01],\n",
      "        [5.63079e+02, 2.02540e+02, 6.39249e+02, 2.56477e+02, 4.31947e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 138.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.12876, 259.49261, 259.59036, 480.00000,   0.59534,  56.00000],\n",
      "        [230.68594,   3.01288, 467.31729, 191.62238,   0.49473,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 113.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.37679, 258.47665, 257.41522, 479.93008,   0.67205,  56.00000],\n",
      "        [232.95193,   1.57816, 459.64560, 187.89828,   0.51512,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 137.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07245e+01, 2.59324e+02, 2.59154e+02, 4.80000e+02, 5.85400e-01, 5.60000e+01],\n",
      "        [2.30442e+02, 2.60447e+00, 4.63537e+02, 1.91260e+02, 4.35135e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 111.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.88078, 259.40173, 258.91187, 480.00000,   0.58039,  56.00000],\n",
      "        [233.16576,   2.34496, 460.36908, 187.71124,   0.49575,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 129.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.70186e+01, 2.59246e+02, 2.58850e+02, 4.80000e+02, 6.07146e-01, 5.60000e+01],\n",
      "        [2.32026e+02, 3.87019e+00, 4.62400e+02, 1.88872e+02, 3.98639e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 105.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31954e+02, 3.53907e+00, 4.59832e+02, 1.87053e+02, 5.07189e-01, 6.20000e+01],\n",
      "        [2.38141e+01, 2.59451e+02, 2.60189e+02, 4.80000e+02, 4.35470e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.68432, 258.77054, 258.45251, 480.00000,   0.56950,  56.00000],\n",
      "        [233.52551,   1.22594, 458.23694, 186.39120,   0.48626,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.71021e+02, 2.40632e+02, 6.39716e+02, 4.27851e+02, 6.67809e-01, 0.00000e+00],\n",
      "        [2.13371e+01, 2.60116e+02, 2.57650e+02, 4.78868e+02, 4.86915e-01, 5.60000e+01],\n",
      "        [2.31429e+02, 3.75628e+00, 4.63257e+02, 1.89465e+02, 3.77477e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 139.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35730e+02, 1.53197e+00, 4.57074e+02, 1.87680e+02, 5.20501e-01, 6.20000e+01],\n",
      "        [1.76594e+01, 2.59275e+02, 2.60004e+02, 4.80000e+02, 4.05299e-01, 5.60000e+01],\n",
      "        [5.79979e+02, 2.48501e+02, 6.39506e+02, 4.47794e+02, 3.56753e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 123.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.69303e+01, 2.58550e+02, 2.59963e+02, 4.80000e+02, 6.58035e-01, 5.60000e+01],\n",
      "        [2.32406e+02, 8.09242e-01, 4.60594e+02, 1.88399e+02, 4.23663e-01, 6.20000e+01],\n",
      "        [5.99666e+02, 2.42982e+02, 6.39949e+02, 4.71567e+02, 2.92441e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 4.0ms pre-process, 139.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.13408e+01, 2.58938e+02, 2.59792e+02, 4.80000e+02, 5.24589e-01, 5.60000e+01],\n",
      "        [2.35123e+02, 1.56190e+00, 4.58381e+02, 1.87541e+02, 4.26565e-01, 6.20000e+01],\n",
      "        [2.57734e+02, 2.32660e+02, 5.63351e+02, 4.78013e+02, 2.55110e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.5ms pre-process, 101.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.56250e+01, 2.58752e+02, 2.58472e+02, 4.80000e+02, 5.60668e-01, 5.60000e+01],\n",
      "        [2.32316e+02, 3.28295e+00, 4.61836e+02, 1.86582e+02, 4.33983e-01, 6.20000e+01],\n",
      "        [2.56354e+02, 2.31810e+02, 5.61139e+02, 4.79172e+02, 2.73807e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 107.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31031e+02, 1.26919e+00, 4.57273e+02, 1.90462e+02, 4.49503e-01, 6.20000e+01],\n",
      "        [2.80334e+01, 2.58926e+02, 2.58820e+02, 4.80000e+02, 4.01263e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.1ms pre-process, 140.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.65137e+01, 2.58620e+02, 2.59465e+02, 4.79338e+02, 5.09083e-01, 5.60000e+01],\n",
      "        [2.32837e+02, 1.00605e+00, 4.59501e+02, 1.88887e+02, 3.84708e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.68158e+01, 2.59112e+02, 2.59750e+02, 4.80000e+02, 6.24180e-01, 5.60000e+01],\n",
      "        [2.30643e+02, 2.80397e+00, 4.62149e+02, 1.89765e+02, 4.37016e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 25.42290, 258.31638, 259.08521, 480.00000,   0.72171,  56.00000],\n",
      "        [233.65332,   3.54649, 461.18848, 186.79768,   0.49333,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 127.9ms inference, 1.2ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.90637e+01, 2.58489e+02, 2.57249e+02, 4.80000e+02, 5.95702e-01, 5.60000e+01],\n",
      "        [2.30242e+02, 1.34537e+00, 4.59418e+02, 1.89367e+02, 3.67435e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92705e+01, 2.58645e+02, 2.59151e+02, 4.80000e+02, 4.63459e-01, 5.60000e+01],\n",
      "        [2.30749e+02, 2.62920e+00, 4.62873e+02, 1.88166e+02, 4.58698e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.28333e+01, 2.58966e+02, 2.58010e+02, 4.80000e+02, 5.99241e-01, 5.60000e+01],\n",
      "        [2.29700e+02, 2.23103e+00, 4.64773e+02, 1.91191e+02, 4.04236e-01, 6.20000e+01],\n",
      "        [5.27520e+02, 2.57767e+02, 6.38808e+02, 4.79170e+02, 3.23887e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.9ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.67478e+01, 2.59010e+02, 2.58422e+02, 4.79993e+02, 6.88458e-01, 5.60000e+01],\n",
      "        [2.33155e+02, 3.15778e+00, 4.64939e+02, 1.90712e+02, 4.08195e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 cat, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37227e+01, 2.58754e+02, 2.58620e+02, 4.78737e+02, 6.68345e-01, 5.60000e+01],\n",
      "        [4.93944e+02, 8.75041e+01, 6.37924e+02, 4.76530e+02, 4.52672e-01, 0.00000e+00],\n",
      "        [2.32329e+02, 3.08177e+00, 4.63971e+02, 1.87927e+02, 3.80298e-01, 6.20000e+01],\n",
      "        [4.97049e+02, 7.03699e+01, 6.38478e+02, 4.67541e+02, 3.74775e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.84389e+02, 1.00437e+01, 6.38402e+02, 4.76472e+02, 5.57783e-01, 0.00000e+00],\n",
      "        [2.26311e+01, 2.59404e+02, 2.57440e+02, 4.80000e+02, 5.16554e-01, 5.60000e+01],\n",
      "        [5.05678e+02, 6.80539e+00, 6.39536e+02, 2.10772e+02, 3.49194e-01, 6.20000e+01],\n",
      "        [2.31823e+02, 3.29026e+00, 4.66802e+02, 1.91735e+02, 2.85398e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32411e+01, 2.59427e+02, 2.60025e+02, 4.80000e+02, 5.35123e-01, 5.60000e+01],\n",
      "        [2.31787e+02, 2.85463e+00, 4.59043e+02, 1.95511e+02, 5.08525e-01, 6.20000e+01],\n",
      "        [4.37214e+02, 2.05255e+01, 6.38487e+02, 4.65714e+02, 4.38716e-01, 0.00000e+00],\n",
      "        [2.55822e+02, 2.33837e+02, 5.61029e+02, 4.78003e+02, 2.99767e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.32337e+02, 2.29583e+01, 6.39441e+02, 4.76041e+02, 7.80771e-01, 0.00000e+00],\n",
      "        [1.76465e+01, 2.58599e+02, 2.60346e+02, 4.80000e+02, 5.45471e-01, 5.60000e+01],\n",
      "        [2.33288e+02, 1.35421e+00, 4.56924e+02, 1.85090e+02, 4.69871e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 108.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.32297e+02, 2.57309e+01, 6.39375e+02, 4.75699e+02, 7.69470e-01, 0.00000e+00],\n",
      "        [2.96099e+01, 2.59381e+02, 2.59845e+02, 4.79788e+02, 5.41810e-01, 5.60000e+01],\n",
      "        [2.37494e+02, 1.48341e+00, 4.72510e+02, 1.85344e+02, 3.71704e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 103.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.36429e+02, 1.67436e+01, 6.39382e+02, 4.69750e+02, 6.51851e-01, 0.00000e+00],\n",
      "        [2.22609e+01, 2.58084e+02, 2.58414e+02, 4.79912e+02, 6.00151e-01, 5.60000e+01],\n",
      "        [2.33948e+02, 1.49005e+00, 4.58856e+02, 1.86064e+02, 4.60281e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07832e+01, 2.58695e+02, 2.59758e+02, 4.79452e+02, 6.87138e-01, 5.60000e+01],\n",
      "        [5.49115e+02, 1.68749e+01, 6.39057e+02, 4.72966e+02, 6.64823e-01, 0.00000e+00],\n",
      "        [2.34340e+02, 1.59203e+00, 4.57026e+02, 1.86983e+02, 5.47847e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.2ms inference, 0.9ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94613e+01, 2.58389e+02, 2.58433e+02, 4.80000e+02, 6.23058e-01, 5.60000e+01],\n",
      "        [2.34283e+02, 1.81227e+00, 4.58406e+02, 1.82020e+02, 5.99630e-01, 6.20000e+01],\n",
      "        [5.33186e+02, 3.78884e+01, 6.39020e+02, 4.73567e+02, 3.07527e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 1.0ms pre-process, 121.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.38960, 259.28162, 258.82404, 480.00000,   0.57737,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 toilet\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.06368e+01, 2.59625e+02, 2.59993e+02, 4.80000e+02, 5.77236e-01, 5.60000e+01],\n",
      "        [2.51995e+02, 1.30119e+02, 5.77083e+02, 4.80000e+02, 3.09220e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 114.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.58204e+02, 1.03625e+02, 6.40000e+02, 3.05901e+02, 2.69439e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 123.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35541e+01, 2.60110e+02, 2.63485e+02, 4.80000e+02, 3.01306e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 136.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.81178e+01, 2.61228e+02, 2.61783e+02, 4.79729e+02, 3.86461e-01, 5.60000e+01],\n",
      "        [4.74862e+02, 1.01428e+02, 6.40000e+02, 3.00838e+02, 3.30800e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.7ms pre-process, 144.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.70660e+02, 9.61406e+01, 6.39368e+02, 2.90205e+02, 2.62917e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 114.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.09539e+01, 2.59903e+02, 2.59414e+02, 4.80000e+02, 4.72262e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 116.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.45485, 259.79578, 261.52612, 480.00000,   0.52191,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 bird\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.72776e+02, 8.70146e+01, 6.39537e+02, 2.69819e+02, 4.11096e-01, 1.40000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 120.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.67196e+02, 9.15711e+01, 6.39710e+02, 2.72941e+02, 3.67779e-01, 0.00000e+00],\n",
      "        [2.21855e+01, 2.60614e+02, 2.61754e+02, 4.80000e+02, 3.58423e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05530e+01, 2.60243e+02, 2.60567e+02, 4.80000e+02, 3.37757e-01, 5.60000e+01],\n",
      "        [4.59299e+02, 9.91244e+01, 6.40000e+02, 2.72012e+02, 2.72940e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 5.0ms pre-process, 131.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.31875e+01, 2.59798e+02, 2.60049e+02, 4.80000e+02, 4.93633e-01, 5.60000e+01],\n",
      "        [4.60491e+02, 1.00198e+02, 6.39925e+02, 2.74956e+02, 2.69545e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.15970e+01, 2.60245e+02, 2.62604e+02, 4.80000e+02, 3.21454e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 125.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.60507e+02, 1.01551e+02, 6.39803e+02, 2.70642e+02, 5.06748e-01, 0.00000e+00],\n",
      "        [1.91040e+01, 2.60263e+02, 2.58746e+02, 4.80000e+02, 4.62365e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29876e+01, 2.60495e+02, 2.60660e+02, 4.80000e+02, 4.91193e-01, 5.60000e+01],\n",
      "        [4.60861e+02, 1.02898e+02, 6.39885e+02, 2.68083e+02, 4.17547e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.19450e+01, 2.60858e+02, 2.61247e+02, 4.80000e+02, 3.25302e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.51499e+01, 2.60685e+02, 2.60558e+02, 4.80000e+02, 3.34340e-01, 5.60000e+01],\n",
      "        [4.59300e+02, 1.00025e+02, 6.39620e+02, 2.71750e+02, 3.26058e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 129.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.01599e+01, 2.60379e+02, 2.61798e+02, 4.80000e+02, 4.99692e-01, 5.60000e+01],\n",
      "        [4.56406e+02, 1.03475e+02, 6.39979e+02, 2.68967e+02, 2.73969e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.14700e+01, 2.60415e+02, 2.61862e+02, 4.80000e+02, 4.96281e-01, 5.60000e+01],\n",
      "        [4.56078e+02, 1.06689e+02, 6.39835e+02, 2.73957e+02, 3.33751e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 127.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07112e+01, 2.60661e+02, 2.60184e+02, 4.80000e+02, 5.82905e-01, 5.60000e+01],\n",
      "        [4.50167e+02, 1.08246e+02, 6.39889e+02, 2.69505e+02, 4.50456e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 108.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.07681, 260.32941, 260.40918, 480.00000,   0.50131,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.5ms pre-process, 142.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.08745e+01, 2.60816e+02, 2.60480e+02, 4.80000e+02, 5.02196e-01, 5.60000e+01],\n",
      "        [2.45780e+02, 9.16483e+01, 6.37232e+02, 4.80000e+02, 2.75077e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.77077e+01, 2.60011e+02, 2.58711e+02, 4.80000e+02, 4.76033e-01, 5.60000e+01],\n",
      "        [2.52461e+02, 9.71826e+01, 6.34105e+02, 4.75887e+02, 3.32533e-01, 0.00000e+00],\n",
      "        [4.50510e+02, 1.14652e+02, 6.39030e+02, 4.28491e+02, 2.99060e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.63791e+01, 2.59421e+02, 2.59578e+02, 4.80000e+02, 5.80523e-01, 5.60000e+01],\n",
      "        [4.61614e+02, 1.80801e+02, 6.39525e+02, 4.22176e+02, 3.29038e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 139.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.91574e+01, 2.59762e+02, 2.61127e+02, 4.80000e+02, 4.86203e-01, 5.60000e+01],\n",
      "        [4.60643e+02, 1.56740e+02, 6.39741e+02, 4.69319e+02, 3.77806e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 24.54647, 259.69922, 261.26718, 480.00000,   0.66115,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 133.2ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37929e+02, 1.81902e+00, 4.62269e+02, 1.81733e+02, 5.82568e-01, 6.20000e+01],\n",
      "        [2.63334e+01, 2.60338e+02, 2.61958e+02, 4.80000e+02, 4.06364e-01, 5.60000e+01],\n",
      "        [4.59876e+02, 6.26071e+01, 6.40000e+02, 4.59789e+02, 3.27555e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.26478e+01, 2.59000e+02, 2.60300e+02, 4.80000e+02, 5.06283e-01, 5.60000e+01],\n",
      "        [4.69179e+02, 8.39509e+01, 6.39899e+02, 4.47176e+02, 3.82114e-01, 0.00000e+00],\n",
      "        [2.35096e+02, 1.22244e+00, 4.65334e+02, 1.83968e+02, 2.77754e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 bird, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 135.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.22340e+01, 2.59408e+02, 2.61816e+02, 4.80000e+02, 5.82379e-01, 5.60000e+01],\n",
      "        [2.37413e+02, 1.24487e+00, 4.59987e+02, 1.83414e+02, 3.10689e-01, 6.20000e+01],\n",
      "        [5.28792e+02, 8.31440e+01, 6.39383e+02, 2.28006e+02, 2.52709e-01, 1.40000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 117.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.71879, 260.66589, 261.68991, 480.00000,   0.56727,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 bird, 1 chair\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.77085e+01, 2.60400e+02, 2.60260e+02, 4.80000e+02, 6.40942e-01, 5.60000e+01],\n",
      "        [4.56579e+02, 6.51425e+01, 6.39723e+02, 2.34732e+02, 3.72565e-01, 1.40000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 112.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.46710e+02, 5.84789e+01, 6.40000e+02, 2.22397e+02, 4.53076e-01, 0.00000e+00],\n",
      "        [1.80643e+01, 2.60154e+02, 2.62293e+02, 4.80000e+02, 4.30908e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 132.6ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.47598e+01, 2.60705e+02, 2.61503e+02, 4.80000e+02, 3.55971e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 99.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.15930, 259.51727, 261.24655, 480.00000,   0.51089,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 104.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.08900e+01, 2.58894e+02, 2.59316e+02, 4.80000e+02, 5.05991e-01, 5.60000e+01],\n",
      "        [5.60590e+02, 3.65430e+01, 6.39631e+02, 1.42738e+02, 2.62721e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.5ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36769e+02, 1.46709e+00, 4.55345e+02, 1.84725e+02, 6.31814e-01, 6.20000e+01],\n",
      "        [1.88206e+01, 2.59079e+02, 2.60672e+02, 4.80000e+02, 5.82109e-01, 5.60000e+01],\n",
      "        [5.78638e+02, 4.61534e+01, 6.39409e+02, 1.35091e+02, 2.63068e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37111e+02, 1.47689e+00, 4.57256e+02, 1.84097e+02, 7.14877e-01, 6.20000e+01],\n",
      "        [2.59915e+01, 2.59652e+02, 2.58746e+02, 4.80000e+02, 6.08784e-01, 5.60000e+01],\n",
      "        [5.66952e+02, 4.07102e+01, 6.40000e+02, 1.52483e+02, 4.23774e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 129.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34341e+02, 1.46963e+00, 4.58774e+02, 1.84122e+02, 5.76046e-01, 6.20000e+01],\n",
      "        [2.14698e+01, 2.59451e+02, 2.57820e+02, 4.80000e+02, 5.17361e-01, 5.60000e+01],\n",
      "        [5.60179e+02, 3.16335e+01, 6.39622e+02, 1.48667e+02, 3.53889e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36294e+02, 1.44482e+00, 4.56017e+02, 1.82883e+02, 6.83866e-01, 6.20000e+01],\n",
      "        [5.62986e+02, 2.58460e+01, 6.39776e+02, 1.39341e+02, 5.77015e-01, 0.00000e+00],\n",
      "        [2.81890e+01, 2.59889e+02, 2.59367e+02, 4.79586e+02, 5.13826e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 147.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35185e+02, 1.54843e+00, 4.56915e+02, 1.84705e+02, 6.75493e-01, 6.20000e+01],\n",
      "        [1.97178e+01, 2.59171e+02, 2.60010e+02, 4.80000e+02, 6.50188e-01, 5.60000e+01],\n",
      "        [5.59862e+02, 5.69626e+00, 6.40000e+02, 1.21967e+02, 3.70436e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.09668e+01, 2.58961e+02, 2.58997e+02, 4.80000e+02, 7.81820e-01, 5.60000e+01],\n",
      "        [5.69542e+02, 4.03608e+00, 6.40000e+02, 1.16904e+02, 5.72863e-01, 0.00000e+00],\n",
      "        [2.36080e+02, 1.10396e+00, 4.58971e+02, 1.82717e+02, 4.97203e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 30.19870, 259.81421, 258.91165, 480.00000,   0.63831,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.69385e+01, 2.59404e+02, 2.60944e+02, 4.80000e+02, 4.87839e-01, 5.60000e+01],\n",
      "        [5.91912e+02, 2.95372e-01, 6.39780e+02, 9.42379e+01, 3.74434e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 119.6ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.41875e+01, 2.59065e+02, 2.59025e+02, 4.80000e+02, 4.05131e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.85959, 259.44226, 258.04166, 480.00000,   0.62808,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 132.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 16.20354, 259.77942, 261.51880, 480.00000,   0.55809,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 toilet, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92060e+01, 2.60276e+02, 2.60551e+02, 4.80000e+02, 5.38480e-01, 5.60000e+01],\n",
      "        [2.36880e+02, 1.25557e-01, 4.57485e+02, 1.82510e+02, 2.89515e-01, 6.20000e+01],\n",
      "        [3.04931e+02, 1.24250e+02, 5.59862e+02, 3.50996e+02, 2.59446e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.54190, 259.23404, 261.76254, 480.00000,   0.72986,  56.00000],\n",
      "        [234.32263,   1.77545, 464.93713, 184.30786,   0.64316,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 140.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94793e+01, 2.58945e+02, 2.59391e+02, 4.80000e+02, 5.46562e-01, 5.60000e+01],\n",
      "        [2.33881e+02, 1.26876e+00, 4.61877e+02, 1.86838e+02, 5.43519e-01, 6.20000e+01],\n",
      "        [5.77201e+02, 9.51401e+01, 6.39569e+02, 2.13002e+02, 5.14094e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.53434e+02, 6.60452e+01, 6.39596e+02, 2.08259e+02, 5.59160e-01, 1.60000e+01],\n",
      "        [1.71608e+01, 2.60811e+02, 2.55939e+02, 4.80000e+02, 3.29784e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 100.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33633e+02, 1.77211e+00, 4.65266e+02, 1.82814e+02, 5.59003e-01, 6.20000e+01],\n",
      "        [5.69577e+02, 7.61059e+01, 6.40000e+02, 2.03989e+02, 4.68759e-01, 1.60000e+01],\n",
      "        [2.66218e+01, 2.59645e+02, 2.59527e+02, 4.80000e+02, 4.68431e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 127.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.78717e+01, 2.59311e+02, 2.60027e+02, 4.80000e+02, 5.47119e-01, 5.60000e+01],\n",
      "        [2.35334e+02, 1.26324e+00, 4.61242e+02, 1.84036e+02, 5.00840e-01, 6.20000e+01],\n",
      "        [5.81421e+02, 9.11019e+01, 6.39590e+02, 2.02939e+02, 4.28209e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 139.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.54163e+01, 2.58570e+02, 2.61462e+02, 4.80000e+02, 6.32120e-01, 5.60000e+01],\n",
      "        [2.33891e+02, 1.45976e+00, 4.61253e+02, 1.81252e+02, 5.81894e-01, 6.20000e+01],\n",
      "        [5.88934e+02, 9.23199e+01, 6.39940e+02, 1.98635e+02, 5.28625e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 132.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.89011, 259.56458, 257.35062, 480.00000,   0.52599,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 2.0ms pre-process, 109.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.01921e+01, 2.59926e+02, 2.58532e+02, 4.79610e+02, 4.22931e-01, 5.60000e+01],\n",
      "        [5.96328e+02, 6.11216e+01, 6.39585e+02, 1.61400e+02, 3.67481e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 4.0ms pre-process, 123.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.55116e+01, 2.59511e+02, 2.58905e+02, 4.80000e+02, 5.61040e-01, 5.60000e+01],\n",
      "        [6.02434e+02, 5.62890e+01, 6.39928e+02, 1.44451e+02, 2.98059e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 132.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.28590, 259.47806, 258.54437, 480.00000,   0.52932,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 117.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.59723, 259.13208, 259.10617, 480.00000,   0.56895,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 125.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 19.62520, 259.22519, 259.10010, 480.00000,   0.63215,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05890e+01, 2.59249e+02, 2.59418e+02, 4.80000e+02, 4.55328e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 117.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 14.54131, 259.27771, 259.63062, 480.00000,   0.48359,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.87557, 259.72244, 261.25821, 480.00000,   0.54550,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.96315e+01, 2.59517e+02, 2.60581e+02, 4.80000e+02, 4.00143e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 26.21004, 259.35007, 258.51624, 480.00000,   0.56809,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.83313, 259.68643, 259.77194, 480.00000,   0.51592,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 167.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 29.84158, 259.79138, 257.95825, 480.00000,   0.48904,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 28.25166, 259.64117, 248.49388, 480.00000,   0.70222,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 131.5ms inference, 2.3ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.00601e+01, 2.60378e+02, 2.39935e+02, 4.80000e+02, 4.70773e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 4.0ms pre-process, 146.5ms inference, 2.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.43091e+02, 1.23019e+01, 6.39846e+02, 1.81986e+02, 4.64526e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 dog\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.30010e+02, 7.08147e+00, 6.39505e+02, 1.85808e+02, 4.68835e-01, 1.60000e+01],\n",
      "        [5.30240e+02, 1.15043e+01, 6.39496e+02, 1.81504e+02, 3.23332e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 137.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.42494e+02, 3.66634e+01, 6.39834e+02, 2.02761e+02, 3.75954e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 137.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.56699e+02, 4.88919e+01, 6.40000e+02, 1.91675e+02, 5.53036e-01, 0.00000e+00],\n",
      "        [2.34383e+01, 2.59238e+02, 2.58461e+02, 4.80000e+02, 5.10913e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.16368e+01, 2.59456e+02, 2.57134e+02, 4.80000e+02, 6.25152e-01, 5.60000e+01],\n",
      "        [2.38231e+02, 1.66187e+00, 4.60258e+02, 1.70482e+02, 4.03040e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.70117, 259.67209, 257.35620, 480.00000,   0.66595,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 112.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.80422e+01, 2.59513e+02, 2.58248e+02, 4.80000e+02, 4.53166e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 2.0ms pre-process, 124.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.88322e+02, 0.00000e+00, 6.39779e+02, 8.73867e+01, 4.07534e-01, 1.60000e+01],\n",
      "        [2.15263e+01, 2.59629e+02, 2.59098e+02, 4.80000e+02, 3.99523e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.52506e+01, 2.60303e+02, 2.58299e+02, 4.80000e+02, 2.62179e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 107.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.64037, 260.10678, 257.20532, 480.00000,   0.48078,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 3.0ms pre-process, 105.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.71937e+01, 2.60564e+02, 2.60550e+02, 4.80000e+02, 4.51050e-01, 5.60000e+01],\n",
      "        [5.89670e+02, 3.47984e-01, 6.39687e+02, 9.64215e+01, 4.18554e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 135.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.65540e+02, 3.15651e-01, 6.39625e+02, 1.12904e+02, 5.47319e-01, 0.00000e+00],\n",
      "        [1.85233e+01, 2.59981e+02, 2.59917e+02, 4.80000e+02, 4.65361e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 chair\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.66466e+01, 2.60213e+02, 2.62598e+02, 4.80000e+02, 3.13948e-01, 5.60000e+01],\n",
      "        [5.36237e+02, 9.20422e-01, 6.39982e+02, 1.26290e+02, 3.10657e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 131.5ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 teddy bear\n",
      "Speed: 2.0ms pre-process, 115.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.11915e+02, 1.45889e-01, 6.39726e+02, 1.22139e+02, 2.94721e-01, 7.70000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 141.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.09906e+02, 1.98677e-01, 6.39333e+02, 1.21999e+02, 4.08313e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 116.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.04128e+02, 2.92168e-01, 6.39977e+02, 1.18912e+02, 4.52108e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 6.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.06833e+02, 6.93970e-01, 6.39224e+02, 8.80508e+01, 3.15713e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 143.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 104.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 124.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.1ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.1ms pre-process, 119.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 128.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 124.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 110.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 108.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 118.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 127.4ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 124.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.52606e+02, 2.00607e-01, 6.39618e+02, 1.67552e+02, 2.90421e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 127.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 109.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 1.0ms pre-process, 118.6ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.65928e+02, 0.00000e+00, 6.38909e+02, 1.82684e+02, 3.57335e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 136.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 120.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.80516e+02, 1.64474e-01, 6.39628e+02, 1.71287e+02, 3.27031e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 118.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.94194e+02, 0.00000e+00, 6.40000e+02, 1.80754e+02, 2.78879e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.98352e+02, 0.00000e+00, 6.39339e+02, 1.67778e+02, 2.70658e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 121.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.01488e+02, 0.00000e+00, 6.39154e+02, 1.69737e+02, 3.53287e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 toilet\n",
      "Speed: 4.0ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.35998e+02, 4.00932e+00, 6.28871e+02, 4.79133e+02, 3.74898e-01, 6.10000e+01],\n",
      "        [5.15586e+02, 2.09633e-01, 6.40000e+02, 1.63770e+02, 3.39077e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.1ms pre-process, 114.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 toilet\n",
      "Speed: 2.0ms pre-process, 108.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.26691e+02, 4.50294e-01, 6.39773e+02, 1.65155e+02, 4.46210e-01, 1.60000e+01],\n",
      "        [1.31711e+02, 1.53630e+01, 6.31641e+02, 4.80000e+02, 3.02037e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.5ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.07440e+02, 8.34196e+00, 6.39406e+02, 2.07058e+02, 5.27136e-01, 1.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 142.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.1ms pre-process, 126.7ms inference, 0.9ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 24.23504, 260.15085, 261.22049, 480.00000,   0.62599,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.99567e+01, 2.59825e+02, 2.60822e+02, 4.80000e+02, 4.50874e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 109.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 33.88863, 259.38568, 259.55283, 479.78448,   0.64474,  56.00000],\n",
      "        [238.11676,   1.47963, 457.82489, 183.64684,   0.62837,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 102.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.54330e+01, 2.59326e+02, 2.58756e+02, 4.80000e+02, 7.19604e-01, 5.60000e+01],\n",
      "        [2.33012e+02, 1.64603e+00, 4.64704e+02, 1.86046e+02, 6.18267e-01, 6.20000e+01],\n",
      "        [4.26294e+02, 1.40171e+01, 6.40000e+02, 4.71711e+02, 3.10332e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.37828e+01, 2.59646e+02, 2.58510e+02, 4.80000e+02, 5.78600e-01, 5.60000e+01],\n",
      "        [2.32665e+02, 1.31319e+00, 4.63758e+02, 1.89082e+02, 4.74381e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33317e+02, 2.99962e+00, 4.63922e+02, 1.91185e+02, 4.34366e-01, 6.20000e+01],\n",
      "        [2.23243e+01, 2.59216e+02, 2.58509e+02, 4.79918e+02, 4.14683e-01, 5.60000e+01],\n",
      "        [4.97168e+02, 2.75748e+01, 6.40000e+02, 3.87990e+02, 2.99957e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 134.7ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.73811e+01, 2.59395e+02, 2.60373e+02, 4.80000e+02, 5.85249e-01, 5.60000e+01],\n",
      "        [2.32734e+02, 3.13285e+00, 4.70089e+02, 1.92723e+02, 4.04473e-01, 6.20000e+01],\n",
      "        [4.82039e+02, 7.55063e+01, 6.38957e+02, 4.80000e+02, 3.31768e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 112.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[232.23967,   2.91360, 465.58484, 195.10089,   0.51375,  62.00000],\n",
      "        [ 24.01617, 259.36169, 258.67142, 480.00000,   0.49161,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[235.20892,   1.82520, 460.06329, 185.74521,   0.60672,  62.00000],\n",
      "        [ 18.89091, 259.03928, 259.08243, 480.00000,   0.59029,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 143.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35017e+02, 1.05575e+00, 4.64647e+02, 1.83352e+02, 6.47634e-01, 6.20000e+01],\n",
      "        [1.85065e+01, 2.59247e+02, 2.59820e+02, 4.78815e+02, 4.54707e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 4.0ms pre-process, 141.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86895e+02, 1.75031e+02, 4.36039e+02, 4.18727e+02, 2.61232e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 pizza\n",
      "Speed: 3.0ms pre-process, 132.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.66620e+01, 4.05897e+02, 2.60047e+02, 4.79481e+02, 3.27500e-01, 5.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.59850e+02, 1.60245e+02, 6.40000e+02, 3.96588e+02, 2.66503e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 dog, 1 toilet\n",
      "Speed: 2.5ms pre-process, 130.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.58772e+02, 1.19494e+02, 6.40000e+02, 3.60033e+02, 3.50675e-01, 1.60000e+01],\n",
      "        [2.15810e+02, 9.68069e+01, 5.12986e+02, 3.90017e+02, 2.64942e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 118.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.69391e+02, 7.36926e+01, 6.32497e+02, 4.71319e+02, 2.58850e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 113.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.46985e+02, 6.58126e+01, 6.39058e+02, 4.73866e+02, 2.56510e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 134.3ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 126.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 128.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.55752e+02, 7.08145e+01, 6.36258e+02, 4.77693e+02, 2.76099e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 121.0ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 156.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 114.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 135.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.67990e+02, 6.36234e+01, 6.34452e+02, 4.74513e+02, 2.53385e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 105.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 132.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 150.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 117.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.30956e+02, 7.80504e+01, 6.38118e+02, 4.03263e+02, 3.08365e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 121.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.42195e+02, 7.34429e+01, 6.40000e+02, 4.76489e+02, 3.20948e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 151.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 1.0ms pre-process, 120.4ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 133.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 122.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 125.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 110.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 toilet\n",
      "Speed: 1.0ms pre-process, 112.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.53234e+02, 8.38283e+01, 6.40000e+02, 3.93990e+02, 2.87591e-01, 6.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 115.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 114.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 130.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 119.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.0ms pre-process, 157.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 3.1ms pre-process, 111.4ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.2ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 0.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 121.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.84991e+01, 2.59233e+02, 2.64051e+02, 4.80000e+02, 3.71624e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.46532, 259.88245, 261.21570, 480.00000,   0.56868,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 6.0ms pre-process, 147.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.89899e+01, 2.59647e+02, 2.58723e+02, 4.80000e+02, 4.52245e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.1ms pre-process, 119.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.11792e+01, 2.59633e+02, 2.58251e+02, 4.80000e+02, 5.22046e-01, 5.60000e+01],\n",
      "        [2.35556e+02, 1.73093e+00, 4.58502e+02, 1.81506e+02, 4.13300e-01, 6.20000e+01],\n",
      "        [3.38588e+02, 1.99451e+01, 6.40000e+02, 4.71957e+02, 3.08971e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38960e+01, 2.58883e+02, 2.57771e+02, 4.80000e+02, 6.43177e-01, 5.60000e+01],\n",
      "        [5.41258e+02, 1.62146e+01, 6.39873e+02, 4.71973e+02, 6.39613e-01, 0.00000e+00],\n",
      "        [2.33178e+02, 1.02530e+00, 4.62136e+02, 1.87304e+02, 3.96799e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 stop sign, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 165.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33168e+02, 2.62201e+00, 4.61366e+02, 1.89963e+02, 4.36525e-01, 6.20000e+01],\n",
      "        [1.76477e+01, 2.58651e+02, 2.59860e+02, 4.78719e+02, 4.01877e-01, 5.60000e+01],\n",
      "        [2.58636e+02, 2.34604e+02, 5.56368e+02, 4.78255e+02, 2.97507e-01, 6.20000e+01],\n",
      "        [5.36643e+02, 2.55140e+01, 6.38537e+02, 2.10994e+02, 2.80665e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 136.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.73783e+01, 2.59067e+02, 2.59274e+02, 4.80000e+02, 5.58583e-01, 5.60000e+01],\n",
      "        [2.33491e+02, 3.04605e+00, 4.62438e+02, 1.90835e+02, 4.09412e-01, 6.20000e+01],\n",
      "        [5.38107e+02, 5.04370e+01, 6.40000e+02, 4.27310e+02, 3.84938e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.44193e+01, 2.59554e+02, 2.59851e+02, 4.79666e+02, 5.79115e-01, 5.60000e+01],\n",
      "        [2.33809e+02, 3.71745e+00, 4.60933e+02, 1.89585e+02, 5.54360e-01, 6.20000e+01],\n",
      "        [5.48102e+02, 4.21488e+01, 6.39087e+02, 4.75746e+02, 5.20809e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 110.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29293e+01, 2.58792e+02, 2.60111e+02, 4.80000e+02, 6.76077e-01, 5.60000e+01],\n",
      "        [2.32118e+02, 3.70815e+00, 4.64240e+02, 1.87897e+02, 4.63322e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 131.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.28518, 258.89795, 257.53918, 480.00000,   0.59413,  56.00000],\n",
      "        [231.16879,   2.43777, 462.07761, 190.23430,   0.54006,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07799e+01, 2.58915e+02, 2.60246e+02, 4.80000e+02, 6.02599e-01, 5.60000e+01],\n",
      "        [2.33657e+02, 1.37534e+00, 4.59580e+02, 1.88243e+02, 2.80787e-01, 6.20000e+01],\n",
      "        [5.59467e+02, 5.80373e+01, 6.39588e+02, 4.77173e+02, 2.74291e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 stop sign, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 106.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.10080e+01, 2.59647e+02, 2.58817e+02, 4.80000e+02, 6.46621e-01, 5.60000e+01],\n",
      "        [5.55626e+02, 1.92574e+02, 6.39373e+02, 4.77884e+02, 5.96206e-01, 0.00000e+00],\n",
      "        [2.34001e+02, 4.00454e+00, 4.61398e+02, 1.88472e+02, 3.81609e-01, 6.20000e+01],\n",
      "        [4.73189e+02, 5.98851e+01, 6.39305e+02, 2.42981e+02, 3.49575e-01, 1.10000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 4.0ms pre-process, 127.8ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.90469e+01, 2.58767e+02, 2.58548e+02, 4.80000e+02, 6.39964e-01, 5.60000e+01],\n",
      "        [2.31356e+02, 3.22193e+00, 4.64605e+02, 1.90243e+02, 5.37961e-01, 6.20000e+01],\n",
      "        [2.58706e+02, 2.32297e+02, 5.59677e+02, 4.80000e+02, 2.79739e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 147.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.62482e+02, 1.11193e+01, 6.39213e+02, 4.70110e+02, 7.54533e-01, 0.00000e+00],\n",
      "        [2.34999e+02, 2.47848e+00, 4.59969e+02, 1.87046e+02, 6.15216e-01, 6.20000e+01],\n",
      "        [1.94539e+01, 2.60223e+02, 2.58306e+02, 4.79593e+02, 4.31046e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 145.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.28786e+01, 2.58627e+02, 2.59808e+02, 4.80000e+02, 8.10034e-01, 5.60000e+01],\n",
      "        [4.46315e+02, 6.07948e+00, 6.38526e+02, 4.72454e+02, 6.44488e-01, 0.00000e+00],\n",
      "        [2.31445e+02, 3.73593e+00, 4.61691e+02, 1.88966e+02, 5.26710e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 132.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36166e+02, 2.98509e+00, 4.81346e+02, 1.90238e+02, 5.56586e-01, 6.20000e+01],\n",
      "        [2.60543e+01, 2.58230e+02, 2.56914e+02, 4.80000e+02, 5.13582e-01, 5.60000e+01],\n",
      "        [4.36988e+02, 8.17945e+01, 6.37500e+02, 4.76938e+02, 5.04948e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[401.30078,  13.56558, 640.00000, 477.54037,   0.68885,   0.00000],\n",
      "        [233.61813,   2.46188, 455.50424, 188.87030,   0.67573,  62.00000],\n",
      "        [ 19.57597, 259.69321, 259.25323, 480.00000,   0.65418,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 117.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34605e+02, 1.02590e+00, 4.56433e+02, 1.85935e+02, 6.68299e-01, 6.20000e+01],\n",
      "        [2.28510e+01, 2.59920e+02, 2.58615e+02, 4.80000e+02, 4.63795e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.23563,   1.02536, 460.35965, 186.40167,   0.56011,  62.00000],\n",
      "        [ 22.90208, 259.08395, 257.34448, 480.00000,   0.55182,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.94813, 259.44360, 258.45221, 480.00000,   0.71682,  56.00000],\n",
      "        [235.39554,   1.82135, 460.91183, 185.80612,   0.61246,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 166.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38067e+02, 1.75899e+00, 4.61318e+02, 1.80642e+02, 5.49704e-01, 6.20000e+01],\n",
      "        [2.51097e+01, 2.60123e+02, 2.59907e+02, 4.80000e+02, 3.22361e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 129.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.70481, 259.49323, 258.95792, 480.00000,   0.59642,  56.00000],\n",
      "        [234.94521,   1.76088, 464.95789, 185.57314,   0.58594,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 141.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36251e+02, 2.07207e+00, 4.62497e+02, 1.87822e+02, 5.58824e-01, 6.20000e+01],\n",
      "        [2.37150e+01, 2.60084e+02, 2.59268e+02, 4.80000e+02, 4.55005e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 123.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 26.39322, 259.12457, 259.60165, 480.00000,   0.67774,  56.00000],\n",
      "        [235.10844,   1.47524, 460.30090, 188.25421,   0.58413,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.55475,   1.07789, 461.35828, 188.72391,   0.55110,  62.00000],\n",
      "        [ 20.62550, 259.26694, 260.62686, 480.00000,   0.54474,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 133.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.75500, 258.93774, 259.75238, 480.00000,   0.64829,  56.00000],\n",
      "        [235.76410,   1.65933, 459.09979, 184.18607,   0.59496,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 142.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.03603e+01, 2.59017e+02, 2.60171e+02, 4.80000e+02, 5.96427e-01, 5.60000e+01],\n",
      "        [2.37334e+02, 1.39005e+00, 4.60665e+02, 1.77436e+02, 4.10550e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36741e+02, 1.79514e+00, 4.59495e+02, 1.80980e+02, 4.89044e-01, 6.20000e+01],\n",
      "        [2.62231e+01, 2.60372e+02, 2.61455e+02, 4.80000e+02, 4.21003e-01, 5.60000e+01],\n",
      "        [4.81496e+02, 2.03940e+02, 6.39845e+02, 3.60170e+02, 2.74501e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 130.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.59152e+01, 2.59059e+02, 2.60078e+02, 4.80000e+02, 6.27930e-01, 5.60000e+01],\n",
      "        [4.73865e+02, 1.44244e+02, 6.39782e+02, 3.13007e+02, 5.03073e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 128.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.96451e+01, 2.60497e+02, 2.59416e+02, 4.80000e+02, 3.82326e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.15775, 259.79593, 258.75064, 480.00000,   0.64381,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 128.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.32512, 259.59607, 257.37640, 480.00000,   0.61901,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 124.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.70702e+01, 2.58080e+02, 2.59892e+02, 4.80000e+02, 6.99682e-01, 5.60000e+01],\n",
      "        [4.77034e+02, 1.16212e+02, 6.39417e+02, 2.73473e+02, 3.84275e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 132.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.62370e+01, 2.58981e+02, 2.59249e+02, 4.80000e+02, 5.64132e-01, 5.60000e+01],\n",
      "        [4.78388e+02, 1.19674e+02, 6.39906e+02, 2.78618e+02, 3.18350e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 6.0ms pre-process, 135.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38078e+01, 2.60657e+02, 2.59221e+02, 4.80000e+02, 5.26296e-01, 5.60000e+01],\n",
      "        [4.85125e+02, 1.13205e+02, 6.39046e+02, 2.82274e+02, 2.66753e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86946e+01, 2.59444e+02, 2.59320e+02, 4.80000e+02, 5.42838e-01, 5.60000e+01],\n",
      "        [4.92829e+02, 1.19912e+02, 6.39468e+02, 2.90903e+02, 4.10748e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.85619e+01, 2.59531e+02, 2.60827e+02, 4.80000e+02, 5.40915e-01, 5.60000e+01],\n",
      "        [4.93389e+02, 1.19997e+02, 6.39761e+02, 2.90687e+02, 3.47861e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.02363e+01, 2.59548e+02, 2.59406e+02, 4.80000e+02, 6.92490e-01, 5.60000e+01],\n",
      "        [4.96845e+02, 1.26496e+02, 6.39838e+02, 2.96588e+02, 3.14423e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 1.0ms pre-process, 126.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.43501, 259.75113, 259.09250, 480.00000,   0.57127,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.19128, 259.74088, 258.71005, 480.00000,   0.74415,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 147.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.79024e+01, 2.59765e+02, 2.56948e+02, 4.80000e+02, 6.80951e-01, 5.60000e+01],\n",
      "        [5.12884e+02, 1.29482e+02, 6.39498e+02, 2.98806e+02, 2.66776e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 114.5ms inference, 3.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.69520, 260.05109, 258.69772, 480.00000,   0.69103,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 134.8ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 22.23077, 259.79962, 259.36304, 480.00000,   0.67273,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 143.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.08892, 259.25934, 260.12741, 480.00000,   0.63222,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 15.80498, 259.52426, 260.57178, 480.00000,   0.63127,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 129.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 27.73538, 259.74112, 258.33475, 480.00000,   0.70450,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.72286, 259.49933, 260.03320, 480.00000,   0.64769,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 129.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.15653e+01, 2.59849e+02, 2.60080e+02, 4.80000e+02, 3.89498e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 127.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 30.25252, 258.97034, 258.50604, 480.00000,   0.64318,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 128.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 25.66010, 260.50122, 258.90326, 480.00000,   0.49985,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 144.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.07699, 260.48248, 260.35876, 480.00000,   0.50236,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 130.0ms inference, 1.1ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.17151e+01, 2.59521e+02, 2.61069e+02, 4.79422e+02, 4.61543e-01, 5.60000e+01],\n",
      "        [2.38905e+02, 1.51778e+00, 4.55497e+02, 1.82210e+02, 4.38246e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 123.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.96098, 259.31729, 260.64252, 480.00000,   0.65485,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.38524e+02, 1.66107e-01, 4.55963e+02, 1.86121e+02, 6.94450e-01, 6.20000e+01],\n",
      "        [2.04179e+01, 2.59474e+02, 2.60007e+02, 4.80000e+02, 6.32986e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.59818e+01, 2.60608e+02, 2.59909e+02, 4.80000e+02, 6.11249e-01, 5.60000e+01],\n",
      "        [2.32656e+02, 4.88267e+00, 5.89248e+02, 1.89057e+02, 4.45515e-01, 6.20000e+01],\n",
      "        [5.10559e+02, 2.31093e+01, 6.39104e+02, 4.74424e+02, 2.98461e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 117.6ms inference, 1.4ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.84247e+02, 2.65692e+01, 6.38478e+02, 4.61964e+02, 6.34370e-01, 0.00000e+00],\n",
      "        [2.37919e+02, 3.25874e-01, 5.39894e+02, 2.14780e+02, 5.37980e-01, 6.20000e+01],\n",
      "        [2.14893e+01, 2.60098e+02, 2.59047e+02, 4.80000e+02, 5.03125e-01, 5.60000e+01],\n",
      "        [4.86623e+02, 3.42716e+00, 6.38254e+02, 2.10336e+02, 3.45731e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.77451e+02, 4.11391e+00, 6.38004e+02, 4.65315e+02, 6.68019e-01, 0.00000e+00],\n",
      "        [1.88603e+01, 2.59497e+02, 2.59553e+02, 4.80000e+02, 5.95253e-01, 5.60000e+01],\n",
      "        [2.40014e+02, 1.97752e+00, 4.69872e+02, 1.82345e+02, 5.82778e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 123.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36070e+02, 2.10538e+00, 4.55822e+02, 1.84586e+02, 5.76597e-01, 6.20000e+01],\n",
      "        [4.90765e+02, 5.29251e+01, 6.40000e+02, 4.67801e+02, 5.33279e-01, 0.00000e+00],\n",
      "        [2.25573e+01, 2.59015e+02, 2.57780e+02, 4.80000e+02, 5.17076e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 21.69344, 259.14301, 259.47009, 480.00000,   0.71405,  56.00000],\n",
      "        [234.27412,   2.75863, 461.59277, 191.37756,   0.49935,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 123.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.90547e+01, 2.59219e+02, 2.59483e+02, 4.80000e+02, 7.14745e-01, 5.60000e+01],\n",
      "        [2.33891e+02, 2.91878e+00, 4.63718e+02, 1.88617e+02, 4.45007e-01, 6.20000e+01],\n",
      "        [2.56388e+02, 2.33302e+02, 5.59674e+02, 4.77809e+02, 2.61780e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.15035e+01, 2.59108e+02, 2.57545e+02, 4.80000e+02, 5.92189e-01, 5.60000e+01],\n",
      "        [2.35062e+02, 3.50409e+00, 4.61655e+02, 1.87560e+02, 4.66683e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.19008, 260.04669, 258.29596, 480.00000,   0.59998,  56.00000],\n",
      "        [235.15707,   2.84151, 462.27664, 188.79156,   0.56687,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 132.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.74812e+02, 1.43277e+02, 6.38921e+02, 2.35706e+02, 5.51505e-01, 0.00000e+00],\n",
      "        [2.01724e+01, 2.59647e+02, 2.57092e+02, 4.78796e+02, 5.48882e-01, 5.60000e+01],\n",
      "        [2.37555e+02, 1.90781e+00, 4.73237e+02, 1.85740e+02, 5.32104e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30516e+01, 2.59332e+02, 2.55968e+02, 4.80000e+02, 6.27009e-01, 5.60000e+01],\n",
      "        [2.37697e+02, 1.83863e+00, 4.69405e+02, 1.81739e+02, 5.31037e-01, 6.20000e+01],\n",
      "        [4.67643e+02, 1.40539e+02, 6.40000e+02, 2.32830e+02, 4.92737e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 2 tvs\n",
      "Speed: 1.6ms pre-process, 118.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.24252e+01, 2.58677e+02, 2.57587e+02, 4.80000e+02, 6.19215e-01, 5.60000e+01],\n",
      "        [2.38606e+02, 2.20760e+00, 4.64859e+02, 1.80999e+02, 5.13064e-01, 6.20000e+01],\n",
      "        [4.64611e+02, 1.54100e+02, 6.37584e+02, 2.38819e+02, 5.02258e-01, 0.00000e+00],\n",
      "        [2.57314e+02, 2.25935e+02, 5.58913e+02, 4.78143e+02, 2.85295e-01, 6.20000e+01],\n",
      "        [4.42440e+02, 1.43960e+02, 6.36686e+02, 4.72645e+02, 2.67465e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.75926e+01, 2.57475e+02, 2.58734e+02, 4.80000e+02, 5.70982e-01, 5.60000e+01],\n",
      "        [2.35417e+02, 2.07102e+00, 4.76126e+02, 1.86408e+02, 4.42722e-01, 6.20000e+01],\n",
      "        [4.67765e+02, 1.56084e+02, 6.38648e+02, 2.35277e+02, 4.39582e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29327e+01, 2.59278e+02, 2.58776e+02, 4.80000e+02, 5.43713e-01, 5.60000e+01],\n",
      "        [2.34419e+02, 1.19840e+00, 4.57533e+02, 1.87976e+02, 5.37997e-01, 6.20000e+01],\n",
      "        [4.60822e+02, 1.53925e+02, 6.39498e+02, 2.37887e+02, 3.07149e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 117.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.40422e+02, 2.15456e+00, 4.59794e+02, 1.83493e+02, 5.77111e-01, 6.20000e+01],\n",
      "        [2.13599e+01, 2.59271e+02, 2.59292e+02, 4.79414e+02, 5.75531e-01, 5.60000e+01],\n",
      "        [4.63549e+02, 1.59753e+02, 6.39712e+02, 2.38493e+02, 3.47553e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36084e+02, 1.69470e+00, 4.57000e+02, 1.87608e+02, 5.32677e-01, 6.20000e+01],\n",
      "        [1.71043e+01, 2.59030e+02, 2.59422e+02, 4.80000e+02, 3.22087e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.84490e+01, 2.59111e+02, 2.58132e+02, 4.80000e+02, 8.24196e-01, 5.60000e+01],\n",
      "        [2.36370e+02, 2.05346e+00, 4.71219e+02, 1.83900e+02, 5.47222e-01, 6.20000e+01],\n",
      "        [4.65749e+02, 1.52795e+02, 6.39736e+02, 2.38548e+02, 3.37447e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 oven\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.79302e+01, 2.59462e+02, 2.58282e+02, 4.80000e+02, 6.62979e-01, 5.60000e+01],\n",
      "        [4.67139e+02, 1.57607e+02, 6.34823e+02, 2.37205e+02, 6.42098e-01, 0.00000e+00],\n",
      "        [2.33308e+02, 2.34823e+00, 4.90399e+02, 1.96567e+02, 4.70314e-01, 6.20000e+01],\n",
      "        [2.56567e+02, 2.27781e+02, 5.60960e+02, 4.77621e+02, 2.77151e-01, 6.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.42424e+01, 2.58664e+02, 2.58450e+02, 4.80000e+02, 7.02736e-01, 5.60000e+01],\n",
      "        [4.65675e+02, 1.52170e+02, 6.35835e+02, 2.39687e+02, 6.88485e-01, 0.00000e+00],\n",
      "        [2.39055e+02, 2.34741e+00, 4.67637e+02, 1.82002e+02, 5.22039e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.15472e+01, 2.59375e+02, 2.58822e+02, 4.80000e+02, 6.97067e-01, 5.60000e+01],\n",
      "        [2.41145e+02, 2.50979e+00, 4.59539e+02, 1.80624e+02, 6.80525e-01, 6.20000e+01],\n",
      "        [4.62676e+02, 1.48868e+02, 6.38304e+02, 2.51460e+02, 6.20149e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34466e+02, 2.53937e+00, 4.65094e+02, 1.92466e+02, 5.01217e-01, 6.20000e+01],\n",
      "        [2.02031e+01, 2.59270e+02, 2.58772e+02, 4.80000e+02, 3.02605e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 125.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.13517e+01, 2.58247e+02, 2.56939e+02, 4.79684e+02, 5.88030e-01, 5.60000e+01],\n",
      "        [2.34724e+02, 1.82924e+00, 4.55439e+02, 1.87231e+02, 5.50244e-01, 6.20000e+01],\n",
      "        [2.58870e+02, 2.32898e+02, 5.61936e+02, 4.76749e+02, 2.94036e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.5ms pre-process, 159.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.83313e+01, 2.59217e+02, 2.58302e+02, 4.79953e+02, 4.70531e-01, 5.60000e+01],\n",
      "        [2.31260e+02, 3.09216e+00, 4.60747e+02, 1.88360e+02, 4.44659e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 121.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.65955e+01, 2.58400e+02, 2.59171e+02, 4.79357e+02, 5.84046e-01, 5.60000e+01],\n",
      "        [2.33895e+02, 2.03061e+00, 4.58475e+02, 1.87548e+02, 4.64758e-01, 6.20000e+01],\n",
      "        [2.57635e+02, 2.32804e+02, 5.61843e+02, 4.77601e+02, 3.51416e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 125.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.52406e+01, 2.58949e+02, 2.57203e+02, 4.80000e+02, 5.81965e-01, 5.60000e+01],\n",
      "        [2.34169e+02, 1.58607e+00, 4.58378e+02, 1.88770e+02, 4.57408e-01, 6.20000e+01],\n",
      "        [5.54793e+02, 1.37838e+02, 6.39489e+02, 4.76240e+02, 3.98175e-01, 0.00000e+00],\n",
      "        [2.57509e+02, 2.32491e+02, 5.63305e+02, 4.77208e+02, 3.83033e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.01625e+01, 2.59453e+02, 2.57511e+02, 4.79569e+02, 5.52201e-01, 5.60000e+01],\n",
      "        [2.32191e+02, 2.74969e+00, 4.60350e+02, 1.87138e+02, 4.35911e-01, 6.20000e+01],\n",
      "        [5.56588e+02, 6.36893e+01, 6.38820e+02, 4.80000e+02, 2.91585e-01, 0.00000e+00],\n",
      "        [2.58243e+02, 2.33848e+02, 5.61129e+02, 4.77542e+02, 2.79447e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 121.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.87228e+01, 2.58778e+02, 2.57860e+02, 4.80000e+02, 5.50286e-01, 5.60000e+01],\n",
      "        [2.34085e+02, 2.60908e+00, 4.58140e+02, 1.88092e+02, 4.21012e-01, 6.20000e+01],\n",
      "        [2.57525e+02, 2.32568e+02, 5.62513e+02, 4.77646e+02, 3.13906e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.04030e+01, 2.58529e+02, 2.58215e+02, 4.80000e+02, 6.95980e-01, 5.60000e+01],\n",
      "        [2.33596e+02, 1.24326e+00, 4.57420e+02, 1.87683e+02, 5.43150e-01, 6.20000e+01],\n",
      "        [5.63743e+02, 4.73927e+01, 6.38974e+02, 4.74027e+02, 3.63458e-01, 0.00000e+00],\n",
      "        [2.55815e+02, 2.33116e+02, 5.62356e+02, 4.78380e+02, 2.54680e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 129.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.91504e+01, 2.59964e+02, 2.58164e+02, 4.80000e+02, 5.62669e-01, 5.60000e+01],\n",
      "        [2.31678e+02, 2.29089e+00, 4.62270e+02, 1.90077e+02, 4.81788e-01, 6.20000e+01],\n",
      "        [5.65099e+02, 4.74614e+01, 6.39029e+02, 4.72510e+02, 3.73225e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 117.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.36975e+02, 3.68496e+00, 4.62531e+02, 1.91049e+02, 5.16342e-01, 6.20000e+01],\n",
      "        [2.70802e+01, 2.60221e+02, 2.60591e+02, 4.80000e+02, 3.88147e-01, 5.60000e+01],\n",
      "        [5.71575e+02, 5.81778e+01, 6.38910e+02, 4.71170e+02, 3.50948e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.78830e+01, 2.58936e+02, 2.57752e+02, 4.80000e+02, 5.46853e-01, 5.60000e+01],\n",
      "        [2.33709e+02, 1.53415e+00, 4.59386e+02, 1.88775e+02, 4.03704e-01, 6.20000e+01],\n",
      "        [5.69524e+02, 5.37675e+01, 6.39209e+02, 4.73997e+02, 3.34701e-01, 0.00000e+00],\n",
      "        [2.55782e+02, 2.32972e+02, 5.61713e+02, 4.77721e+02, 2.86906e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.9ms pre-process, 123.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.39986e+01, 2.59172e+02, 2.60723e+02, 4.79570e+02, 4.81248e-01, 5.60000e+01],\n",
      "        [2.34067e+02, 1.56851e+00, 4.55824e+02, 1.89244e+02, 4.46920e-01, 6.20000e+01],\n",
      "        [2.55005e+02, 2.30859e+02, 5.70939e+02, 4.78254e+02, 3.26953e-01, 6.20000e+01],\n",
      "        [5.72383e+02, 4.44051e+01, 6.39517e+02, 4.59105e+02, 3.26615e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.77102e+01, 2.58910e+02, 2.59646e+02, 4.77502e+02, 5.23460e-01, 5.60000e+01],\n",
      "        [5.66948e+02, 4.23676e+01, 6.39726e+02, 4.47149e+02, 4.21278e-01, 0.00000e+00],\n",
      "        [2.35411e+02, 2.93707e+00, 4.75153e+02, 2.06334e+02, 3.21990e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 119.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98320e+01, 2.59197e+02, 2.58571e+02, 4.80000e+02, 4.91231e-01, 5.60000e+01],\n",
      "        [2.31871e+02, 2.52164e+00, 4.61677e+02, 1.92067e+02, 4.45658e-01, 6.20000e+01],\n",
      "        [5.67433e+02, 3.68124e+01, 6.39632e+02, 4.80000e+02, 3.42551e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 179.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.50988e+01, 2.59135e+02, 2.59593e+02, 4.80000e+02, 5.30069e-01, 5.60000e+01],\n",
      "        [2.31315e+02, 2.42423e+00, 4.63925e+02, 1.89782e+02, 4.41214e-01, 6.20000e+01],\n",
      "        [5.75127e+02, 4.54398e+01, 6.39895e+02, 3.09453e+02, 3.09016e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33105e+02, 2.85628e+00, 4.60329e+02, 1.88811e+02, 4.91617e-01, 6.20000e+01],\n",
      "        [1.75787e+01, 2.59682e+02, 2.57940e+02, 4.80000e+02, 3.51188e-01, 5.60000e+01],\n",
      "        [5.88365e+02, 2.15233e+02, 6.39437e+02, 4.80000e+02, 3.41383e-01, 0.00000e+00],\n",
      "        [2.58190e+02, 2.32796e+02, 5.60095e+02, 4.77273e+02, 2.64387e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 118.8ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.63419e+01, 2.59156e+02, 2.58413e+02, 4.80000e+02, 5.94356e-01, 5.60000e+01],\n",
      "        [2.33199e+02, 2.67200e+00, 4.59132e+02, 1.89669e+02, 4.38020e-01, 6.20000e+01],\n",
      "        [5.70081e+02, 4.41951e+01, 6.39573e+02, 3.06361e+02, 2.72975e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.84333e+01, 2.59491e+02, 2.59496e+02, 4.79778e+02, 4.87861e-01, 5.60000e+01],\n",
      "        [2.33288e+02, 1.81479e+00, 4.58956e+02, 1.90118e+02, 4.35913e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 126.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.31504e+02, 2.95514e+00, 4.64275e+02, 1.94390e+02, 5.27161e-01, 6.20000e+01],\n",
      "        [1.79366e+01, 2.59014e+02, 2.60251e+02, 4.80000e+02, 5.01388e-01, 5.60000e+01],\n",
      "        [2.58947e+02, 2.32359e+02, 5.62510e+02, 4.76722e+02, 3.82134e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 125.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.51880e+01, 2.59408e+02, 2.58274e+02, 4.80000e+02, 3.20824e-01, 5.60000e+01],\n",
      "        [2.34128e+02, 1.82503e+00, 4.57318e+02, 1.89374e+02, 3.07302e-01, 6.20000e+01],\n",
      "        [5.88663e+02, 1.77004e+02, 6.38998e+02, 4.76463e+02, 2.55491e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 131.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.95140e+01, 2.59130e+02, 2.58404e+02, 4.80000e+02, 5.36597e-01, 5.60000e+01],\n",
      "        [2.31837e+02, 2.67305e+00, 4.61137e+02, 1.87936e+02, 4.62051e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 148.7ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.57136e+01, 2.59619e+02, 2.59740e+02, 4.79971e+02, 5.33204e-01, 5.60000e+01],\n",
      "        [2.31528e+02, 3.14908e+00, 4.62592e+02, 1.88653e+02, 3.83013e-01, 6.20000e+01],\n",
      "        [5.90357e+02, 1.81448e+02, 6.39593e+02, 4.73586e+02, 2.55924e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.71427e+01, 2.59261e+02, 2.58170e+02, 4.79023e+02, 5.68894e-01, 5.60000e+01],\n",
      "        [2.31659e+02, 3.07227e+00, 4.60641e+02, 1.92358e+02, 4.59002e-01, 6.20000e+01],\n",
      "        [2.54933e+02, 2.33255e+02, 5.60330e+02, 4.77055e+02, 2.56371e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 1.0ms pre-process, 127.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 19.09401, 259.58978, 258.67993, 479.83331,   0.54273,  56.00000],\n",
      "        [236.77298,   1.74516, 456.07120, 188.78815,   0.51954,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 4.0ms pre-process, 138.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.29872e+02, 2.72888e+00, 4.60251e+02, 1.89192e+02, 4.26340e-01, 6.20000e+01],\n",
      "        [2.20388e+01, 2.59626e+02, 2.57932e+02, 4.79202e+02, 4.14088e-01, 5.60000e+01],\n",
      "        [2.56015e+02, 2.32884e+02, 5.62311e+02, 4.77011e+02, 2.95783e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 1.0ms pre-process, 121.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.76776e+01, 2.58790e+02, 2.58391e+02, 4.80000e+02, 6.95507e-01, 5.60000e+01],\n",
      "        [2.32827e+02, 2.17113e+00, 4.64643e+02, 2.01392e+02, 3.89772e-01, 6.20000e+01],\n",
      "        [2.55930e+02, 2.31277e+02, 5.63416e+02, 4.77533e+02, 2.83344e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 152.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92158e+01, 2.58955e+02, 2.59615e+02, 4.80000e+02, 5.17411e-01, 5.60000e+01],\n",
      "        [2.32331e+02, 3.50801e+00, 4.62781e+02, 1.92746e+02, 3.63008e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.75558e+01, 2.58368e+02, 2.57344e+02, 4.80000e+02, 6.00212e-01, 5.60000e+01],\n",
      "        [2.34725e+02, 1.00574e+00, 4.58427e+02, 1.89750e+02, 4.34372e-01, 6.20000e+01],\n",
      "        [5.67968e+02, 3.74325e+01, 6.39492e+02, 2.71266e+02, 2.51212e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.5ms pre-process, 119.8ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.26604e+01, 2.59464e+02, 2.57733e+02, 4.79756e+02, 5.93343e-01, 5.60000e+01],\n",
      "        [2.31951e+02, 1.27182e+00, 4.59811e+02, 1.88889e+02, 5.14134e-01, 6.20000e+01],\n",
      "        [5.66282e+02, 3.40945e+01, 6.39784e+02, 3.02175e+02, 3.42690e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.78294e+01, 2.60090e+02, 2.58536e+02, 4.80000e+02, 5.28211e-01, 5.60000e+01],\n",
      "        [2.33601e+02, 1.46688e+00, 4.58161e+02, 1.85891e+02, 4.06094e-01, 6.20000e+01],\n",
      "        [5.69078e+02, 4.48328e+01, 6.39668e+02, 4.80000e+02, 2.76269e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 123.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86789e+01, 2.59081e+02, 2.58806e+02, 4.80000e+02, 6.71574e-01, 5.60000e+01],\n",
      "        [2.32691e+02, 2.74575e+00, 4.62977e+02, 1.92798e+02, 4.06401e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 124.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.13995e+01, 2.58813e+02, 2.58744e+02, 4.80000e+02, 5.52090e-01, 5.60000e+01],\n",
      "        [2.28386e+02, 3.15720e+00, 4.62303e+02, 1.88406e+02, 4.37268e-01, 6.20000e+01],\n",
      "        [5.89461e+02, 1.60409e+02, 6.39344e+02, 4.77461e+02, 4.14324e-01, 0.00000e+00],\n",
      "        [2.53655e+02, 2.34829e+02, 5.62333e+02, 4.77115e+02, 2.85943e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 0.8ms pre-process, 120.5ms inference, 1.5ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.99767e+01, 2.59239e+02, 2.57360e+02, 4.80000e+02, 5.95678e-01, 5.60000e+01],\n",
      "        [2.33405e+02, 2.87650e+00, 4.61135e+02, 1.89040e+02, 4.16390e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 3.5ms pre-process, 143.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07260e+01, 2.59159e+02, 2.61423e+02, 4.79330e+02, 4.43412e-01, 5.60000e+01],\n",
      "        [2.29384e+02, 2.82760e+00, 4.63257e+02, 1.88590e+02, 3.86019e-01, 6.20000e+01],\n",
      "        [2.56753e+02, 2.33708e+02, 5.63482e+02, 4.78501e+02, 2.70316e-01, 6.20000e+01],\n",
      "        [5.65464e+02, 3.63209e+01, 6.39523e+02, 2.93280e+02, 2.50190e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 148.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.48864e+01, 2.58661e+02, 2.59997e+02, 4.80000e+02, 5.29110e-01, 5.60000e+01],\n",
      "        [2.31718e+02, 2.78762e+00, 4.62621e+02, 1.87408e+02, 4.24559e-01, 6.20000e+01],\n",
      "        [5.62931e+02, 3.58977e+01, 6.39597e+02, 2.58240e+02, 2.88301e-01, 0.00000e+00],\n",
      "        [2.57111e+02, 2.33144e+02, 5.64157e+02, 4.78125e+02, 2.54440e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.05588e+01, 2.59776e+02, 2.58575e+02, 4.80000e+02, 4.90764e-01, 5.60000e+01],\n",
      "        [2.35129e+02, 3.41145e+00, 4.60735e+02, 1.88517e+02, 4.83309e-01, 6.20000e+01],\n",
      "        [2.51654e+02, 2.35403e+02, 5.61506e+02, 4.76890e+02, 3.78318e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 125.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.33820e+02, 1.53417e+00, 4.55409e+02, 1.90163e+02, 5.13593e-01, 6.20000e+01],\n",
      "        [1.80466e+01, 2.59757e+02, 2.51318e+02, 4.78998e+02, 2.54688e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 tv\n",
      "Speed: 3.0ms pre-process, 149.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.30618e+02, 1.83868e+00, 4.62030e+02, 1.95203e+02, 2.90152e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.59630e+02, 5.00412e+01, 6.39157e+02, 4.78699e+02, 4.08986e-01, 0.00000e+00],\n",
      "        [1.97998e+01, 2.60371e+02, 2.58681e+02, 4.79380e+02, 3.36936e-01, 5.60000e+01],\n",
      "        [2.32840e+02, 8.70293e-01, 4.58445e+02, 1.87782e+02, 2.82327e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 124.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.97699e+01, 2.58589e+02, 2.60224e+02, 4.80000e+02, 4.57032e-01, 5.60000e+01],\n",
      "        [2.29731e+02, 1.90147e+00, 4.65038e+02, 1.89614e+02, 3.79463e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 124.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.55140e+01, 2.59056e+02, 2.58793e+02, 4.80000e+02, 6.52603e-01, 5.60000e+01],\n",
      "        [2.31790e+02, 1.72276e+00, 4.59707e+02, 1.88648e+02, 3.54312e-01, 6.20000e+01],\n",
      "        [2.57837e+02, 2.32371e+02, 5.63334e+02, 4.77461e+02, 2.72376e-01, 6.20000e+01],\n",
      "        [5.65144e+02, 3.64233e+01, 6.39914e+02, 3.01957e+02, 2.58252e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86714e+01, 2.58599e+02, 2.58523e+02, 4.80000e+02, 6.30486e-01, 5.60000e+01],\n",
      "        [5.73247e+02, 2.91387e+01, 6.40000e+02, 4.61556e+02, 3.39758e-01, 0.00000e+00],\n",
      "        [2.34054e+02, 2.06705e+00, 4.59222e+02, 1.87931e+02, 3.03128e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 119.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.72133e+02, 4.07386e+01, 6.39305e+02, 4.69599e+02, 5.55886e-01, 0.00000e+00],\n",
      "        [2.32025e+02, 1.59158e+00, 4.58941e+02, 1.89115e+02, 4.44715e-01, 6.20000e+01],\n",
      "        [1.97800e+01, 2.59907e+02, 2.58456e+02, 4.78642e+02, 3.60520e-01, 5.60000e+01],\n",
      "        [2.55936e+02, 2.33680e+02, 5.61045e+02, 4.78299e+02, 3.02505e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.79912e+01, 2.59569e+02, 2.56967e+02, 4.80000e+02, 4.70089e-01, 5.60000e+01],\n",
      "        [2.31096e+02, 3.18309e+00, 4.61569e+02, 1.88242e+02, 4.27549e-01, 6.20000e+01],\n",
      "        [5.92478e+02, 1.77332e+02, 6.39251e+02, 4.77578e+02, 2.68584e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 2.0ms pre-process, 119.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.54678e+01, 2.59472e+02, 2.58476e+02, 4.80000e+02, 4.42981e-01, 5.60000e+01],\n",
      "        [2.31178e+02, 2.83900e+00, 4.63205e+02, 1.91995e+02, 3.94167e-01, 6.20000e+01],\n",
      "        [2.58066e+02, 2.31877e+02, 5.64309e+02, 4.78218e+02, 2.65740e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.97506e+01, 2.58835e+02, 2.58496e+02, 4.80000e+02, 5.88760e-01, 5.60000e+01],\n",
      "        [2.31712e+02, 3.47248e+00, 4.61655e+02, 1.90237e+02, 3.91536e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 138.4ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.15719, 259.42114, 259.16302, 480.00000,   0.65598,  56.00000],\n",
      "        [234.28951,   2.16599, 460.95343, 188.05420,   0.52560,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 118.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.95218e+01, 2.59474e+02, 2.59496e+02, 4.78786e+02, 5.51383e-01, 5.60000e+01],\n",
      "        [2.33996e+02, 3.06519e+00, 4.61122e+02, 1.87957e+02, 4.72136e-01, 6.20000e+01],\n",
      "        [5.62597e+02, 3.30160e+01, 6.38995e+02, 4.79816e+02, 4.41056e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 117.7ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.46462e+02, 1.85567e+01, 6.39122e+02, 4.74073e+02, 6.56083e-01, 0.00000e+00],\n",
      "        [2.18614e+01, 2.58432e+02, 2.59567e+02, 4.80000e+02, 5.27162e-01, 5.60000e+01],\n",
      "        [2.32342e+02, 1.97427e+00, 4.65142e+02, 1.94245e+02, 4.75404e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.90877e+01, 2.58751e+02, 2.58891e+02, 4.80000e+02, 6.63351e-01, 5.60000e+01],\n",
      "        [5.16909e+02, 1.44378e+01, 6.39360e+02, 4.65407e+02, 6.08319e-01, 0.00000e+00],\n",
      "        [2.33610e+02, 2.63391e+00, 4.63051e+02, 1.89043e+02, 4.67255e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 116.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.13068e+01, 2.58360e+02, 2.58295e+02, 4.80000e+02, 5.84327e-01, 5.60000e+01],\n",
      "        [2.34221e+02, 9.04694e-02, 5.01259e+02, 2.15190e+02, 4.46014e-01, 6.20000e+01],\n",
      "        [5.01133e+02, 6.79877e+00, 6.39679e+02, 4.69943e+02, 3.82102e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv, 1 oven\n",
      "Speed: 4.0ms pre-process, 126.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[4.93055e+02, 9.21304e+00, 6.39452e+02, 4.73187e+02, 7.26684e-01, 0.00000e+00],\n",
      "        [1.68122e+01, 2.59637e+02, 2.59064e+02, 4.79155e+02, 5.69858e-01, 5.60000e+01],\n",
      "        [2.34533e+02, 1.79178e+00, 4.84222e+02, 1.98433e+02, 4.02437e-01, 6.20000e+01],\n",
      "        [2.60424e+02, 2.31184e+02, 5.59156e+02, 4.78059e+02, 3.16226e-01, 6.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 122.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32536e+02, 1.65039e-01, 5.15603e+02, 2.21467e+02, 5.99563e-01, 6.20000e+01],\n",
      "        [2.06600e+01, 2.59151e+02, 2.58501e+02, 4.80000e+02, 3.63725e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 3.0ms pre-process, 124.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[5.04086e+02, 1.49341e+01, 6.38593e+02, 4.61528e+02, 6.67349e-01, 0.00000e+00],\n",
      "        [1.91769e+01, 2.58796e+02, 2.59114e+02, 4.80000e+02, 6.05495e-01, 5.60000e+01],\n",
      "        [2.31541e+02, 4.15764e-01, 4.92226e+02, 2.16238e+02, 4.88498e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv, 1 remote\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.51530e+01, 2.59197e+02, 2.58149e+02, 4.79666e+02, 5.96344e-01, 5.60000e+01],\n",
      "        [4.86423e+02, 2.61508e+00, 6.37679e+02, 4.77112e+02, 3.99405e-01, 0.00000e+00],\n",
      "        [2.30148e+02, 1.95817e+00, 4.65148e+02, 1.96660e+02, 3.81786e-01, 6.20000e+01],\n",
      "        [4.98672e+02, 1.10829e+02, 6.38049e+02, 3.40737e+02, 3.24058e-01, 0.00000e+00],\n",
      "        [5.15844e+02, 1.94554e+02, 6.36184e+02, 2.45291e+02, 2.72893e-01, 6.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.24033e+01, 2.58719e+02, 2.57364e+02, 4.80000e+02, 5.96400e-01, 5.60000e+01],\n",
      "        [2.31495e+02, 2.13020e+00, 4.66973e+02, 1.90975e+02, 4.58780e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 144.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.78907e+01, 2.59777e+02, 2.59290e+02, 4.80000e+02, 5.32024e-01, 5.60000e+01],\n",
      "        [2.30354e+02, 2.09234e+00, 4.68802e+02, 1.93198e+02, 3.93380e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 chair, 1 tv\n",
      "Speed: 5.0ms pre-process, 128.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.63117e+01, 2.59164e+02, 2.58820e+02, 4.80000e+02, 5.58242e-01, 5.60000e+01],\n",
      "        [2.30598e+02, 2.11707e+00, 4.62807e+02, 1.88053e+02, 4.27618e-01, 6.20000e+01],\n",
      "        [5.86089e+02, 9.38297e+00, 6.39369e+02, 1.89446e+02, 2.79717e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 20.24432, 260.15186, 260.28009, 479.89972,   0.48277,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 laptop\n",
      "Speed: 2.0ms pre-process, 128.9ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.92363e+01, 2.59960e+02, 2.57292e+02, 4.80000e+02, 6.63086e-01, 5.60000e+01],\n",
      "        [2.65835e+02, 9.74005e+01, 4.00991e+02, 2.36058e+02, 2.99277e-01, 6.30000e+01],\n",
      "        [3.53045e+02, 1.60624e+02, 6.39900e+02, 2.63176e+02, 2.63610e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 27.81246, 259.33459, 256.89420, 479.39685,   0.52491,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 121.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.65699e+01, 2.59098e+02, 2.59339e+02, 4.80000e+02, 4.79013e-01, 5.60000e+01],\n",
      "        [3.50305e+02, 1.45029e+02, 6.40000e+02, 2.86323e+02, 2.51286e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 146.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.98272e+01, 2.60564e+02, 2.57756e+02, 4.80000e+02, 4.41237e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 126.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.02934e+01, 2.60162e+02, 2.58674e+02, 4.80000e+02, 4.62038e-01, 5.60000e+01],\n",
      "        [3.50950e+02, 1.57780e+02, 6.40000e+02, 2.67735e+02, 2.65174e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 124.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34982e+01, 2.58790e+02, 2.57877e+02, 4.80000e+02, 4.75887e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 2 chairs\n",
      "Speed: 3.0ms pre-process, 127.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.07598e+01, 2.60206e+02, 2.58201e+02, 4.80000e+02, 6.59952e-01, 5.60000e+01],\n",
      "        [3.49668e+02, 7.08160e+01, 6.40000e+02, 4.36336e+02, 3.36616e-01, 0.00000e+00],\n",
      "        [2.56475e+02, 2.26675e+02, 5.58164e+02, 4.79124e+02, 3.16030e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 126.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.40546e+01, 2.59452e+02, 2.58303e+02, 4.80000e+02, 4.88815e-01, 5.60000e+01],\n",
      "        [3.56781e+02, 1.07168e+02, 6.40000e+02, 3.94986e+02, 2.60283e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 suitcase, 1 chair\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.48794e+01, 2.60158e+02, 2.58118e+02, 4.79483e+02, 4.44342e-01, 5.60000e+01],\n",
      "        [2.51299e+02, 2.26356e+02, 5.66265e+02, 4.79634e+02, 2.97428e-01, 2.80000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 139.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.79036, 260.06747, 257.93341, 480.00000,   0.55202,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.44114e+01, 2.60094e+02, 2.58292e+02, 4.79102e+02, 4.50621e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 129.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 19.42435, 259.72186, 256.82321, 480.00000,   0.51292,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.6ms pre-process, 127.6ms inference, 2.2ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 27.96400, 259.59814, 256.92523, 480.00000,   0.66996,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 159.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94716e+01, 2.59608e+02, 2.57194e+02, 4.78707e+02, 3.81576e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 120.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.02135e+01, 2.59514e+02, 2.56891e+02, 4.80000e+02, 3.71785e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 23.82391, 259.65778, 257.21820, 480.00000,   0.76407,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 3.0ms pre-process, 140.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.56283e+01, 2.59493e+02, 2.57978e+02, 4.80000e+02, 4.44795e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 126.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.71066, 259.78720, 256.89603, 480.00000,   0.57302,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 126.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.88418e+01, 2.59720e+02, 2.58020e+02, 4.80000e+02, 4.57176e-01, 5.60000e+01],\n",
      "        [3.53984e+02, 8.07547e+01, 6.40000e+02, 4.29916e+02, 3.08457e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 4.0ms pre-process, 144.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.94375e+01, 2.60556e+02, 2.57883e+02, 4.78459e+02, 6.06380e-01, 5.60000e+01],\n",
      "        [3.53610e+02, 8.22126e+01, 6.40000e+02, 4.19040e+02, 2.57168e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 4.0ms pre-process, 126.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 17.03934, 259.44574, 257.04416, 480.00000,   0.57694,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 4.0ms pre-process, 147.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 118.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.25540e+01, 2.59510e+02, 2.57166e+02, 4.78833e+02, 3.10478e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 121.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.55016e+02, 8.25256e+01, 6.40000e+02, 4.19960e+02, 3.19205e-01, 0.00000e+00],\n",
      "        [2.28169e+01, 2.60056e+02, 2.54991e+02, 4.79181e+02, 2.53712e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 1.0ms pre-process, 119.6ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.25486e+01, 2.59961e+02, 2.57369e+02, 4.79774e+02, 3.91725e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 suitcase, 1 chair\n",
      "Speed: 3.0ms pre-process, 162.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.81777e+01, 2.59246e+02, 2.58437e+02, 4.80000e+02, 5.04984e-01, 5.60000e+01],\n",
      "        [2.55259e+02, 2.30304e+02, 5.57480e+02, 4.80000e+02, 3.10591e-01, 2.80000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 3.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.86867e+01, 2.59785e+02, 2.57154e+02, 4.80000e+02, 4.43005e-01, 5.60000e+01],\n",
      "        [3.55375e+02, 7.58679e+01, 6.40000e+02, 4.16165e+02, 3.43264e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 130.3ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.40790e+01, 2.59673e+02, 2.56309e+02, 4.80000e+02, 5.88750e-01, 5.60000e+01],\n",
      "        [3.54984e+02, 5.69266e+01, 6.40000e+02, 4.36273e+02, 3.07271e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair\n",
      "Speed: 2.0ms pre-process, 131.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.96092e+01, 2.59056e+02, 2.56856e+02, 4.79939e+02, 5.74941e-01, 5.60000e+01],\n",
      "        [3.53495e+02, 4.17863e+01, 6.40000e+02, 4.47993e+02, 3.72129e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 4.0ms pre-process, 125.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.60319e+01, 2.59886e+02, 2.57970e+02, 4.80000e+02, 6.21163e-01, 5.60000e+01],\n",
      "        [2.32022e+02, 1.69855e+00, 4.59160e+02, 1.87265e+02, 2.93302e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair\n",
      "Speed: 2.0ms pre-process, 122.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 18.20482, 259.87732, 257.15662, 480.00000,   0.62786,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 frisbee, 2 chairs, 1 tv, 1 laptop\n",
      "Speed: 3.0ms pre-process, 124.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.13704e+01, 2.59111e+02, 2.58063e+02, 4.79301e+02, 5.16453e-01, 5.60000e+01],\n",
      "        [2.31354e+02, 2.71558e+00, 4.55604e+02, 2.03863e+02, 4.93550e-01, 6.20000e+01],\n",
      "        [2.87929e+02, 7.26146e+01, 3.93307e+02, 2.41452e+02, 4.35564e-01, 6.30000e+01],\n",
      "        [2.51233e+02, 2.29561e+02, 5.64713e+02, 4.77930e+02, 3.07850e-01, 5.60000e+01],\n",
      "        [2.87732e+02, 7.68972e+01, 3.79540e+02, 2.31777e+02, 2.79612e-01, 2.90000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 suitcase, 1 chair\n",
      "Speed: 2.0ms pre-process, 123.9ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.48414e+01, 2.58828e+02, 2.56092e+02, 4.80000e+02, 5.09761e-01, 5.60000e+01],\n",
      "        [3.47960e+02, 1.54793e+02, 6.40000e+02, 2.54537e+02, 4.06407e-01, 0.00000e+00],\n",
      "        [2.55032e+02, 2.25719e+02, 5.68749e+02, 4.79082e+02, 2.80347e-01, 2.80000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv, 1 laptop\n",
      "Speed: 2.0ms pre-process, 124.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.57954e+01, 2.58898e+02, 2.56781e+02, 4.80000e+02, 5.91215e-01, 5.60000e+01],\n",
      "        [2.34348e+02, 1.51163e+00, 4.60160e+02, 1.86059e+02, 4.26925e-01, 6.20000e+01],\n",
      "        [2.83362e+02, 1.14557e+02, 6.26411e+02, 2.50999e+02, 2.67589e-01, 6.30000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 129.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34400e+02, 2.69099e+00, 4.60610e+02, 1.88002e+02, 4.96154e-01, 6.20000e+01],\n",
      "        [1.79182e+01, 2.59171e+02, 2.59765e+02, 4.80000e+02, 4.72490e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 3.0ms pre-process, 121.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[1.79800e+01, 2.58455e+02, 2.58080e+02, 4.80000e+02, 5.78453e-01, 5.60000e+01],\n",
      "        [2.35049e+02, 2.83326e+00, 4.61931e+02, 1.89531e+02, 4.83415e-01, 6.20000e+01],\n",
      "        [2.58040e+02, 2.32748e+02, 5.61846e+02, 4.79271e+02, 2.59487e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 2 tvs\n",
      "Speed: 4.0ms pre-process, 123.0ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.03900e+01, 2.58820e+02, 2.58013e+02, 4.80000e+02, 6.88843e-01, 5.60000e+01],\n",
      "        [2.32363e+02, 3.06721e+00, 4.59568e+02, 1.87590e+02, 5.01302e-01, 6.20000e+01],\n",
      "        [2.58754e+02, 2.32411e+02, 5.60591e+02, 4.78271e+02, 2.69709e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 chair, 1 tv\n",
      "Speed: 0.9ms pre-process, 117.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.42825e+01, 2.59662e+02, 2.57580e+02, 4.79398e+02, 6.42101e-01, 5.60000e+01],\n",
      "        [2.33959e+02, 2.10220e+00, 4.59068e+02, 1.89420e+02, 4.26799e-01, 6.20000e+01],\n",
      "        [5.78626e+02, 2.90593e+01, 6.39685e+02, 3.26789e+02, 2.51780e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 126.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[234.15793,   1.67326, 458.12601, 189.20885,   0.55365,  62.00000],\n",
      "        [ 20.35969, 260.15424, 258.82388, 480.00000,   0.53051,  56.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 125.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.34961e+02, 2.63638e+00, 4.61627e+02, 1.88731e+02, 5.95814e-01, 6.20000e+01],\n",
      "        [2.11012e+02, 0.00000e+00, 6.39427e+02, 4.77703e+02, 3.94424e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 cat, 1 tv\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.32798e+02, 3.44448e+00, 4.61673e+02, 1.88777e+02, 5.52637e-01, 6.20000e+01],\n",
      "        [3.48500e+02, 4.70253e+01, 6.37822e+02, 4.78897e+02, 2.61573e-01, 1.50000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 tv\n",
      "Speed: 2.0ms pre-process, 129.0ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[231.90976,   2.43343, 461.32492, 195.31868,   0.56587,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 tv\n",
      "Speed: 4.0ms pre-process, 136.6ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[229.74602,   2.20036, 462.28204, 188.19687,   0.50189,  62.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 144.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 143.1ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[  0.00000,   0.00000, 411.44861, 466.39450,   0.49786,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 129.9ms inference, 1.6ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[  0.00000,   8.76663, 594.51416, 480.00000,   0.66359,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 130.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[  0.00000,   6.23024, 406.63571, 403.03052,   0.49379,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 (no detections)\n",
      "Speed: 2.0ms pre-process, 111.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([], size=(0, 6))\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 123.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[286.69714,   0.66380, 569.40356, 363.73395,   0.59938,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 3.0ms pre-process, 122.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[3.59927e+02, 0.00000e+00, 6.40000e+02, 3.92823e+02, 5.06970e-01, 0.00000e+00],\n",
      "        [2.37377e+02, 3.32057e+00, 4.19499e+02, 1.86502e+02, 3.05332e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 2 persons, 1 chair, 1 tv\n",
      "Speed: 2.0ms pre-process, 119.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[2.35958e+02, 1.21849e+00, 4.53526e+02, 1.84090e+02, 5.52247e-01, 6.20000e+01],\n",
      "        [7.59857e+01, 0.00000e+00, 6.33690e+02, 4.80000e+02, 3.79399e-01, 0.00000e+00],\n",
      "        [3.57354e+02, 0.00000e+00, 6.40000e+02, 4.04044e+02, 2.62243e-01, 0.00000e+00],\n",
      "        [2.82199e+01, 2.60266e+02, 1.96078e+02, 4.76800e+02, 2.54522e-01, 5.60000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 113.5ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.86309e+01, 1.41660e+01, 5.49558e+02, 4.80000e+02, 4.27690e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 4.0ms pre-process, 121.2ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[8.02846e+01, 1.65258e+01, 5.98158e+02, 4.80000e+02, 4.77639e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 152.1ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.40545e+01, 1.42118e+01, 5.94769e+02, 4.74603e+02, 4.12480e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 128.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[ 64.44051,  11.60878, 554.62512, 480.00000,   0.56852,   0.00000]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 2.0ms pre-process, 133.7ms inference, 2.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.85447e+01, 1.38114e+01, 5.93224e+02, 4.77798e+02, 5.28025e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 2.0ms pre-process, 120.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[6.76370e+01, 1.63288e+01, 6.40000e+02, 4.80000e+02, 6.59230e-01, 0.00000e+00],\n",
      "        [2.37335e+02, 1.77854e+00, 4.54821e+02, 1.85674e+02, 2.68631e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person\n",
      "Speed: 3.0ms pre-process, 109.5ms inference, 1.0ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.87934e+01, 1.44927e+01, 5.94520e+02, 4.80000e+02, 5.15072e-01, 0.00000e+00]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "YOLOv5 Results: image 1/1: 480x640 1 person, 1 tv\n",
      "Speed: 4.0ms pre-process, 115.0ms inference, 2.3ms NMS per image at shape (1, 3, 480, 640)\n",
      "Detections: tensor([[7.13213e+01, 1.41389e+01, 6.40000e+02, 4.77657e+02, 6.06423e-01, 0.00000e+00],\n",
      "        [2.37428e+02, 0.00000e+00, 4.58064e+02, 1.87329e+02, 3.11459e-01, 6.20000e+01]])\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLOv5 on the current frame\n",
    "    results = yolo_model(frame)\n",
    "    # Get detection results\n",
    "    detections = results.xyxy[0]  # xyxy format: (x1, y1, x2, y2, conf, cls)\n",
    "    print(\"YOLOv5 Results:\", results)\n",
    "    print(\"Detections:\", detections)\n",
    "\n",
    "    for *box, conf, cls in detections:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_sign = frame[y1:y2, x1:x2]\n",
    "\n",
    "        if cropped_sign.size == 0:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Preprocess and classify using CNN\n",
    "            processed = preprocess_sign(cropped_sign)\n",
    "            prediction = cnn_model.predict(processed)\n",
    "            label = class_names[np.argmax(prediction)]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        except Exception as e:\n",
    "            print(\"Error classifying:\", e)\n",
    "            continue\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Traffic Sign Recognition\", frame)\n",
    "\n",
    "    # Press 'ESC' to exit\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a7ccf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5su.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba13bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "yolo_model = YOLO(\"yolov5su.pt\")         # or \"best.pt\" if it's your fine-tuned YOLOv5 model\n",
    "cnn_model = load_model('cnn_model_v01.h5')\n",
    "\n",
    "# Input/output\n",
    "image_folder = \"GTSDB/images/train\"\n",
    "output_file = \"yolo_cnn_predictions.txt\"\n",
    "\n",
    "# Image size expected by CNN model\n",
    "cnn_input_size = (64, 64)  # adjust as per your model\n",
    "\n",
    "# Open results file\n",
    "with open(output_file, \"w\") as f:\n",
    "    for img_name in os.listdir(image_folder):\n",
    "        if img_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            image = cv2.imread(img_path)\n",
    "\n",
    "            # YOLO detection\n",
    "            results = yolo_model.predict(img_path, save=False, conf=0.25, verbose=False)\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)  # (x1, y1, x2, y2)\n",
    "\n",
    "            # Start writing image name\n",
    "            f.write(f\"{img_name} \")\n",
    "\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                crop = image[y1:y2, x1:x2]\n",
    "\n",
    "                # Resize crop for CNN\n",
    "                # crop_resized = cv2.resize(crop, cnn_input_size)\n",
    "                # crop_input = crop_resized.astype(\"float32\") / 255.0\n",
    "                # crop_input = np.expand_dims(crop_input, axis=0)\n",
    "\n",
    "                # CNN Prediction\n",
    "                pred = cnn_model.predict(preprocess_sign(crop), verbose=0)\n",
    "                class_id = np.argmax(pred)\n",
    "\n",
    "                # Write class to file\n",
    "                f.write(f\"{class_id} \")\n",
    "\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163febb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b51287",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"yolo_cnn_predictions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b4446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(' ')\n",
    "    img_name = parts[0]\n",
    "    if len(parts) > 1:\n",
    "        class_ids = [int(cls_id) for cls_id in parts[1:]]\n",
    "    else:\n",
    "        class_ids = [99]\n",
    "    if class_ids:\n",
    "        pred.append((img_name, class_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "935c349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = []\n",
    "for file in os.listdir(\"GTSDB/labels/train\"):\n",
    "    with open(os.path.join(\"GTSDB/labels/train\", file), \"r\") as f:\n",
    "        content = f.readlines()\n",
    "        content = [line.strip() for line in content if line.strip()]\n",
    "        if content:\n",
    "            class_id = []\n",
    "            for line in content:\n",
    "                class_id.append(int(line.split()[0]))\n",
    "            actual.append((file, class_id))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11517346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05016181229773463"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(len(actual)):\n",
    "    count += (max(len(actual[i][1]), len(pred[i][1])))\n",
    "\n",
    "for i in range(len(actual)):\n",
    "    for j in pred[i][1]:\n",
    "        if j in actual[i][1]:\n",
    "            sum += 1\n",
    "accuracy = sum / count\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4d043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
